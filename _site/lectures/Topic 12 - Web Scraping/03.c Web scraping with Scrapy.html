<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <title>03.c Web scraping with Scrapy - Data Focused Python</title> <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-139655036-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-139655036-1'); </script> <script type="text/javascript" src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.7.1 --> <title>03.c Web scraping with Scrapy | Data Focused Python</title> <meta name="generator" content="Jekyll v3.9.0" /> <meta property="og:title" content="03.c Web scraping with Scrapy" /> <meta property="og:locale" content="en_US" /> <link rel="canonical" href="http://localhost:4000/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html" /> <meta property="og:url" content="http://localhost:4000/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html" /> <meta property="og:site_name" content="Data Focused Python" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="03.c Web scraping with Scrapy" /> <script type="application/ld+json"> {"headline":"03.c Web scraping with Scrapy","url":"http://localhost:4000/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html","@type":"WebPage","@context":"https://schema.org"}</script> <!-- End Jekyll SEO tag --> </head> <body> <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header"> <a href="http://localhost:4000/" class="site-title lh-tight"> Data Focused Python </a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a> </div> <nav role="navigation" aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="http://localhost:4000/" class="nav-list-link">Data Focused Python</a></li><li class="nav-list-item active"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/lectures" class="nav-list-link">Lectures</a><ul class="nav-list "><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/lectures/Topic%2001%20-%20Getting%20Started.html" class="nav-list-link">Topic 01 - Getting Started</a><ul class="nav-list"><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2001%20-%20Getting%20Started/01%20-%20Getting%20Started.html" class="nav-list-link">01 - Getting Started</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2001%20-%20Getting%20Started/02%20-%20Running%20Python.html" class="nav-list-link">02 - Running Python</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2001%20-%20Getting%20Started/03%20-%20Your%20First%20Python%20Program.html" class="nav-list-link">03 - Your First Python Program</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2001%20-%20Getting%20Started/04%20-%20Python%20Modules%20and%20Packages.html" class="nav-list-link">04 - Python Modules and Packages</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2001%20-%20Getting%20Started/05%20-%20Start%20a%20process%20in%20Python.html" class="nav-list-link">05 - Start a process in Python</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2001%20-%20Getting%20Started/06.a%20-%20Writing%20Terminal%20Application.html" class="nav-list-link">06.a - Writing Terminal Application</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2001%20-%20Getting%20Started/06.b%20-%20Argparse.html" class="nav-list-link">06.b - Argparse</a> </li></ul></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/lectures/Topic%2002%20-%20Python%20basics.html" class="nav-list-link">Topic 02 - Python basics</a><ul class="nav-list"><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2002%20-%20Python%20basics/01%20-%20Decision%20Structures%20and%20Boolean%20Logic.html" class="nav-list-link">01 - Decision Structures and Boolean Logic</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2002%20-%20Python%20basics/02%20-%20Python%20Tuples.html" class="nav-list-link">02 - Python Tuples</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2002%20-%20Python%20basics/03%20-%20Loops.html" class="nav-list-link">03 - Loops</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html" class="nav-list-link">04 - Python Functions</a> </li></ul></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/lectures/Topic%2003%20-%20Generating%20Data.html" class="nav-list-link">Topic 03 - Generating Data</a><ul class="nav-list"><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2003%20-%20Generating%20Data/01%20-%20Generating%20Fake%20Data.html" class="nav-list-link">01 - Generating Fake Data</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html" class="nav-list-link">02 - Using Faker</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2003%20-%20Generating%20Data/03%20-%20Generating%20Synthetic%20Healthcare%20Data.html" class="nav-list-link">03 - Generating Synthetic Healthcare Data</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2003%20-%20Generating%20Data/04%20-%20Using%20Synthea.html" class="nav-list-link">04 - Using Synthea</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html" class="nav-list-link">05 - Generating Other Synthetic Data</a> </li></ul></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/lectures/Topic%2004%20-%20Writing%20Testable%20Code.html" class="nav-list-link">Topic 04 - Writing Testable Code</a><ul class="nav-list"><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2004%20-%20Writing%20Testable%20Code/01%20-%20Python%20Code%20Quality%20Tools%20and%20Best%20Practices.html" class="nav-list-link">01 - Python Code Quality Tools and Best Practices</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2004%20-%20Writing%20Testable%20Code/02%20-%20Python%20Testing.html" class="nav-list-link">02 - Python Testing</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2004%20-%20Writing%20Testable%20Code/03%20-%20Getting%20Started%20With%20Testing%20in%20Python.html" class="nav-list-link">03 - Getting Started With Testing in Python</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2004%20-%20Writing%20Testable%20Code/04-%20Choosing%20a%20Test%20Runner.html" class="nav-list-link">04- Choosing a Test Runner</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2004%20-%20Writing%20Testable%20Code/05%20-%20Writing%20Your%20First%20Test.html" class="nav-list-link">05 - Writing Your First Test</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2004%20-%20Writing%20Testable%20Code/06%20-%20Writing%20Integration%20Tests.html" class="nav-list-link">06 - Writing Integration Tests</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2004%20-%20Writing%20Testable%20Code/07%20-%20Testing%20in%20Multiple%20Environments.html" class="nav-list-link">07 - Testing in Multiple Environments</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2004%20-%20Writing%20Testable%20Code/08%20-%20Introducing%20Linters%20Into%20Your%20Application.html" class="nav-list-link">08 - Introducing Linters Into Your Application</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2004%20-%20Writing%20Testable%20Code/09%20-%20Testing%20for%20Performance%20Degradation%20Between%20Changes.html" class="nav-list-link">09 - Testing for Performance Degradation Between Changes</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2004%20-%20Writing%20Testable%20Code/10%20-%20Testing%20for%20Security%20Flaws%20in%20Your%20Application.html" class="nav-list-link">10 - Testing for Security Flaws in Your Application</a> </li></ul></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/lectures/Topic%2005%20-%20Processing%20files.html" class="nav-list-link">Topic 05 - Processing files</a><ul class="nav-list"><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html" class="nav-list-link">01.a - File Processing</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html" class="nav-list-link">01.b - CSV Files</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html" class="nav-list-link">01.c - JSON Files</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2005%20-%20Processing%20files/01.d%20-%20XML%20Files.html" class="nav-list-link">01.d - XML Files</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2005%20-%20Processing%20files/03%20-%20Using%20fhirclient%20to%20parse%20Healthcare%20Data.html" class="nav-list-link">03 - Using fhirclient to parse Healthcare Data</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2005%20-%20Processing%20files/04%20-%20Reading%20synthea%20data.html" class="nav-list-link">04 - Reading synthea data</a> </li></ul></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators.html" class="nav-list-link">Topic 06 - Comprehensions and Generators</a><ul class="nav-list"><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html" class="nav-list-link">02 - Lists and List Comprehensions</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/03%20-%20Sets%20and%20Set%20Comprehensions.html" class="nav-list-link">03 - Sets and Set Comprehensions</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.a%20-%20Dictionaries.html" class="nav-list-link">04.a - Dictionaries</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.b%20-%20Dictionary%20Comprehensions.html" class="nav-list-link">04.b - Dictionary Comprehensions</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/05%20-%20Python%20Generators.html" class="nav-list-link">05 - Python Generators</a> </li></ul></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/lectures/Topic%2007%20-%20Classes.html" class="nav-list-link">Topic 07 - Classes</a><ul class="nav-list"><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html" class="nav-list-link">01 - Python Classes</a> </li></ul></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/lectures/Topic%2008%20-%20Making%20Web%20Requests.html" class="nav-list-link">Topic 08 - Making Web Requests</a><ul class="nav-list"><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2008%20-%20Making%20Web%20Requests/03%20-%20Using%20requests%20to%20fetch%20Healthcare%20Data.html" class="nav-list-link">03 - Using requests to fetch Healthcare Data</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html" class="nav-list-link">05 - Getting Data from Web APIs</a> </li></ul></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/lectures/Topic%2009%20-%20Interfacing%20with%20Databases.html" class="nav-list-link">Topic 09 - Interfacing with Databases</a><ul class="nav-list"></ul></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201.html" class="nav-list-link">Topic 10 - Data Processing and Visualization Part 1</a><ul class="nav-list"><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.a%20-%20Cleaning%20and%20Transforming%20Data.html" class="nav-list-link">01.a - Cleaning and Transforming Data</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.b%20-%20Grouping%20Data.html" class="nav-list-link">01.b - Grouping Data</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.c%20-%20Descriptive%20Statistics.html" class="nav-list-link">01.c - Descriptive Statistics</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html" class="nav-list-link">02 - Basic Medical Data Visualization</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html" class="nav-list-link">03.a - NumPy Introduction</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html" class="nav-list-link">03.b - NumPy Data analysis</a> </li></ul></li><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202.html" class="nav-list-link">Topic 11 - Data Processing and Visualization Part 2</a><ul class="nav-list"><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/00%20-%20Additional%20Links.html" class="nav-list-link">00 - Additional Links</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/01%20-%20Pandas%20Introduction.html" class="nav-list-link">01 - Pandas Introduction</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html" class="nav-list-link">02.a - Pandas Data analysis Part 1</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html" class="nav-list-link">02.b - Pandas Data analysis Part 2</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html" class="nav-list-link">03 - Matplotlib Tutorial Python Plotting</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html" class="nav-list-link">04 - The Ultimate Python Seaborn Tutorial</a> </li></ul></li><li class="nav-list-item active"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/lectures/Topic%2012%20-%20Web%20Scraping.html" class="nav-list-link">Topic 12 - Web Scraping</a><ul class="nav-list"><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2012%20-%20Web%20Scraping/01.%20Introduction%20to%20Web%20Scraping.html" class="nav-list-link">01. Introduction to Web Scraping</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html" class="nav-list-link">02. CSS Selectors</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html" class="nav-list-link">03.a Web Scraping with Beautiful Soup</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2012%20-%20Web%20Scraping/03.b%20Web%20Scraping%20Using%20Selenium.html" class="nav-list-link">03.b Web Scraping Using Selenium</a> </li><li class="nav-list-item active"> <a href="http://localhost:4000/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html" class="nav-list-link active">03.c Web scraping with Scrapy</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2012%20-%20Web%20Scraping/04.a%20-%20More%20Web%20Scraping%20with%20BeautifulSoup.html" class="nav-list-link">04.a - More Web Scraping with BeautifulSoup</a> </li><li class="nav-list-item "> <a href="http://localhost:4000/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html" class="nav-list-link">04.b - Even More Web Scraping with BeautifulSoup</a> </li></ul></li></ul></li><li class="nav-list-item"><a href="http://localhost:4000/resources" class="nav-list-link">Resources</a></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> </div> <div id="main-content-wrap" class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/lectures">Lectures</a></li> <li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/lectures/Topic%2012%20-%20Web%20Scraping.html">Topic 12 - Web Scraping</a></li> <li class="breadcrumb-nav-list-item"><span>03.c Web scraping with Scrapy</span></li> </ol> </nav> <div id="main-content" class="main-content" role="main"> <h1 id="web-scraping-with-scrapy"> <a href="#web-scraping-with-scrapy" class="anchor-heading" aria-labelledby="web-scraping-with-scrapy"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Web scraping with Scrapy </h1> <p><a href="https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3">source</a></p> <h2 id="introduction"> <a href="#introduction" class="anchor-heading" aria-labelledby="introduction"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Introduction </h2> <p>Web scraping, often called web crawling or web spidering, or “programmatically going over a collection of web pages and extracting data,” is a powerful tool for working with data on the web.</p> <p>With a web scraper, you can mine data about a set of products, get a large corpus of text or quantitative data to play around with, get data from a site without an official API, or just satisfy your own personal curiosity.</p> <p>In this tutorial, you’ll learn about the fundamentals of the scraping and spidering process as you explore a playful data set. We’ll use BrickSet, a community-run site that contains information about LEGO sets. By the end of this tutorial, you’ll have a fully functional Python web scraper that walks through a series of pages on Brickset and extracts data about LEGO sets from each page, displaying the data to your screen.</p> <p>The scraper will be easily expandable so you can tinker around with it and use it as a foundation for your own projects scraping data from the web.</p> <h2 id="prerequisites"> <a href="#prerequisites" class="anchor-heading" aria-labelledby="prerequisites"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Prerequisites </h2> <p>To complete this tutorial, you’ll need a local development environment for Python 3.</p> <h2 id="step-1--creating-a-basic-scraper"> <a href="#step-1--creating-a-basic-scraper" class="anchor-heading" aria-labelledby="step-1--creating-a-basic-scraper"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Step 1 — Creating a Basic Scraper </h2> <p>Scraping is a two step process:</p> <ol> <li>You systematically find and download web pages.</li> <li>You take those web pages and extract information from them.</li> </ol> <p>Both of those steps can be implemented in a number of ways in many languages.</p> <p>You can build a scraper from scratch using modules or libraries provided by your programming language, but then you have to deal with some potential headaches as your scraper grows more complex. For example, you’ll need to handle concurrency so you can crawl more than one page at a time. You’ll probably want to figure out how to transform your scraped data into different formats like <code class="language-plaintext highlighter-rouge">CSV</code>, <code class="language-plaintext highlighter-rouge">XML</code>, or <code class="language-plaintext highlighter-rouge">JSON</code>. And you’ll sometimes have to deal with sites that require specific settings and access patterns.</p> <p>You’ll have better luck if you build your scraper on top of an existing library that handles those issues for you. For this tutorial, we’re going to use Python and <a href="http://doc.scrapy.org/en/1.1/intro/overview.html">Scrapy</a> to build our scraper.</p> <h3 id="scrapy"> <a href="#scrapy" class="anchor-heading" aria-labelledby="scrapy"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Scrapy </h3> <p>Scrapy is one of the most popular and powerful Python scraping libraries; it takes a “batteries included” approach to scraping, meaning that it handles a lot of the common functionality that all scrapers need so developers don’t have to reinvent the wheel each time. It makes scraping a quick and fun process!</p> <p>Scrapy, like most Python packages, is on PyPI (also known as <code class="language-plaintext highlighter-rouge">pip</code>). PyPI, the Python Package Index, is a community-owned repository of all published Python software.</p> <p>If you have a Python installation like the one outlined in the prerequisite for this tutorial, you already have pip installed on your machine, so you can install Scrapy with the following command:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">scrapy</span>
</code></pre></div></div> <p>If you run into any issues with the installation, or you want to install Scrapy without using pip, check out the <a href="https://doc.scrapy.org/en/1.1/intro/install.html">official installation docs</a>.</p> <p>With Scrapy installed, let’s create a new folder for our project. You can do this in the terminal by running:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">mkdir</span> <span class="n">scrapers</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mkdir: scrapers: File exists
</code></pre></div></div> <p>Then create a new Python file for our scraper called scraper.py. We’ll place all of our code in this file for this tutorial. You can create this file in the terminal with the touch command, like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">touch</span> <span class="n">scrapers</span><span class="o">/</span><span class="n">scraper</span><span class="p">.</span><span class="n">py</span>
</code></pre></div></div> <p>We’ll start by making a very basic scraper that uses Scrapy as its foundation. To do that, we’ll create a <a href="https://www.digitalocean.com/community/tutorials/how-to-construct-classes-and-define-objects-in-python-3">Python class</a> that subclasses <code class="language-plaintext highlighter-rouge">scrapy.Spider</code>, a basic spider class provided by Scrapy.</p> <p>This class will have two required attributes:</p> <ul> <li><code class="language-plaintext highlighter-rouge">name</code> — just a name for the spider.</li> <li><code class="language-plaintext highlighter-rouge">start_urls</code> — a list of URLs that you start to crawl from. We’ll start with one URL.</li> </ul> <p>Open the <code class="language-plaintext highlighter-rouge">scrapy.py</code> file in your text editor and add this code to create the basic spider:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">BrickSetSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="p">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">"brickset_spider"</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">'http://brickset.com/sets/year-2016'</span><span class="p">]</span>
</code></pre></div></div> <p>or you can copy <code class="language-plaintext highlighter-rouge">scrapers/scraper1.py</code> where this has already been done for you.</p> <p>Let’s break this down line by line:</p> <ol> <li>First, we import <code class="language-plaintext highlighter-rouge">scrapy</code> so that we can use the classes that the package provides.</li> <li>Next, we take the <code class="language-plaintext highlighter-rouge">Spider</code> class provided by Scrapy and make a subclass out of it called <code class="language-plaintext highlighter-rouge">BrickSetSpider</code>. Think of a subclass as a more specialized form of its parent class. The <code class="language-plaintext highlighter-rouge">Spider</code> subclass has methods and behaviors that define how to follow URLs and extract data from the pages it finds, but it doesn’t know where to look or what data to look for. By subclassing it, we can give it that information.</li> <li>Then we give the spider the name <code class="language-plaintext highlighter-rouge">brickset_spider</code>.</li> <li>Finally, we give our scraper a single URL to start from: http://brickset.com/sets/year-2016. If you open that URL in your browser, it will take you to a search results page, showing the first of many pages containing LEGO sets.</li> </ol> <p>Now let’s test out the scraper. You typically run Python files by running a command like <code class="language-plaintext highlighter-rouge">python path/to/file.py</code>. However, Scrapy comes with its <a href="https://doc.scrapy.org/en/latest/topics/commands.html">own command line interface</a> to streamline the process of starting a scraper.</p> <p>Start your scraper with the following command:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">scrapy</span> <span class="n">runspider</span> <span class="n">scrapers</span><span class="o">/</span><span class="n">scraper1</span><span class="p">.</span><span class="n">py</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2021-10-21 11:30:37 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)
2021-10-21 11:30:37 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.9.7 (default, Sep 16 2021, 08:50:36) - [Clang 10.0.0 ], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 35.0.0, Platform macOS-10.16-x86_64-i386-64bit
2021-10-21 11:30:37 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2021-10-21 11:30:37 [scrapy.crawler] INFO: Overridden settings:
{'SPIDER_LOADER_WARN_ONLY': True}
2021-10-21 11:30:37 [scrapy.extensions.telnet] INFO: Telnet Password: 69d098b24fbe31f9
2021-10-21 11:30:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2021-10-21 11:30:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-21 11:30:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-21 11:30:37 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2021-10-21 11:30:37 [scrapy.core.engine] INFO: Spider opened
2021-10-21 11:30:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-21 11:30:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-10-21 11:30:37 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://brickset.com/sets/year-2016&gt; (referer: None)
2021-10-21 11:30:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://brickset.com/sets/year-2016&gt;: HTTP status code is not handled or not allowed
2021-10-21 11:30:37 [scrapy.core.engine] INFO: Closing spider (finished)
2021-10-21 11:30:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 230,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2140,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 0.198192,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 10, 21, 15, 30, 37, 904414),
 'httpcompression/response_bytes': 3136,
 'httpcompression/response_count': 1,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/DEBUG': 1,
 'log_count/INFO': 11,
 'memusage/max': 228245504,
 'memusage/startup': 228245504,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2021, 10, 21, 15, 30, 37, 706222)}
2021-10-21 11:30:37 [scrapy.core.engine] INFO: Spider closed (finished)
</code></pre></div></div> <p>That’s a lot of output, so let’s break it down.</p> <ul> <li>The scraper initialized and loaded additional components and extensions it needed to handle reading data from URLs.</li> <li>It used the URL we provided in the <code class="language-plaintext highlighter-rouge">start_urls</code> list and grabbed the HTML, just like your web browser would do.</li> <li>It passed that HTML to the <code class="language-plaintext highlighter-rouge">parse</code> method, which doesn’t do anything by default. Since we never wrote our own <code class="language-plaintext highlighter-rouge">parse</code> method, the spider just finishes without doing any work.</li> </ul> <p>Now let’s pull some data from the page.</p> <h2 id="step-2--extracting-data-from-a-page"> <a href="#step-2--extracting-data-from-a-page" class="anchor-heading" aria-labelledby="step-2--extracting-data-from-a-page"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Step 2 — Extracting Data from a Page </h2> <p>We’ve created a very basic program that pulls down a page, but it doesn’t do any scraping or spidering yet. Let’s give it some data to extract.</p> <p>If you look at <a href="http://brickset.com/sets/year-2016">the page we want to scrape</a>, you’ll see it has the following structure:</p> <ul> <li>There’s a header that’s present on every page.</li> <li>There’s some top-level search data, including the number of matches, what we’re searching for, and the breadcrumbs for the site.</li> <li>Then there are the sets themselves, displayed in what looks like a table or ordered list. Each set has a similar format.</li> </ul> <p>When writing a scraper, it’s a good idea to look at the source of the HTML file and familiarize yourself with the structure. So here it is, with some things removed for readability:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">&lt;!-- brickset.com/sets/year-2016 --&gt;</span>
<span class="nt">&lt;body&gt;</span>
  <span class="nt">&lt;section</span> <span class="na">class=</span><span class="s">"setlist"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;article</span> <span class="na">class=</span><span class="s">'set'</span><span class="nt">&gt;</span>
      <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">"https://images.brickset.com/sets/large/10251-1.jpg?201510121127"</span> 
      <span class="na">class=</span><span class="s">"highslide plain mainimg"</span> <span class="na">onclick=</span><span class="s">"return hs.expand(this)"</span><span class="nt">&gt;&lt;img</span> 
      <span class="na">src=</span><span class="s">"https://images.brickset.com/sets/small/10251-1.jpg?201510121127"</span> <span class="na">title=</span><span class="s">"10251-1: 
      Brick Bank"</span> <span class="na">onError=</span><span class="s">"this.src='/assets/images/spacer.png'"</span> <span class="nt">/&gt;&lt;/a&gt;</span>
      <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"highslide-caption"</span><span class="nt">&gt;</span>
        <span class="nt">&lt;h1&gt;</span>Brick Bank<span class="nt">&lt;/h1&gt;&lt;div</span> <span class="na">class=</span><span class="s">'tags floatleft'</span><span class="nt">&gt;&lt;a</span> <span class="na">href=</span><span class="s">'/sets/10251-1/Brick- 
        Bank'</span><span class="nt">&gt;</span>10251-1<span class="nt">&lt;/a&gt;</span> <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">'/sets/theme-Creator-Expert'</span><span class="nt">&gt;</span>Creator Expert<span class="nt">&lt;/a&gt;</span> <span class="nt">&lt;a</span> 
        <span class="na">class=</span><span class="s">'subtheme'</span> <span class="na">href=</span><span class="s">'/sets/theme-Creator-Expert/subtheme-Modular- 
        Buildings'</span><span class="nt">&gt;</span>Modular Buildings<span class="nt">&lt;/a&gt;</span> <span class="nt">&lt;a</span> <span class="na">class=</span><span class="s">'year'</span> <span class="na">href=</span><span class="s">'/sets/theme-Creator- 
        Expert/year-2016'</span><span class="nt">&gt;</span>2016<span class="nt">&lt;/a&gt;</span> <span class="nt">&lt;/div&gt;&lt;div</span> <span class="na">class=</span><span class="s">'floatright'</span><span class="nt">&gt;</span><span class="ni">&amp;copy;</span>2016 LEGO 
        Group<span class="nt">&lt;/div&gt;</span>
          <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"pn"</span><span class="nt">&gt;</span>
            <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">"#"</span> <span class="na">onclick=</span><span class="s">"return hs.previous(this)"</span> <span class="na">title=</span><span class="s">"Previous (left arrow 
            key)"</span><span class="nt">&gt;</span><span class="ni">&amp;#171;</span> Previous<span class="nt">&lt;/a&gt;</span>
            <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">"#"</span> <span class="na">onclick=</span><span class="s">"return hs.next(this)"</span> <span class="na">title=</span><span class="s">"Next (right arrow key)"</span><span class="nt">&gt;</span>Next 
            <span class="ni">&amp;#187;</span><span class="nt">&lt;/a&gt;</span>
          <span class="nt">&lt;/div&gt;</span>
      <span class="nt">&lt;/div&gt;</span>

...

    <span class="nt">&lt;/article&gt;</span>
  <span class="nt">&lt;/section&gt;</span>
<span class="nt">&lt;/body&gt;</span>
</code></pre></div></div> <p>Scraping this page is a two step process:</p> <ol> <li>First, grab each LEGO set by looking for the parts of the page that have the data we want.</li> <li>Then, for each set, grab the data we want from it by pulling the data out of the HTML tags.</li> </ol> <p><code class="language-plaintext highlighter-rouge">scrapy</code> grabs data based on <code class="language-plaintext highlighter-rouge">selectors</code> that you provide. Selectors are patterns we can use to find one or more elements on a page so we can then work with the data within the element. <code class="language-plaintext highlighter-rouge">scrapy</code> supports either CSS selectors or <a href="https://en.wikipedia.org/wiki/XPath">XPath selectors</a>.</p> <p>We’ll use CSS selectors for now since CSS is the easier option and a perfect fit for finding all the sets on the page. If you look at the HTML for the page, you’ll see that each set is specified with the class <code class="language-plaintext highlighter-rouge">set</code>. Since we’re looking for a class, we’d use <code class="language-plaintext highlighter-rouge">.set</code> for our CSS selector. All we have to do is pass that selector into the <code class="language-plaintext highlighter-rouge">response</code> object, like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">BrickSetSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="p">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">"brickset_spider"</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">'http://brickset.com/sets/year-2016'</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">SET_SELECTOR</span> <span class="o">=</span> <span class="s">'.set'</span>
        <span class="k">for</span> <span class="n">brickset</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="n">css</span><span class="p">(</span><span class="n">SET_SELECTOR</span><span class="p">):</span>
            <span class="k">pass</span>
</code></pre></div></div> <p>This code grabs all the sets on the page and loops over them to extract the data. Now let’s extract the data from those sets so we can display it.</p> <p>Another look at the source of the page we’re parsing tells us that the name of each set is stored within an <code class="language-plaintext highlighter-rouge">h1</code> tag for each set:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brickset.com/sets/year-2016
<span class="nt">&lt;h1&gt;</span>Brick Bank<span class="nt">&lt;/h1&gt;&lt;div</span> <span class="na">class=</span><span class="s">'tags floatleft'</span><span class="nt">&gt;&lt;a</span> <span class="na">href=</span><span class="s">'/sets/10251-1/Brick-Bank'</span><span class="nt">&gt;</span>10251-1<span class="nt">&lt;/a&gt;</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">brickset</code> object we’re looping over has its own <code class="language-plaintext highlighter-rouge">css</code> method, so we can pass in a selector to locate child elements. Modify your code as follows to locate the name of the set and display it:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">BrickSetSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="p">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">"brickset_spider"</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">'http://brickset.com/sets/year-2016'</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">SET_SELECTOR</span> <span class="o">=</span> <span class="s">'.set'</span>
        <span class="k">for</span> <span class="n">brickset</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="n">css</span><span class="p">(</span><span class="n">SET_SELECTOR</span><span class="p">):</span>

            <span class="n">NAME_SELECTOR</span> <span class="o">=</span> <span class="s">'h1 ::text'</span>
            <span class="k">yield</span> <span class="p">{</span>
                <span class="s">'name'</span><span class="p">:</span> <span class="n">brickset</span><span class="p">.</span><span class="n">css</span><span class="p">(</span><span class="n">NAME_SELECTOR</span><span class="p">).</span><span class="n">extract_first</span><span class="p">(),</span>
            <span class="p">}</span>
</code></pre></div></div> <p><em>Note: The trailing comma after extract_first() isn’t a typo. We’re going to add more to this section soon, so we’ve left the comma there to make adding to this section easier later.</em></p> <p>You’ll notice two things going on in this code:</p> <ul> <li>We append <code class="language-plaintext highlighter-rouge">::text</code> to our selector for the name. That’s a CSS pseudo-selector that fetches the text inside of the <code class="language-plaintext highlighter-rouge">a</code> tag rather than the tag itself.</li> <li>We call <code class="language-plaintext highlighter-rouge">extract_first()</code> on the object returned by <code class="language-plaintext highlighter-rouge">brickset.css(NAME_SELECTOR)</code> because we just want the first element that matches the selector. This gives us a string, rather than a list of elements.</li> </ul> <p>Save the file as <code class="language-plaintext highlighter-rouge">scrapers/scraper2.py</code> and run the scraper again:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">scrapy</span> <span class="n">runspider</span> <span class="n">scrapers</span><span class="o">/</span><span class="n">scraper2</span><span class="p">.</span><span class="n">py</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2021-10-21 11:30:48 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)
2021-10-21 11:30:48 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.9.7 (default, Sep 16 2021, 08:50:36) - [Clang 10.0.0 ], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 35.0.0, Platform macOS-10.16-x86_64-i386-64bit
2021-10-21 11:30:48 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2021-10-21 11:30:48 [scrapy.crawler] INFO: Overridden settings:
{'SPIDER_LOADER_WARN_ONLY': True}
2021-10-21 11:30:48 [scrapy.extensions.telnet] INFO: Telnet Password: 9f07aa540cf71937
2021-10-21 11:30:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2021-10-21 11:30:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-21 11:30:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-21 11:30:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2021-10-21 11:30:48 [scrapy.core.engine] INFO: Spider opened
2021-10-21 11:30:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-21 11:30:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-10-21 11:30:48 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://brickset.com/sets/year-2016&gt; (referer: None)
2021-10-21 11:30:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://brickset.com/sets/year-2016&gt;: HTTP status code is not handled or not allowed
2021-10-21 11:30:48 [scrapy.core.engine] INFO: Closing spider (finished)
2021-10-21 11:30:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 230,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2139,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 0.173081,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 10, 21, 15, 30, 48, 436068),
 'httpcompression/response_bytes': 3136,
 'httpcompression/response_count': 1,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/DEBUG': 1,
 'log_count/INFO': 11,
 'memusage/max': 226410496,
 'memusage/startup': 226410496,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2021, 10, 21, 15, 30, 48, 262987)}
2021-10-21 11:30:48 [scrapy.core.engine] INFO: Spider closed (finished)
</code></pre></div></div> <h2 id="step-3--crawling-multiple-pages"> <a href="#step-3--crawling-multiple-pages" class="anchor-heading" aria-labelledby="step-3--crawling-multiple-pages"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Step 3 — Crawling Multiple Pages </h2> <p>We’ve successfully extracted data from that initial page, but we’re not progressing past it to see the rest of the results. The whole point of a spider is to detect and traverse links to other pages and grab data from those pages too.</p> <p>You’ll notice that the top and bottom of each page has a little right carat (&gt;) that links to the next page of results. Here’s the HTML for that:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">&lt;!-- brickset.com/sets/year-2016 --&gt;</span>
<span class="nt">&lt;ul</span> <span class="na">class=</span><span class="s">"pagelength"</span><span class="nt">&gt;</span>

  ...

  <span class="nt">&lt;li</span> <span class="na">class=</span><span class="s">"next"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">"http://brickset.com/sets/year-2017/page-2"</span><span class="nt">&gt;</span><span class="ni">&amp;#8250;</span><span class="nt">&lt;/a&gt;</span>
  <span class="nt">&lt;/li&gt;</span>
  <span class="nt">&lt;li</span> <span class="na">class=</span><span class="s">"last"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">"http://brickset.com/sets/year-2016/page-32"</span><span class="nt">&gt;</span><span class="ni">&amp;#187;</span><span class="nt">&lt;/a&gt;</span>
  <span class="nt">&lt;/li&gt;</span>
<span class="nt">&lt;/ul&gt;</span>
</code></pre></div></div> <p>As you can see, there’s a <code class="language-plaintext highlighter-rouge">li</code> tag with the class of next, and inside that tag, there’s an a tag with a link to the next page. All we have to do is tell the scraper to follow that link if it exists.</p> <p>Modify your code as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">BrickSetSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="p">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">'brick_spider'</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">'http://brickset.com/sets/year-2016'</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">SET_SELECTOR</span> <span class="o">=</span> <span class="s">'.set'</span>
        <span class="k">for</span> <span class="n">brickset</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="n">css</span><span class="p">(</span><span class="n">SET_SELECTOR</span><span class="p">):</span>

            <span class="n">NAME_SELECTOR</span> <span class="o">=</span> <span class="s">'h1 ::text'</span>
            <span class="n">PIECES_SELECTOR</span> <span class="o">=</span> <span class="s">'.//dl[dt/text() = "Pieces"]/dd/a/text()'</span>
            <span class="n">MINIFIGS_SELECTOR</span> <span class="o">=</span> <span class="s">'.//dl[dt/text() = "Minifigs"]/dd[2]/a/text()'</span>
            <span class="n">IMAGE_SELECTOR</span> <span class="o">=</span> <span class="s">'img ::attr(src)'</span>
            <span class="k">yield</span> <span class="p">{</span>
                <span class="s">'name'</span><span class="p">:</span> <span class="n">brickset</span><span class="p">.</span><span class="n">css</span><span class="p">(</span><span class="n">NAME_SELECTOR</span><span class="p">).</span><span class="n">extract_first</span><span class="p">(),</span>
                <span class="s">'pieces'</span><span class="p">:</span> <span class="n">brickset</span><span class="p">.</span><span class="n">xpath</span><span class="p">(</span><span class="n">PIECES_SELECTOR</span><span class="p">).</span><span class="n">extract_first</span><span class="p">(),</span>
                <span class="s">'minifigs'</span><span class="p">:</span> <span class="n">brickset</span><span class="p">.</span><span class="n">xpath</span><span class="p">(</span><span class="n">MINIFIGS_SELECTOR</span><span class="p">).</span><span class="n">extract_first</span><span class="p">(),</span>
                <span class="s">'image'</span><span class="p">:</span> <span class="n">brickset</span><span class="p">.</span><span class="n">css</span><span class="p">(</span><span class="n">IMAGE_SELECTOR</span><span class="p">).</span><span class="n">extract_first</span><span class="p">(),</span>
            <span class="p">}</span>

        <span class="n">NEXT_PAGE_SELECTOR</span> <span class="o">=</span> <span class="s">'.next a ::attr(href)'</span>
        <span class="n">next_page</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">css</span><span class="p">(</span><span class="n">NEXT_PAGE_SELECTOR</span><span class="p">).</span><span class="n">extract_first</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">next_page</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="p">.</span><span class="n">Request</span><span class="p">(</span>
                <span class="n">response</span><span class="p">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">next_page</span><span class="p">),</span>
                <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">parse</span>
            <span class="p">)</span>
</code></pre></div></div> <p>First, we define a selector for the “next page” link, extract the first match, and check if it exists. The <code class="language-plaintext highlighter-rouge">scrapy.Request</code> is a value that we return saying “Hey, crawl this page”, and <code class="language-plaintext highlighter-rouge">callback=self.parse</code> says “once you’ve gotten the HTML from this page, pass it back to this method so we can parse it, extract the data, and find the next page.“</p> <p>This means that once we go to the next page, we’ll look for a link to the next page there, and on that page we’ll look for a link to the next page, and so on, until we don’t find a link for the next page. This is the key piece of web scraping: finding and following links. In this example, it’s very linear; one page has a link to the next page until we’ve hit the last page, But you could follow links to tags, or other search results, or any other URL you’d like.</p> <p>Now, if you save your code and run the spider again you’ll see that it doesn’t just stop once it iterates through the first page of sets. It keeps on going through all 779 matches on 23 pages! In the grand scheme of things it’s not a huge chunk of data, but now you know the process by which you automatically find new pages to scrape.</p> <p>Save this as <code class="language-plaintext highlighter-rouge">scrapers/scraper3.py</code> and run the file</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">scrapy</span> <span class="n">runspider</span> <span class="n">scrapers</span><span class="o">/</span><span class="n">scraper3</span><span class="p">.</span><span class="n">py</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2021-10-21 11:30:50 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)
2021-10-21 11:30:50 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.9.7 (default, Sep 16 2021, 08:50:36) - [Clang 10.0.0 ], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 35.0.0, Platform macOS-10.16-x86_64-i386-64bit
2021-10-21 11:30:50 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2021-10-21 11:30:50 [scrapy.crawler] INFO: Overridden settings:
{'SPIDER_LOADER_WARN_ONLY': True}
2021-10-21 11:30:50 [scrapy.extensions.telnet] INFO: Telnet Password: 49053a2ee67de37f
2021-10-21 11:30:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2021-10-21 11:30:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-10-21 11:30:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-10-21 11:30:50 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2021-10-21 11:30:50 [scrapy.core.engine] INFO: Spider opened
2021-10-21 11:30:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-10-21 11:30:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-10-21 11:30:51 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://brickset.com/sets/year-2016&gt; (referer: None)
2021-10-21 11:30:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://brickset.com/sets/year-2016&gt;: HTTP status code is not handled or not allowed
2021-10-21 11:30:51 [scrapy.core.engine] INFO: Closing spider (finished)
2021-10-21 11:30:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 230,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2138,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'elapsed_time_seconds': 0.185113,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 10, 21, 15, 30, 51, 160618),
 'httpcompression/response_bytes': 3136,
 'httpcompression/response_count': 1,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/DEBUG': 1,
 'log_count/INFO': 11,
 'memusage/max': 228081664,
 'memusage/startup': 228081664,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2021, 10, 21, 15, 30, 50, 975505)}
2021-10-21 11:30:51 [scrapy.core.engine] INFO: Spider closed (finished)
</code></pre></div></div> <h2 id="conclusion"> <a href="#conclusion" class="anchor-heading" aria-labelledby="conclusion"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Conclusion </h2> <p>In this tutorial you built a fully-functional spider that extracts data from web pages in less than thirty lines of code. That’s a great start, but there’s a lot of fun things you can do with this spider. Here are some ways you could expand the code you’ve written. They’ll give you some practice scraping data.</p> <ol> <li>Right now we’re only parsing results from 2016, as you might have guessed from the <code class="language-plaintext highlighter-rouge">2016</code> part of http://brickset.com/sets/year-2016 — how would you crawl results from other years?</li> <li>There’s a retail price included on most sets. How do you extract the data from that cell? How would you get a raw number out of it? <em>Hint: you’ll find the data in a <code class="language-plaintext highlighter-rouge">dt</code> just like the number of pieces and minifigs.</em></li> <li>Most of the results have tags that specify semantic data about the sets or their context. How do we crawl these, given that there are multiple tags for a single set?</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div> </div> </div> </div> </body> </html>
