{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Benefis of Unit Testing\n",
    "[8 benefits of unit testing](https://dzone.com/articles/top-8-benefits-of-unit-testing)\n",
    "\n",
    "The goal of unit testing is to segregate each part of the program and test that the individual parts are working correctly. \n",
    "1. It isolates the smallest piece of testable software from the remainder of the code and determines whether it behaves exactly as you expect. \n",
    "1. Unit testing has proven its value in that a large percentage of defects are identified during its use. \n",
    "1. It allows automation of the testing process, reduces difficulties of discovering errors contained in more complex pieces of the application, and enhances test coverage because attention is given to each unit.\n",
    "\n",
    "## 1. Makes the Process Agile\n",
    "One of the main benefits of unit testing is that it makes the coding process more Agile. When you add more and more features to a software, you sometimes need to change old design and code. However, changing already-tested code is both risky and costly. If we have unit tests in place, then we can proceed for refactoring confidently.\n",
    "\n",
    "Unit testing really goes hand-in-hand with agile programming of all flavors because it builds in tests that allow you to make changes more easily. In other words, unit tests facilitate safe refactoring. \n",
    "\n",
    "## 2. Quality of Code\n",
    "Unit testing improves the quality of the code. It identifies every defect that may have come up before code is sent further for integration testing. Writing tests before actual coding makes you think harder about the problem. It exposes the edge cases and makes you write better code. \n",
    "\n",
    "## 3. Finds Software Bugs Early\n",
    "Issues are found at an early stage. Since unit testing is carried out by developers who test individual code before integration, issues can be found very early and can be resolved then and there without impacting the other pieces of the code. This includes both bugs in the programmer’s implementation and flaws or missing parts of the specification for the unit.\n",
    "\n",
    "## 4. Facilitates Changes and Simplifies Integration\n",
    "Unit testing allows the programmer to refactor code or upgrade system libraries at a later date and make sure the module still works correctly. Unit tests detect changes that may break a design contract. They help with maintaining and changing the code.\n",
    "\n",
    "Unit testing reduces defects in the newly developed features or reduces bugs when changing the existing functionality. \n",
    "\n",
    "Unit testing verifies the accuracy of the each unit. Afterward, the units are integrated into an application by testing parts of the application via unit testing. Later testing of the application during the integration process is easier due to the verification of the individual units.\n",
    "\n",
    "## 5. Provides Documentation\n",
    "Unit testing provides documentation of the system. Developers looking to learn what functionality is provided by a unit and how to use it can look at the unit tests to gain a basic understanding of the unit’s interface (API).\n",
    "\n",
    "## 6. Debugging Process\n",
    "Unit testing helps simplify the debugging process. If a test fails, then only the latest changes made in the code need to be debugged.\n",
    "\n",
    "## 7. Design\n",
    "Writing the test first forces you to think through your design and what it must accomplish before you write the code. This not only keeps you focused; it makes you create better designs. Testing a piece of code forces you to define what that code is responsible for. If you can do this easily, that means the code’s responsibility is well-defined and therefore that it has high cohesion.\n",
    "\n",
    "## 8. Reduce Costs\n",
    "Since the bugs are found early, unit testing helps reduce the cost of bug fixes. Just imagine the cost of a bug found during the later stages of development, like during system testing or during acceptance testing. Of course, bugs detected earlier are easier to fix because bugs detected later are usually the result of many changes, and you don’t really know which one caused the bug. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started With Testing in Python\n",
    "[Getting Started With Testing in Python\n",
    "](https://realpython.com/python-testing/)\n",
    "\n",
    "## Automated vs. Manual Testing\n",
    "\n",
    "* To have a complete set of manual tests, all you need to do is make a list of all the features your application has, the different types of input it can accept, and the expected results. Now, every time you make a change to your code, you need to go through every single item on that list and check it.\n",
    "* Automated testing is the execution of your test plan (the parts of your application you want to test, the order in which you want to test them, and the expected responses) by a script instead of a human. Python already comes with a set of tools and libraries to help you create automated tests for your application. \n",
    "\n",
    "## Unit Tests vs. Integration Tests\n",
    "\n",
    "* Think of how you might test the lights on a car. You would turn on the lights (known as the test step) and go outside the car or ask a friend to check that the lights are on (known as the test assertion). Testing multiple components is known as integration testing.\n",
    "* Think of all the things that need to work correctly in order for a simple task to give the right result. These components are like the parts to your application, all of those classes, functions, and modules you’ve written.\n",
    "  \n",
    "You have just seen two types of tests:\n",
    "\n",
    "1. An integration test checks that components in your application operate with each other.\n",
    "1. A unit test checks a small component in your application.\n",
    "\n",
    "You can write both integration tests and unit tests in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To write a unit test for the built-in function sum(), you would check the output of sum() against a known output.\n",
    "assert sum([1, 2, 3]) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Should be 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_36949/1130538575.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# If the result from sum() is incorrect, this will fail with an AssertionError and the message \"Should be 6\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Should be 6\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Should be 6"
     ]
    }
   ],
   "source": [
    "# If the result from sum() is incorrect, this will fail with an AssertionError and the message \"Should be 6\".\n",
    "assert sum([1, 1, 1]) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instead of testing on the REPL, you’ll want to put this into a new Python file called test_sum.py and execute it again:\n",
    "def test_sum():\n",
    "    assert sum([1, 2, 3]) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything passed\n"
     ]
    }
   ],
   "source": [
    "test_sum()\n",
    "print(\"Everything passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_sum_tuple():\n",
    "    assert sum((1, 2, 2)) == 6, \"Should be 6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Should be 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_36949/2203313225.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_sum_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Everything passed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_36949/2526289347.py\u001b[0m in \u001b[0;36mtest_sum_tuple\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_sum_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Should be 6\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Should be 6"
     ]
    }
   ],
   "source": [
    "test_sum_tuple()\n",
    "print(\"Everything passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a Test Runner\n",
    "* [Pytest](https://docs.pytest.org/en/latest/)\n",
    "* [Getting Started with Pytest](https://docs.pytest.org/en/latest/getting-started.html)\n",
    "\n",
    "There are many test runners available for Python. The one built into the Python standard library is called unittest. In this tutorial, you will be using unittest test cases and the unittest test runner. The principles of unittest are easily portable to other frameworks. The three most popular test runners are:\n",
    "\n",
    "* unittest\n",
    "* nose or nose2\n",
    "* **pytest** *(we'll use pytest)*\n",
    "\n",
    "content of [test_sample.py](test_sample.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "def test_answer():\n",
    "    assert inc(3) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.7, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\n",
      "rootdir: /Users/bk/Source/data-focused-python/lectures/Week 01 - Language basics, Generating Data, Storing Data\n",
      "plugins: Faker-9.3.1, anyio-3.3.3\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_sample.py \u001b[31mF\u001b[0m\u001b[31m                                                         [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_answer __________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_answer\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m inc(\u001b[94m3\u001b[39;49;00m) == \u001b[94m5\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 4 == 5\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 4 = inc(3)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_sample.py\u001b[0m:7: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_sample.py::test_answer - assert 4 == 5\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.10s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_sample.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contents of [test_sysexit.py](test_sysexit.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# content of test_sysexit.py\n",
    "import pytest\n",
    "def f():\n",
    "    raise SystemExit(1)\n",
    "\n",
    "def test_mytest():\n",
    "    with pytest.raises(SystemExit):\n",
    "        f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                        [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Execute the test function with “quiet” reporting mode:\n",
    "!pytest -q test_sysexit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group multiple tests in a class\n",
    "\n",
    "Once you develop multiple tests, you may want to group them into a class. pytest makes it easy to create a class containing more than one test.\n",
    "\n",
    "pytest discovers all tests following its Conventions for Python test discovery, so it finds both test_ prefixed functions. There is no need to subclass anything. We can simply run the module by passing its filename:\n",
    "\n",
    "contentx of [test_class.py](test_class.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# content of test_class.py\n",
    "class TestClass(object):\n",
    "    def test_one(self):\n",
    "        x = \"this\"\n",
    "        assert 'h' in x\n",
    "\n",
    "    def test_two(self):\n",
    "        x = \"hello\"\n",
    "        assert hasattr(x, 'check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                       [100%]\u001b[0m\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________________ TestClass.test_two ______________________________\u001b[0m\n",
      "\n",
      "self = <test_class.TestClass object at 0x7ff03131ddf0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_two\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        x = \u001b[33m\"\u001b[39;49;00m\u001b[33mhello\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(x, \u001b[33m'\u001b[39;49;00m\u001b[33mcheck\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where False = hasattr('hello', 'check')\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_class.py\u001b[0m:9: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_class.py::TestClass::test_two - AssertionError: assert False\n",
      "\u001b[31m\u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -q test_class.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request a unique temporary directory for functional tests¶\n",
    "\n",
    "pytest provides Builtin fixtures/function arguments to request arbitrary resources, like a unique temporary directory:\n",
    "\n",
    "contents of [test_tmpdir.py](test_tmpdir.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# content of test_tmpdir.py\n",
    "def test_needsfiles(tmpdir):\n",
    "    print(tmpdir)\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.7, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\n",
      "rootdir: /Users/bk/Source/data-focused-python/lectures/Week 01 - Language basics, Generating Data, Storing Data\n",
      "plugins: Faker-9.3.1, anyio-3.3.3\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_tmpdir.py /private/var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/pytest-of-bk/pytest-17/test_needsfiles0\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -s test_tmpdir.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the tmp_path fixture which will provide a temporary directory unique to the test invocation, created in the base temporary directory.\n",
    "\n",
    "contents of [test_tmppath.py](test_tmppath.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# content of test_tmppath.py\n",
    "import os\n",
    "\n",
    "CONTENT = u\"content\"\n",
    "\n",
    "def test_create_file(tmp_path):\n",
    "    d = tmp_path / \"sub\"\n",
    "    d.mkdir()\n",
    "    p = d / \"hello.txt\"\n",
    "    p.write_text(CONTENT)\n",
    "    assert p.read_text() == CONTENT\n",
    "    assert len(list(tmp_path.iterdir())) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.7, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\n",
      "rootdir: /Users/bk/Source/data-focused-python/lectures/Week 01 - Language basics, Generating Data, Storing Data\n",
      "plugins: Faker-9.3.1, anyio-3.3.3\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_tmppath.py /private/var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/pytest-of-bk/pytest-16/test_create_file0/sub\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -s test_tmppath.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
