{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24457bb9-9669-48b7-bf01-cc912fc6adbf",
   "metadata": {},
   "source": [
    "# Testing for Performance Degradation Between Changes\n",
    "[Source](https://realpython.com/python-testing/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ff451-0808-4b17-b028-c547eb160e46",
   "metadata": {},
   "source": [
    "There are many ways to benchmark code in Python. The standard library provides the `timeit` module, which can time functions a number of times and give you the distribution. This example will execute `test()` 100 times and `print()` the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c787a2c6-df5a-4a6c-8d5a-dd721512989a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5682077080000454\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from functools import reduce\n",
    "    \n",
    "def add(numbers):\n",
    "    total = reduce(lambda x, y: x + y, numbers)\n",
    "    return total\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import timeit\n",
    "    numbers = [random.random() for i in range(100000)]\n",
    "    print(timeit.timeit(f\"add({numbers})\", setup=\"from __main__ import add\", number=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13730091-5721-4530-bf38-f5c62ebba04e",
   "metadata": {},
   "source": [
    "Another option, if you decided to use pytest as a test runner, is the pytest-benchmark plugin. This provides a pytest fixture called benchmark. You can pass benchmark() any callable, and it will log the timing of the callable to the results of pytest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c0865-9d95-4adc-9362-1a28b57c31a1",
   "metadata": {},
   "source": [
    "You can install pytest-benchmark from PyPI using pip:\n",
    "\n",
    "```bash\n",
    "pip install pytest-benchmark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aed9da-f73f-4924-a0a0-bc4261012203",
   "metadata": {},
   "source": [
    "Then, you can add a test that uses the fixture and passes the callable to be executed:\n",
    "\n",
    "```python\n",
    "def test_my_function(benchmark):\n",
    "    result = benchmark(test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97108b8-30ee-40bd-9424-d2663e464daa",
   "metadata": {},
   "source": [
    "More information is available at the [Documentation Website](https://pytest-benchmark.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb45a7e-9b6e-4f8e-bae6-2f9f06830f82",
   "metadata": {},
   "source": [
    "See the `benchmark` folder for a runnable example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca48c12-0c54-47f9-90f0-8b7d7d445ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
