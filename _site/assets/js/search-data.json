{"0": {
    "doc": "00 - Additional Links",
    "title": "Links",
    "content": " ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/00%20-%20Additional%20Links.html#links",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/00%20-%20Additional%20Links.html#links"
  },"1": {
    "doc": "00 - Additional Links",
    "title": "Python for Data Analysis, 2nd Edition",
    "content": ". | Python for Data Analysis, 2nd Edition Book | Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks | Chapter 3: Built-in Data Structures, Functions, and Files | Chapter 4: NumPy Basics: Arrays and Vectorized Computation | Chapter 5: Getting Started with pandas | Chapter 6: Data Loading, Storage, and File Formats | Chapter 7: Data Cleaning and Preparation | Chapter 8: Data Wrangling: Join, Combine, and Reshape | Chapter 9: Plotting and Visualization | Chapter 10: Data Aggregation and Group Operations | Chapter 11: Time Series | Chapter 12: Advanced pandas | Chapter 13: Introduction to Modeling Libraries in Python | Chapter 14: Data Analysis Examples | Appendix A: Advanced NumPy | . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/00%20-%20Additional%20Links.html#python-for-data-analysis-2nd-edition",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/00%20-%20Additional%20Links.html#python-for-data-analysis-2nd-edition"
  },"2": {
    "doc": "00 - Additional Links",
    "title": "00 - Additional Links",
    "content": " ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/00%20-%20Additional%20Links.html",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/00%20-%20Additional%20Links.html"
  },"3": {
    "doc": "01 - Decision Structures and Boolean Logic",
    "title": "if statement",
    "content": "if 2 &gt; 1: print(“2 is greater than 1”) . # boolean expressions 1 &gt; 2 . False . 'a' &gt; 'b' . False . 'a' &gt; 'A' . True . chr(65) . 'A' . ord('A') . 65 . ord('a') . 97 . balance = 119 payment = 20 . balance == 0 . False . payment != balance . True . if payment == balance: print('paid in full') else: print('you owe ' + str(balance - payment)) . you owe 99 . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/01%20-%20Decision%20Structures%20and%20Boolean%20Logic.html#if-statement",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/01%20-%20Decision%20Structures%20and%20Boolean%20Logic.html#if-statement"
  },"4": {
    "doc": "01 - Decision Structures and Boolean Logic",
    "title": "01 - Decision Structures and Boolean Logic",
    "content": "if condition: # statement # statement . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/01%20-%20Decision%20Structures%20and%20Boolean%20Logic.html",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/01%20-%20Decision%20Structures%20and%20Boolean%20Logic.html"
  },"5": {
    "doc": "01 - Generating Fake Data",
    "title": "Generating Fake Data",
    "content": "It’s often useful, especially in healthcare, to generate fake data in order to test your system while protecting the privacy of your production data sets. ## let's generate a fake person . person = {} . print(person) . {} . person['age'] = 28 . print(person) . {'age': 28} . # let's generate a random age import random person['age'] = random.randrange(100) print(person) . {'age': 2} . person['age'] = random.randrange(100) print(person) . {'age': 42} . # in healthcare kids and elderly have special coverage so let's exclude both groups person['age'] = random.randint(18, 65) print(person) . {'age': 32} . person['employment'] = 'Full Time' print(person) . {'age': 32, 'employment': 'Full Time'} . # let's set employment status employment_status = ['Full Time', 'Part Time', 'Contract', 'Seasonal', 'Unemployed', 'Retired'] person['employment'] = random.choice(employment_status) print(person) . {'age': 32, 'employment': 'Unemployed'} . person['employment'] = random.choice(employment_status) print(person) . {'age': 32, 'employment': 'Retired'} . person['employment'] = random.choices(employment_status, [.5, .2, .1, .1, .05, .05])[0] print(person) . {'age': 32, 'employment': 'Part Time'} . person['employment'] = random.choices(employment_status, [.5, .2, .1, .1, .05, .05])[0] print(person) . {'age': 32, 'employment': 'Part Time'} . def generate_employment(): employment_status = ['Full Time', 'Part Time', 'Contract', 'Seasonal', 'Unemployed', 'Retired'] employment = random.choices(employment_status, [.5, .2, .1, .1, .05, .05])[0] return employment . # let's construct some fake addresses def generate_address(): street_number = random.randint(1, 100) street_name= random.choice(['Main', 'Bluff', 'Federal']) city = random.choice(['Pittsburgh', 'Cleveland' ]) city_details = { 'Pittsburgh' : { 'zip': 15106, 'state': 'PA' }, 'Cleveland' : { 'zip': 44101, 'state': 'OH' } } state = city_details[city]['zip'] zip_code = city_details[city]['state'] address = f\"{street_number} {street_name}\\n{city}, {state} {zip_code}\" return address . address = generate_address() print(address) . 24 Federal Pittsburgh, 15106 PA . def generate_name(): first = ['Ben', 'Jen', 'Joan', 'John'] last = ['Jones', 'Smith', 'Doe'] return f\"{random.choice(first)} {random.choice(last)}\" . name = generate_name() print(name) . John Doe . def generate_person(): person = {} person['name'] = generate_name() person['address'] = generate_address() person['employment'] = generate_employment() return person . person = generate_person() print(person) . {'name': 'Ben Jones', 'address': '12 Bluff\\nCleveland, 44101 OH', 'employment': 'Seasonal'} . person = generate_person() print(person) . {'name': 'John Jones', 'address': '64 Federal\\nCleveland, 44101 OH', 'employment': 'Full Time'} . person = generate_person() print(person) . {'name': 'Ben Jones', 'address': '47 Federal\\nPittsburgh, 15106 PA', 'employment': 'Part Time'} . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/01%20-%20Generating%20Fake%20Data.html#generating-fake-data",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/01%20-%20Generating%20Fake%20Data.html#generating-fake-data"
  },"6": {
    "doc": "01 - Generating Fake Data",
    "title": "01 - Generating Fake Data",
    "content": " ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/01%20-%20Generating%20Fake%20Data.html",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/01%20-%20Generating%20Fake%20Data.html"
  },"7": {
    "doc": "01 - Getting Started",
    "title": "Getting Started",
    "content": "Welcome to Data Focused Python. In this guide we’ll walk through all the steps you need to get started with the course. Python has a robust community which is great, but it also means there’s going to be many options from python distribution to IDEs to libraries. There’s no right or wrong selection, only the one that works best for you. ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/01%20-%20Getting%20Started.html#getting-started",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/01%20-%20Getting%20Started.html#getting-started"
  },"8": {
    "doc": "01 - Getting Started",
    "title": "Required Software",
    "content": ". | Python 3.9 | Git | . ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/01%20-%20Getting%20Started.html#required-software",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/01%20-%20Getting%20Started.html#required-software"
  },"9": {
    "doc": "01 - Getting Started",
    "title": "Recommended",
    "content": ". | Anaconda Python | Pycharm or VSCode | . ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/01%20-%20Getting%20Started.html#recommended",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/01%20-%20Getting%20Started.html#recommended"
  },"10": {
    "doc": "01 - Getting Started",
    "title": "Environment Setup",
    "content": "Python is open source and as with many open source solutions the ecosystem can be overwhelming. If you stick with the recommended Anaconda and Pycharm setup you’ll avoid most of the complexity. However, for awareness we’ll review some of the most common tooling options used. | An Effective Python Environment: Making Yourself at Home | . ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/01%20-%20Getting%20Started.html#environment-setup",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/01%20-%20Getting%20Started.html#environment-setup"
  },"11": {
    "doc": "01 - Getting Started",
    "title": "Cloning the Course Materials",
    "content": "After you’re comfortable working with your Python IDE to create basic python programs, you’ll want to clone the course materials from GitHub. Before you can clone the repository, you’ll have to install git on your local machine. Git is a distributed version control system that’s great for managing projects with distributed team members. There’s a lot of great resources on how to use git. Here’s 2 of my favorite interactive tutorials: . | Try Git | Learn Git Branching | . We won’t be doing any branching in this course, but I think it’s important to know so I’ve included the link. Once you’ve installed git and are somewhat comfortable with the command line tool, you can read about how to Clone a Repository from GitHub. It’s as easy as: . git clone https://github.com/BrianKolowitz/data-focused-python.git . However, I personally like to Add SSH Keys to GitHub and clone using this command: . git clone git@github.com:BrianKolowitz/data-focused-python.git . ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/01%20-%20Getting%20Started.html#cloning-the-course-materials",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/01%20-%20Getting%20Started.html#cloning-the-course-materials"
  },"12": {
    "doc": "01 - Getting Started",
    "title": "01 - Getting Started",
    "content": " ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/01%20-%20Getting%20Started.html",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/01%20-%20Getting%20Started.html"
  },"13": {
    "doc": "01 - Pandas Introduction",
    "title": "Pandas Tutorial: DataFrames in Python",
    "content": "Source . Explore data analysis with Python. Pandas DataFrames make manipulating your data easy, from selecting or replacing columns and indices to reshaping your data. Pandas is a popular Python package for data science, and with good reason: it offers powerful, expressive and flexible data structures that make data manipulation and analysis easy, among many other things. The DataFrame is one of these structures. This tutorial covers Pandas DataFrames, from basic manipulations to advanced operations, by tackling 11 of the most popular questions so that you understand -and avoid- the doubts of the Pythonistas who have gone before you. Content . | How To Create a Pandas DataFrame | How To Select an Index or Column From a DataFrame | How To Add an Index, Row or Column to a DataFrame | How To Delete Indices, Rows or Columns From a DataFrame | How To Rename the Columns or Indices of a DataFrame | How To Format the Data in Your DataFrame | How To Create an Empty DataFrame | Does Pandas Recognize Dates When Importing Data? | When, Why and How You Should Reshape Your DataFrame | How To Iterate Over a DataFrame | How To Write a DataFrame to a File | . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/01%20-%20Pandas%20Introduction.html#pandas-tutorial-dataframes-in-python",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/01%20-%20Pandas%20Introduction.html#pandas-tutorial-dataframes-in-python"
  },"14": {
    "doc": "01 - Pandas Introduction",
    "title": "What Are Pandas Data Frames?",
    "content": "Before you start, let’s have a brief recap of what DataFrames are. Those who are familiar with R know the data frame as a way to store data in rectangular grids that can easily be overviewed. Each row of these grids corresponds to measurements or values of an instance, while each column is a vector containing data for a specific variable. This means that a data frame’s rows do not need to contain, but can contain, the same type of values: they can be numeric, character, logical, etc. Now, DataFrames in Python are very similar: they come with the Pandas library, and they are defined as a two-dimensional labeled data structures with columns of potentially different types. In general, you could say that the Pandas DataFrame consists of three main components: the data, the index, and the columns. Firstly, the DataFrame can contain data that is: . | a Pandas DataFrame | a Pandas Series: a one-dimensional labeled array capable of holding any data type with axis labels or index. An example of a Series object is one column from a DataFrame. | a NumPy ndarray, which can be a record or structured | a two-dimensional ndarray | dictionaries of one-dimensional ndarray’s, list’s, dictionarie’s or Series. | . Note the difference between np.ndarray and np.array(). The former is an actual data type, while the latter is a function to make arrays from other data structures. Structured arrays allow users to manipulate the data by named fields: in the example below, a structured array of three tuples is created. The first element of each tuple will be called foo and will be of type int, while the second element will be named bar and will be a float. import numpy as np import pandas as pd . # A structured array my_array = np.ones(3, dtype=([('foo', int), ('bar', float)])) # Print the structured array print(type(my_array['foo']), my_array['foo']) . &lt;class 'numpy.ndarray'&gt; [1 1 1] . Record arrays, on the other hand, expand the properties of structured arrays. They allow users to access fields of structured arrays by attribute rather than by index. You see below that the foo values are accessed in the r2 record array. # A record array my_array2 = my_array.view(np.recarray) # Print the record array print(type(my_array2.foo), my_array2.foo) . &lt;class 'numpy.ndarray'&gt; [1 1 1] . Besides data, you can also specify the index and column names for your DataFrame. The index, on the one hand, indicates the difference in rows, while the column names indicate the difference in columns. You will see later that these two components of the DataFrame will come in handy when you’re manipulating your data. Note that in this post, most of the times, the libraries that you need have already been loaded in. The Pandas library is usually imported under the alias pd, while the NumPy library is loaded as np. Remember that when you code in your own data science environment, you shouldn’t forget this import step, which you write just like this: . import numpy as np import pandas as pd . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/01%20-%20Pandas%20Introduction.html#what-are-pandas-data-frames",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/01%20-%20Pandas%20Introduction.html#what-are-pandas-data-frames"
  },"15": {
    "doc": "01 - Pandas Introduction",
    "title": "1. How To Create a Pandas DataFrame",
    "content": "Obviously, making your DataFrames is your first step in almost anything that you want to do when it comes to data munging in Python. Sometimes, you will want to start from scratch, but you can also convert other data structures, such as lists or NumPy arrays, to Pandas DataFrames. In this section, you’ll will only cover the latter. Among the many things that can serve as input to make a DataFrame, a NumPy ndarray is one of them. To make a data frame from a NumPy array, you can just pass it to the DataFrame() function in the data argument. columns = ['Col1', 'Col2'] index = ['Row1', 'Row2'] data = [[1, 2], [3, 4]] df = pd.DataFrame(data=data, index=index, columns=columns) df . | | Col1 | Col2 | . | Row1 | 1 | 2 | . | Row2 | 3 | 4 | . data = np.array([['','Col1','Col2'], ['Row1',1,2], ['Row2',3,4]]) df = pd.DataFrame(data=data[1:,1:], index=data[1:,0], columns=data[0,1:]) df . | | Col1 | Col2 | . | Row1 | 1 | 2 | . | Row2 | 3 | 4 | . Pay attention to how the code chunks above select elements from the NumPy array to construct the DataFrame: . | you first select the values that are contained in the lists that start with Row1 and Row2 | then you select the index or row numbers Row1 and Row2 and then the column names Col1 and Col2 | . Next, you also see that, in the chunk above, you printed out a small selection of the data. This works the same as subsetting 2D NumPy arrays: you first indicate the row that you want to look in for your data, then the column. Don’t forget that the indices start at 0! For data in the example above, you go and look in the rows at index 1 to end and you select all elements that come after index 1. As a result, you end up selecting 1, 2, 3 and 4. This approach to making DataFrames will be the same for all the structures that DataFrame() can take on as input. # Take a 2D array as input to your DataFrame my_2darray = np.array([[1, 2, 3], [4, 5, 6]]) print(my_2darray) . [[1 2 3] [4 5 6]] . # Take a dictionary as input to your DataFrame my_dict = { 1: ['1', '3'], 2: ['1', '2'], 3: ['2', '4']} my_df = pd.DataFrame(my_dict) my_df . | | 1 | 2 | 3 | . | 0 | 1 | 1 | 2 | . | 1 | 3 | 2 | 4 | . # Take a DataFrame as input to your DataFrame my_df = pd.DataFrame( data=[4,5,6,7], index=range(0,4), columns=['A']) my_df . | | A | . | 0 | 4 | . | 1 | 5 | . | 2 | 6 | . | 3 | 7 | . # Take a Series as input to your DataFrame my_series = pd.Series({ \"Belgium\":\"Brussels\", \"India\":\"New Delhi\", \"United Kingdom\":\"London\", \"United States\":\"Washington\"}) my_series . Belgium Brussels India New Delhi United Kingdom London United States Washington dtype: object . Note that the index of your Series (and DataFrame) contains the keys of the original dictionary, but that they are sorted: Belgium will be the index at 0, while United States will be the index at 3. After you have created your DataFrame, you might want to know a little bit more about it. You can use the shape property or the len() function in combination with the .index property: . df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6]])) # Use the `shape` property print(df.shape) . (2, 3) . # Or use the `len()` function with the `index` property print(len(df.index)) . 2 . These two options give you slightly different information on your DataFrame: . | the shape property will give you the dimensions of your DataFrame. That means that you will get to know the width and the height of your DataFrame. | the len() function, in combination with the index property, will only give you information on the height of your DataFrame. | . This all is totally not extraordinary, though, as you explicitly give in the index property. You could also use df[0].count() to get to know more about the height of your DataFrame, but this will exclude the NaN values (if there are any). That is why calling ```.count()`` on your DataFrame is not always the better option. If you want more information on your DataFrame columns, you can always execute list(df.columns.values). list(df.columns.values) . [0, 1, 2] . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/01%20-%20Pandas%20Introduction.html#1-how-to-create-a-pandas-dataframe",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/01%20-%20Pandas%20Introduction.html#1-how-to-create-a-pandas-dataframe"
  },"16": {
    "doc": "01 - Pandas Introduction",
    "title": "2. How To Select an Index or Column From a Pandas DataFrame",
    "content": "Before you start with adding, deleting and renaming the components of your DataFrame, you first need to know how you can select these elements. So, how do you do this? . Even though you might still remember how to do it from the previous section: selecting an index, column or value from your DataFrame isn’t that hard, quite the contrary. It’s similar to what you see in other languages (or packages!) that are used for data analysis. If you aren’t convinced, consider the following: In R, you use the [,] notation to access the data frame’s values. Now, let’s say you have a DataFrame like this one . def create_df(): vals = [[1, 4, 7], [2, 5, 8], [3, 6, 9]] d = { k:v for (k, v) in zip('ABC', vals)} df = pd.DataFrame(d) return df . df = create_df() df . | | A | B | C | . | 0 | 1 | 2 | 3 | . | 1 | 4 | 5 | 6 | . | 2 | 7 | 8 | 9 | . And you want to access the value that is at index 0, in column ‘A’. There are various options that exist to get your value 1 back: . # Using `iloc[]` print(df.iloc[0][0]) # Using `loc[]` print(df.loc[0]['A']) # Using `at[]` print(df.at[0,'A']) # Using `iat[]` print(df.iat[0,0]) . 1 1 1 1 . The most important ones to remember are, without a doubt, .loc[] and .iloc[]. The subtle differences between these two will be discussed in the next sections. Enough for now about selecting values from your DataFrame. What about selecting rows and columns? In that case, you would use: . # Use `iloc[]` to select row `0` print(df.iloc[0]) . A 1 B 2 C 3 Name: 0, dtype: int64 . # Use `loc[]` to select column `'A'` print(df.loc[:,'A']) . 0 1 1 4 2 7 Name: A, dtype: int64 . For now, it’s enough to know that you can either access the values by calling them by their label or by their position in the index or column. If you don’t see this, look again at the slight differences in the commands: one time, you see [0][0], the other time, you see [0,'A'] to retrieve your value 1. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/01%20-%20Pandas%20Introduction.html#2-how-to-select-an-index-or-column-from-a-pandas-dataframe",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/01%20-%20Pandas%20Introduction.html#2-how-to-select-an-index-or-column-from-a-pandas-dataframe"
  },"17": {
    "doc": "01 - Pandas Introduction",
    "title": "3. How To Add an Index, Row or Column to a Pandas DataFrame",
    "content": "Now that you have learned how to select a value from a DataFrame, it’s time to get to the real work and add an index, row or column to it! . Adding an Index to a DataFrame When you create a DataFrame, you have the option to add input to the index argument to make sure that you have the index that you desire. When you don’t specify this, your DataFrame will have, by default, a numerically valued index that starts with 0 and continues until the last row of your DataFrame. However, even when your index is specified for you automatically, you still have the power to re-use one of your columns and make it your index. You can easily do this by calling set_index() on your DataFrame. # Print out your DataFrame `df` to check it out df . | | A | B | C | . | 0 | 1 | 2 | 3 | . | 1 | 4 | 5 | 6 | . | 2 | 7 | 8 | 9 | . # Set 'C' as the index of your DataFrame df.set_index('C') . | | A | B | . | C | | | . | 3 | 1 | 2 | . | 6 | 4 | 5 | . | 9 | 7 | 8 | . Adding Rows to a DataFrame Before you can get to the solution, it’s first a good idea to grasp the concept of loc and how it differs from other indexing attributes such as .iloc[] and .ix[]: . | .loc[] works on labels of your index. This means that if you give in loc[2], you look for the values of your DataFrame that have an index labeled 2. | .iloc[] works on the positions in your index. This means that if you give in iloc[2], you look for the values of your DataFrame that are at index ’2`. | .ix[] is a more complex case: when the index is integer-based, you pass a label to .ix[] .ix[2] then means that you’re looking in your DataFrame for values that have an index labeled 2. This is just like .loc[]! However, if your index is not solely integer-based, ix will work with positions, just like .iloc[]. | . This all might seem very complicated. Let’s illustrate all of this with a small example: . df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), index=[2, 'A', 4], columns=[48, 49, 50]) df . | | 48 | 49 | 50 | . | 2 | 1 | 2 | 3 | . | A | 4 | 5 | 6 | . | 4 | 7 | 8 | 9 | . # Pass `2` to `loc` df.loc[2] . 48 1 49 2 50 3 Name: 2, dtype: int64 . # Pass `2` to `iloc` print(df.iloc[2]) . 48 7 49 8 50 9 Name: 4, dtype: int64 . # Pass `2` to `ix` print(df.ix[2]) . --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_23541/3838644476.py in &lt;module&gt; 1 # Pass `2` to `ix` ----&gt; 2 print(df.ix[2]) ~/opt/miniconda3/envs/cmu39/lib/python3.9/site-packages/pandas/core/generic.py in __getattr__(self, name) 5485 ): 5486 return self[name] -&gt; 5487 return object.__getattribute__(self, name) 5488 5489 def __setattr__(self, name: str, value) -&gt; None: AttributeError: 'DataFrame' object has no attribute 'ix' . Note that in this case you used an example of a DataFrame that is not solely integer-based as to make it easier for you to understand the differences. You clearly see that passing 2 to .loc[] or .iloc[]/.ix[] does not give back the same result! . | You know that .loc[] will go and look at the values that are at label 2. The result that you get back, will be 48 1 49 2 50 3 . | You also know that .iloc[] will go and look at the positions in the index. When you pass 2, you will get back: 48 7 49 8 50 9 . | Since the index doesn’t only contain integers, .ix[] will have the same behavior as iloc and look at the positions in the index. You will get back the same result as .iloc[]. | . Now that the difference between .iloc[], .loc[] and .ix[] is clear, you are ready to give adding rows to your DataFrame a go! . Tip: as a consequence of what you have just read, you understand now also that the general recommendation is that you use .loc to insert rows in your DataFrame. That is because if you would use df.ix[], you might try to reference a numerically valued index with the index value and accidentally overwrite an existing row of your DataFrame. You better avoid this! . Check out the difference once more in the DataFrame below: . df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), index= [2.5, 12.6, 4.8], columns=[48, 49, 50]) . # There's no index labeled `2`, so you will change the index at position `2` df.ix[2] = [60, 50, 40] print(df) . --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_23541/2080172866.py in &lt;module&gt; 1 # There's no index labeled `2`, so you will change the index at position `2` ----&gt; 2 df.ix[2] = [60, 50, 40] 3 print(df) ~/opt/miniconda3/envs/cmu39/lib/python3.9/site-packages/pandas/core/generic.py in __getattr__(self, name) 5485 ): 5486 return self[name] -&gt; 5487 return object.__getattribute__(self, name) 5488 5489 def __setattr__(self, name: str, value) -&gt; None: AttributeError: 'DataFrame' object has no attribute 'ix' . # This will make an index labeled `2` and add the new values df.loc[2] = [11, 12, 13] print(df) . 48 49 50 2.5 1 2 3 12.6 4 5 6 4.8 7 8 9 2.0 11 12 13 . Adding a Column to Your DataFrame . In some cases, you want to make your index part of your DataFrame. You can easily do this by taking a column from your DataFrame or by referring to a column that you haven’t made yet and assigning it to the .index property, just like this: . df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['A', 'B', 'C']) df . | | A | B | C | . | 0 | 1 | 2 | 3 | . | 1 | 4 | 5 | 6 | . | 2 | 7 | 8 | 9 | . # Use `.index` df['D'] = df.index # Print `df` print(df) . A B C D 0 1 2 3 0 1 4 5 6 1 2 7 8 9 2 . In other words, you tell your DataFrame that it should take column A as its index. However, if you want to append columns to your DataFrame, you could also follow the same approach as when you would add an index to your DataFrame: you use .loc[] or .iloc[]. In this case, you add a Series to an existing DataFrame with the help of .loc[]: . # Study the DataFrame `df` df . | | A | B | C | D | . | 0 | 1 | 2 | 3 | 0 | . | 1 | 4 | 5 | 6 | 1 | . | 2 | 7 | 8 | 9 | 2 | . # Append a column to `df` df.loc[:, 'E'] = pd.Series(['5', '6', '7'], index=df.index) # Print out `df` again to see the changes df . | | A | B | C | D | E | . | 0 | 1 | 2 | 3 | 0 | 5 | . | 1 | 4 | 5 | 6 | 1 | 6 | . | 2 | 7 | 8 | 9 | 2 | 7 | . Remember a Series object is much like a column of a DataFrame; That explains why you can easily add a Series to an existing DataFrame. Note also that the observation that was made earlier about .loc[] still stays valid, also when you’re adding columns to your DataFrame! . Resetting the Index of Your DataFrame . When your index doesn’t look entirely the way you want it to, you can opt to reset it. You can easily do this with .reset_index(). However, you should still watch out, as you can pass several arguments that can make or break the success of your reset: . # Check out the weird index of your dataframe df = df.set_index('C') df . | | A | B | D | E | . | C | | | | | . | 3 | 1 | 2 | 0 | 5 | . | 6 | 4 | 5 | 1 | 6 | . | 9 | 7 | 8 | 2 | 7 | . # Use `reset_index()` to reset the values. df = df.reset_index(level=0, drop=True) df . | | A | B | D | E | . | 0 | 1 | 2 | 0 | 5 | . | 1 | 4 | 5 | 1 | 6 | . | 2 | 7 | 8 | 2 | 7 | . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/01%20-%20Pandas%20Introduction.html#3-how-to-add-an-index-row-or-column-to-a-pandas-dataframe",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/01%20-%20Pandas%20Introduction.html#3-how-to-add-an-index-row-or-column-to-a-pandas-dataframe"
  },"18": {
    "doc": "01 - Pandas Introduction",
    "title": "4. How to Delete Indices, Rows or Columns From a Pandas Data Frame",
    "content": "Now that you have seen how to select and add indices, rows, and columns to your DataFrame, it’s time to consider another use case: removing these three from your data structure. Deleting an Index from Your DataFrame . If you want to remove the index from your DataFrame, you should reconsider because DataFrames and Series always have an index. However, what you can do is, for example: . | resetting the index of your DataFrame (go back to the previous section to see how it is done) | remove the index name, if there is any, by executing del df.index.name | remove duplicate index values by resetting the index, dropping the duplicates of the index column that has been added to your DataFrame and reinstating that duplicateless column again as the index: | . df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [40, 50, 60], [23, 35, 37]]), index= [2.5, 12.6, 4.8, 4.8, 2.5], columns=[48, 49, 50]) print(df) . 48 49 50 2.5 1 2 3 12.6 4 5 6 4.8 7 8 9 4.8 40 50 60 2.5 23 35 37 . x = df.reset_index().drop_duplicates(subset='index', keep='last').set_index('index') print(x) . 48 49 50 index 12.6 4 5 6 4.8 40 50 60 2.5 23 35 37 . Deleting a Column from Your DataFrame . To get rid of (a selection of) columns from your DataFrame, you can use the drop() method: . # Check out the DataFrame `df` df = create_df() print(df) . A B C 0 1 2 3 1 4 5 6 2 7 8 9 . # Drop the column with label 'A' df.drop('A', axis=1, inplace=True) print(df) . B C 0 2 3 1 5 6 2 8 9 . # Drop the column at position 1 df.drop(df.columns[[1]], axis=1) print(df) . B C 0 2 3 1 5 6 2 8 9 . You might think now: well, this is not so straightforward; There are some extra arguments that are passed to the drop() method! . | The axis argument is either 0 when it indicates rows and 1 when it is used to drop columns. | You can set inplace to True to delete the column without having to reassign the DataFrame. | . Removing a Row from Your DataFrame . You can remove duplicate rows from your DataFrame by executing df.drop_duplicates(). You can also remove rows from your DataFrame, taking into account only the duplicate values that exist in one column. # Check out your DataFrame `df` df = create_df() df = df.append({ 'A': 1, 'B': 2, 'C': 3}, ignore_index=True) print(df) . A B C 0 1 2 3 1 4 5 6 2 7 8 9 3 1 2 3 . # Drop the duplicates in `df` x = df.drop_duplicates(subset='A', keep='last') print(x) . A B C 1 4 5 6 2 7 8 9 3 1 2 3 . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/01%20-%20Pandas%20Introduction.html#4-how-to-delete-indices-rows-or-columns-from-a-pandas-data-frame",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/01%20-%20Pandas%20Introduction.html#4-how-to-delete-indices-rows-or-columns-from-a-pandas-data-frame"
  },"19": {
    "doc": "01 - Pandas Introduction",
    "title": "01 - Pandas Introduction",
    "content": " ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/01%20-%20Pandas%20Introduction.html",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/01%20-%20Pandas%20Introduction.html"
  },"20": {
    "doc": "01 - Python Classes",
    "title": "Python Classes",
    "content": "Object-Oriented Programming (OOP) in Python . ",
    "url": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#python-classes",
    "relUrl": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#python-classes"
  },"21": {
    "doc": "01 - Python Classes",
    "title": "Classes in Python",
    "content": "Focusing first on the data, each thing or object is an instance of some class. Classes are used to create new user-defined data structures that contain arbitrary information about something. In the case of an person, we could create a Person() class to track properties about the Person like the name and age. It’s important to note that a class just provides structure—it’s a blueprint for how something should be defined, but it doesn’t actually provide any real content itself. The Person() class may specify that the name and age are necessary for defining an person, but it will not actually state what a specific person’s name or age is. It may help to think of a class as an idea for how something should be defined. ",
    "url": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#classes-in-python",
    "relUrl": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#classes-in-python"
  },"22": {
    "doc": "01 - Python Classes",
    "title": "Python Objects (Instances)",
    "content": "While the class is the blueprint, an instance is a copy of the class with actual values, literally an object belonging to a specific class. It’s not an idea anymore; it’s an actual person, like a person named Bill who’s twenty years old. Put another way, a class is like a form or questionnaire. It defines the needed information. After you fill out the form, your specific copy is an instance of the class; it contains actual information relevant to you. You can fill out multiple copies to create many different instances, but without the form as a guide, you would be lost, not knowing what information is required. Thus, before you can create individual instances of an object, we must first specify what is needed by defining a class. ",
    "url": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#python-objects-instances",
    "relUrl": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#python-objects-instances"
  },"23": {
    "doc": "01 - Python Classes",
    "title": "How To Define a Class in Python",
    "content": "class Person: pass . ",
    "url": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#how-to-define-a-class-in-python",
    "relUrl": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#how-to-define-a-class-in-python"
  },"24": {
    "doc": "01 - Python Classes",
    "title": "Instance Attributes",
    "content": "All classes create objects, and all objects contain characteristics called attributes (referred to as properties in the opening paragraph). Use the __init__() method to initialize (e.g., specify) an object’s initial attributes by giving them their default value (or state). This method must have at least one argument as well as the self variable, which refers to the object itself (e.g., Person). class Person: # Initializer / Instance Attributes def __init__(self, name, age): self.name = name self.age = age . In the case of our Person() class: . | each person has a specific name and age, which is obviously important to know for when you start actually creating different persons. Remember: the class is just for defining the Person, not actually creating instances of individual persons with specific names and ages; we’ll get to that shortly. | Similarly, the self variable is also an instance of the class. Since instances of a class have varying values we could state Person.name = name rather than self.name = name. But since not all persons share the same name, we need to be able to assign different values to different instances. Hence the need for the special self variable, which will help to keep track of individual instances of each class. | . Note: You will never have to call the __init__() method; it gets called automatically when you create a new ‘Person’ instance. ",
    "url": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#instance-attributes",
    "relUrl": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#instance-attributes"
  },"25": {
    "doc": "01 - Python Classes",
    "title": "Class Attributes",
    "content": "While instance attributes are specific to each object, class attributes are the same for all instances—which in this case is all persons. So while each person has a unique name and age, every person is a homo sapien. class Person: # Class Attribute species = 'homo sapien' # Initializer / Instance Attributes def __init__(self, name, age): self.name = name self.age = age . ",
    "url": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#class-attributes",
    "relUrl": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#class-attributes"
  },"26": {
    "doc": "01 - Python Classes",
    "title": "Instantiating Objects",
    "content": "Instantiating is a fancy term for creating a new, unique instance of a class. class Animal: pass . a = Animal() print('a is type', type(a), 'value', a) b = Animal() print('b is type', type(b), 'value', b) c = b . a is type &lt;class '__main__.Animal'&gt; value &lt;__main__.Animal object at 0x7ff4b06b02b0&gt; b is type &lt;class '__main__.Animal'&gt; value &lt;__main__.Animal object at 0x7ff4b06b0a30&gt; . a == b . False . print(a == c) print(b == c) . False True . # Instantiate the Person object ben = Person(\"Ben\", 37) geno = Person(\"Geno\", 32) # Access the instance attributes print(f\"{ben.name} is {ben.age} and {geno.name} is {geno.age}.\") # Is Ben a mammal? if ben.species == \"homo sapien\": print(f\"{ben.name} is a {ben.species}!\") . Ben is 37 and Geno is 32. Ben is a homo sapien! . ",
    "url": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#instantiating-objects",
    "relUrl": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#instantiating-objects"
  },"27": {
    "doc": "01 - Python Classes",
    "title": "Instance Methods",
    "content": "Instance methods are defined inside a class and are used to get the contents of an instance. They can also be used to perform operations with the attributes of our objects. Like the __init__ method, the first argument is always self: . class Dog: # Class Attribute species = 'mammal' # Initializer / Instance Attributes def __init__(self, name, age): self.name = name self.age = age # instance method def description(self): return \"{} is {} years old\".format(self.name, self.age) # instance method def speak(self, sound): return \"{} says {}\".format(self.name, sound) . # Instantiate the Dog object mikey = Dog(\"Mikey\", 6) . # call our instance methods print(mikey.description()) print(mikey.speak(\"Gruff Gruff\")) . Mikey is 6 years old Mikey says Gruff Gruff . Modifying Attributes . You can change the value of attributes based on some behavior: . class Email: def __init__(self): self.is_sent = False def send_email(self): self.is_sent = True . my_email = Email() my_email.is_sent . False . my_email.send_email() my_email.is_sent . True . ",
    "url": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#instance-methods",
    "relUrl": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#instance-methods"
  },"28": {
    "doc": "01 - Python Classes",
    "title": "Python Object Inheritance",
    "content": "Inheritance is the process by which one class takes on the attributes and methods of another. Newly formed classes are called child classes, and the classes that child classes are derived from are called parent classes. It’s important to note that child classes override or extend the functionality (e.g., attributes and behaviors) of parent classes. In other words, child classes inherit all of the parent’s attributes and behaviors but can also specify different behavior to follow. The most basic type of class is an object, which generally all other classes inherit as their parent. ",
    "url": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#python-object-inheritance",
    "relUrl": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#python-object-inheritance"
  },"29": {
    "doc": "01 - Python Classes",
    "title": "Dog Park Example",
    "content": "Let’s pretend that we’re at a dog park. There are multiple Dog objects engaging in Dog behaviors, each with different attributes. In regular-speak that means some dogs are running, while some are stretching and some are just watching other dogs. Furthermore, each dog has been named by its owner and, since each dog is living and breathing, each ages. class Dog: def __init__(self, breed): self.breed = breed . spencer = Dog(\"German Shepard\") spencer.breed . 'German Shepard' . sara = Dog(\"Boston Terrier\") sara.breed . 'Boston Terrier' . Extending the Functionality of a Parent Class . # Parent class class Dog: # Class attribute species = 'mammal' # Initializer / Instance attributes def __init__(self, name, age): self.name = name self.age = age # instance method def description(self): return \"{} is {} years old\".format(self.name, self.age) # instance method def speak(self, sound): return \"{} says {}\".format(self.name, sound) . # Child class (inherits from Dog class) class RussellTerrier(Dog): def run(self, speed): return \"{} runs {}\".format(self.name, speed) . # Child class (inherits from Dog class) class Bulldog(Dog): def run(self, speed): return \"{} runs {}\".format(self.name, speed) . # Child classes inherit attributes and # behaviors from the parent class jim = Bulldog(\"Jim\", 12) print(jim.description()) . Jim is 12 years old . # Child classes have specific attributes # and behaviors as well print(jim.run(\"slowly\")) . Jim runs slowly . ",
    "url": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#dog-park-example",
    "relUrl": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#dog-park-example"
  },"30": {
    "doc": "01 - Python Classes",
    "title": "Parent vs. Child Classes",
    "content": "The isinstance() function is used to determine if an instance is also an instance of a certain parent class. # Child classes inherit attributes and # behaviors from the parent class jim = Bulldog(\"Jim\", 12) # Is jim an instance of Dog()? print(isinstance(jim, Dog)) . True . print(isinstance(jim, Bulldog)) . True . print(isinstance(jim, RussellTerrier)) . False . ",
    "url": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#parent-vs-child-classes",
    "relUrl": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#parent-vs-child-classes"
  },"31": {
    "doc": "01 - Python Classes",
    "title": "Overriding the Functionality of a Parent Class",
    "content": "Remember that child classes can also override attributes and behaviors from the parent class. For examples: . class FrenchBulldog(Bulldog): species = 'french bulldog' . sleepy = FrenchBulldog('sleepy', 2) . print(sleepy.species) . french bulldog . print(isinstance(sleepy, Bulldog)) . True . print(isinstance(sleepy, FrenchBulldog)) . True . ",
    "url": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#overriding-the-functionality-of-a-parent-class",
    "relUrl": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html#overriding-the-functionality-of-a-parent-class"
  },"32": {
    "doc": "01 - Python Classes",
    "title": "01 - Python Classes",
    "content": " ",
    "url": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html",
    "relUrl": "/lectures/Topic%2007%20-%20Classes/01%20-%20Python%20Classes.html"
  },"33": {
    "doc": "01. Introduction to Web Scraping",
    "title": "Introduction to web scraping with python",
    "content": "source . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/01.%20Introduction%20to%20Web%20Scraping.html#introduction-to-web-scraping-with-python",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/01.%20Introduction%20to%20Web%20Scraping.html#introduction-to-web-scraping-with-python"
  },"34": {
    "doc": "01. Introduction to Web Scraping",
    "title": "What is web scraping?",
    "content": "This is the process of extracting information from a webpage by taking advantage of patterns in the web page’s underlying code. We can use web scraping to gather unstructured data from the internet, process it and store it in a structured format. In this walkthrough, we’ll be storing our data in a JSON file. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/01.%20Introduction%20to%20Web%20Scraping.html#what-is-web-scraping",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/01.%20Introduction%20to%20Web%20Scraping.html#what-is-web-scraping"
  },"35": {
    "doc": "01. Introduction to Web Scraping",
    "title": "Alternatives to web scraping",
    "content": "Though web scraping is a useful tool in extracting data from a website, it’s not the only means to achieve this task. Before starting to web scrape, find out if the page you seek to extract data from provides an API. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/01.%20Introduction%20to%20Web%20Scraping.html#alternatives-to-web-scraping",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/01.%20Introduction%20to%20Web%20Scraping.html#alternatives-to-web-scraping"
  },"36": {
    "doc": "01. Introduction to Web Scraping",
    "title": "robots.txt file",
    "content": "Ensure that you check the robots.txt file of a website before making your scrapper. This file tells if the website allows scraping or if they do not. To check for the file, simply type the base URL followed by “/robots.txt” An example is, “mysite.com/robots.txt”. For more about robots.txt files click here. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/01.%20Introduction%20to%20Web%20Scraping.html#robotstxt-file",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/01.%20Introduction%20to%20Web%20Scraping.html#robotstxt-file"
  },"37": {
    "doc": "01. Introduction to Web Scraping",
    "title": "Getting started",
    "content": "In this tutorial, we’ll be extracting data from books to scrape which you can use to practise your web scraping. We’ll extract the title, rating, link to more information about the book and the cover image of the book. Code can be found on GitHub. Importing libraries . The python libraries perform the following tasks. | requests - will be used to make Http requests to the webpage. | json - we’ll use this to store the extracted information to a JSON file. | BeautifulSoup - for parsing HTML. | . import requests import json from bs4 import BeautifulSoup . walkthrough . We’re initializing three variables here. | header-HTTP headers provide additional parameters to HTTP transactions. By sending the appropriate HTTP headers, one can access the response data in a different format. | base_url - is the webpage we want to scrape since we’ll be needing the URL quite often, it’s good to have a single initialization and reuse this variable going forward. | r - this is the response object returned by the get method. Here, we pass the base_url and header as parameters. | . header = {'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'} base_url = \"http://books.toscrape.com/\" r = requests.get(base_url, headers=header) . To ensure our scraper runs when the http response is ok we’ll use the if statement as a check. The number 200 is the status code for Ok. To get a list of all codes and their meanings check out this resource. We’ll then parse the response object using the BeautifulSoup method and store the new object to a variable called soup. if r.status_code == 200: soup = BeautifulSoup(r.text, 'html.parser') books = soup.find_all('li',attrs={\"class\":\"col-xs-6 col-sm-4 col-md-3 col-lg-3\"}) result=[] for book in books: title=book.find('h3').text link=base_url +book.find('a')['href'] stars = str(len(book.find_all('i',attrs= {\"class\":\"icon-star\"}))) + \" out of 5\" price=\"$\"+book.find('p',attrs={'class':'price_color'}).text[2:] picture = base_url + book.find('img')['src'] single ={'title':title,'stars':stars,'price':price,'link':link,'picture':picture} result.append(single) with open('books.json','w') as f: json.dump(result,f,indent=4) else: print(r.status_code) . import pandas as pd df = pd.read_json('books.json') df.head() . | | title | stars | price | link | picture | . | 0 | A Light in the ... | 5 out of 5 | $51.77 | http://books.toscrape.com/catalogue/a-light-in... | http://books.toscrape.com/media/cache/2c/da/2c... | . | 1 | Tipping the Velvet | 5 out of 5 | $53.74 | http://books.toscrape.com/catalogue/tipping-th... | http://books.toscrape.com/media/cache/26/0c/26... | . | 2 | Soumission | 5 out of 5 | $50.10 | http://books.toscrape.com/catalogue/soumission... | http://books.toscrape.com/media/cache/3e/ef/3e... | . | 3 | Sharp Objects | 5 out of 5 | $47.82 | http://books.toscrape.com/catalogue/sharp-obje... | http://books.toscrape.com/media/cache/32/51/32... | . | 4 | Sapiens: A Brief History ... | 5 out of 5 | $54.23 | http://books.toscrape.com/catalogue/sapiens-a-... | http://books.toscrape.com/media/cache/be/a5/be... | . Let’s take a look at a single record from our webpage to identify the patterns. Once we can see the page, we’ll loop through every record in the page as they contain similar traits. From the image above, we’ll notice that all books are contained within a list item with the class. col-xs-6 col-sm-4 col-md-3 col-lg-3 . By using the find_all() method, we can find all references of this HTML tag in the webpage. we pass the tag as the first argument and then using the attrs argument which takes in a python dictionary, we can specify attributes of the HTML tag selected. In this case, it was a class indicated above, but you can even use id as an attribute. Store the result in a variable, I chose the name books. title = book.find('h3').text link = base_url + book.find('a')['href'] . If we observe keenly, we’ll notice that each of the elements we want to extract is nested within the list item tag are all contained in similar tags, in the example above. The title of the book is between h3 tags. The find() method returns the first matching tag. text will simply return any text found within the tags specified. For the anchor tags, we’ll be extracting the hyper reference link. As opposed to h3 tag, the href element is within anchor tags in HTML. Like so: . &lt;a href=\"somelink.com\"&gt;&lt;/a&gt; . In this case, the returned object will behave like a dictionary where we have a . dictionary_name[key] . We do this iteratively for all the values we seek to extract because we are taking advantage of the pattern in the underlying code of the webpage. Hence the use of the python for loop. The extracted elements are then stored in respective variables which we’ll put in a dictionary. With this information, we can then comfortably append the dictionary object to the initialized result list set before our for loop. single ={'title':title,'stars':stars,'price':price,'link':link,'picture':picture} result.append(single) with open('books2.json','w') as f: json.dump(result,f,indent=4) . Finally, store the python list in a JSON file by the name “books.json” with an indent of 4 for readability purposes. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/01.%20Introduction%20to%20Web%20Scraping.html#getting-started",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/01.%20Introduction%20to%20Web%20Scraping.html#getting-started"
  },"38": {
    "doc": "01. Introduction to Web Scraping",
    "title": "01. Introduction to Web Scraping",
    "content": " ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/01.%20Introduction%20to%20Web%20Scraping.html",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/01.%20Introduction%20to%20Web%20Scraping.html"
  },"39": {
    "doc": "01.a - Cleaning and Transforming Data",
    "title": "01.a - Cleaning and Transforming Data",
    "content": "import csv import json from pprint import pprint . with open('../data/csv/patients.csv') as f: reader = csv.reader(f) header = next(reader, None) rows = [row for row in reader] print(header) print(rows[0]) . ['Id', 'BIRTHDATE', 'DEATHDATE', 'SSN', 'DRIVERS', 'PASSPORT', 'PREFIX', 'FIRST', 'LAST', 'SUFFIX', 'MAIDEN', 'MARITAL', 'RACE', 'ETHNICITY', 'GENDER', 'BIRTHPLACE', 'ADDRESS', 'CITY', 'STATE', 'ZIP'] ['3287bb9c-e395-4146-8dd5-1fc3d887d220', '2015-07-12', '', '999-82-9751', '', '', '', 'Victor265', 'Kilback373', '', '', '', 'white', 'polish', 'M', 'Newton Massachusetts US', '657 Heathcote Divide', 'Fitchburg', 'Massachusetts', '01420'] . import datetime from datetime import date, timedelta from dateutil import parser def get_age(birth_date): if isinstance(birth_date, str): birth_date = parser.parse(birth_date).date() age = (date.today() - birth_date) // timedelta(days=365) return age . date.today() . datetime.date(2021, 10, 21) . datetime.datetime.now() . datetime.datetime(2021, 10, 21, 11, 4, 20, 310225) . print(get_age('2011-09-10')) . 10 . print(get_age('2014-01-10')) . 7 . ages = [get_age(row[1]) for row in rows] print(ages) . [6, 18, 13, 17, 8, 9, 55, 84, 14, 20, 53, 23, 18, 64, 11, 39, 34, 5, 82, 19, 18, 15, 26, 28, 29, 8, 71, 32, 82, 41, 51, 17, 40, 53, 60, 65, 11, 21, 32, 52, 3, 52, 82, 30, 69, 24, 99, 12, 51, 99, 27, 76, 78, 29, 16, 69, 34, 78, 58, 76, 99, 65, 70, 60, 50, 32, 4, 78, 65, 48, 3, 66, 77, 99, 25, 18, 47, 79, 99, 32, 10, 74, 61, 99, 62, 59, 67, 2, 50, 99, 3, 70, 61, 62, 43, 18, 71, 29, 99, 52, 71, 26, 28, 56, 32, 69, 23, 31, 46, 99, 106, 30, 21, 67, 67, 106, 99, 106, 99, 106, 106, 99, 106, 99, 99, 99, 99, 90, 90] . patients = [ {'birth_date': row[1], 'age': get_age(row[1])} for row in rows ] pprint(patients) . [{'age': 6, 'birth_date': '2015-07-12'}, {'age': 18, 'birth_date': '2003-04-15'}, {'age': 13, 'birth_date': '2008-06-26'}, {'age': 17, 'birth_date': '2004-06-22'}, {'age': 8, 'birth_date': '2012-12-12'}, {'age': 9, 'birth_date': '2012-10-04'}, {'age': 55, 'birth_date': '1966-10-02'}, {'age': 84, 'birth_date': '1937-04-18'}, {'age': 14, 'birth_date': '2007-01-01'}, {'age': 20, 'birth_date': '2001-09-12'}, {'age': 53, 'birth_date': '1967-11-05'}, {'age': 23, 'birth_date': '1998-05-27'}, {'age': 18, 'birth_date': '2003-06-22'}, {'age': 64, 'birth_date': '1957-02-03'}, {'age': 11, 'birth_date': '2009-12-15'}, {'age': 39, 'birth_date': '1981-11-24'}, {'age': 34, 'birth_date': '1987-04-23'}, {'age': 5, 'birth_date': '2016-02-26'}, {'age': 82, 'birth_date': '1939-05-17'}, {'age': 19, 'birth_date': '2002-05-04'}, {'age': 18, 'birth_date': '2003-07-13'}, {'age': 15, 'birth_date': '2006-09-05'}, {'age': 26, 'birth_date': '1995-04-13'}, {'age': 28, 'birth_date': '1993-08-19'}, {'age': 29, 'birth_date': '1992-02-09'}, {'age': 8, 'birth_date': '2013-09-10'}, {'age': 71, 'birth_date': '1950-02-06'}, {'age': 32, 'birth_date': '1989-05-19'}, {'age': 82, 'birth_date': '1939-05-17'}, {'age': 41, 'birth_date': '1980-08-01'}, {'age': 51, 'birth_date': '1970-10-17'}, {'age': 17, 'birth_date': '2004-09-02'}, {'age': 40, 'birth_date': '1981-09-17'}, {'age': 53, 'birth_date': '1967-11-06'}, {'age': 60, 'birth_date': '1961-07-15'}, {'age': 65, 'birth_date': '1956-04-05'}, {'age': 11, 'birth_date': '2010-04-02'}, {'age': 21, 'birth_date': '2000-09-13'}, {'age': 32, 'birth_date': '1989-04-14'}, {'age': 52, 'birth_date': '1969-08-30'}, {'age': 3, 'birth_date': '2018-02-15'}, {'age': 52, 'birth_date': '1968-11-10'}, {'age': 82, 'birth_date': '1939-05-17'}, {'age': 30, 'birth_date': '1991-04-14'}, {'age': 69, 'birth_date': '1951-12-13'}, {'age': 24, 'birth_date': '1997-09-25'}, {'age': 99, 'birth_date': '1922-04-03'}, {'age': 12, 'birth_date': '2009-06-25'}, {'age': 51, 'birth_date': '1970-01-22'}, {'age': 99, 'birth_date': '1922-04-03'}, {'age': 27, 'birth_date': '1994-10-02'}, {'age': 76, 'birth_date': '1945-05-24'}, {'age': 78, 'birth_date': '1943-02-09'}, {'age': 29, 'birth_date': '1992-01-30'}, {'age': 16, 'birth_date': '2004-11-05'}, {'age': 69, 'birth_date': '1951-12-13'}, {'age': 34, 'birth_date': '1987-04-26'}, {'age': 78, 'birth_date': '1943-02-09'}, {'age': 58, 'birth_date': '1963-03-09'}, {'age': 76, 'birth_date': '1945-05-24'}, {'age': 99, 'birth_date': '1922-04-03'}, {'age': 65, 'birth_date': '1955-11-30'}, {'age': 70, 'birth_date': '1950-12-18'}, {'age': 60, 'birth_date': '1961-05-19'}, {'age': 50, 'birth_date': '1971-07-29'}, {'age': 32, 'birth_date': '1988-11-14'}, {'age': 4, 'birth_date': '2017-05-08'}, {'age': 78, 'birth_date': '1943-02-09'}, {'age': 65, 'birth_date': '1955-11-30'}, {'age': 48, 'birth_date': '1973-09-21'}, {'age': 3, 'birth_date': '2018-03-25'}, {'age': 66, 'birth_date': '1955-03-13'}, {'age': 77, 'birth_date': '1944-05-19'}, {'age': 99, 'birth_date': '1922-04-03'}, {'age': 25, 'birth_date': '1996-09-26'}, {'age': 18, 'birth_date': '2002-11-08'}, {'age': 47, 'birth_date': '1974-11-01'}, {'age': 79, 'birth_date': '1942-06-19'}, {'age': 99, 'birth_date': '1922-09-27'}, {'age': 32, 'birth_date': '1989-07-17'}, {'age': 10, 'birth_date': '2011-10-02'}, {'age': 74, 'birth_date': '1947-05-03'}, {'age': 61, 'birth_date': '1960-06-11'}, {'age': 99, 'birth_date': '1922-04-03'}, {'age': 62, 'birth_date': '1958-11-22'}, {'age': 59, 'birth_date': '1962-06-28'}, {'age': 67, 'birth_date': '1954-08-17'}, {'age': 2, 'birth_date': '2019-03-24'}, {'age': 50, 'birth_date': '1971-07-25'}, {'age': 99, 'birth_date': '1922-09-27'}, {'age': 3, 'birth_date': '2018-09-20'}, {'age': 70, 'birth_date': '1951-09-17'}, {'age': 61, 'birth_date': '1960-06-11'}, {'age': 62, 'birth_date': '1959-03-18'}, {'age': 43, 'birth_date': '1978-06-04'}, {'age': 18, 'birth_date': '2003-10-09'}, {'age': 71, 'birth_date': '1949-11-25'}, {'age': 29, 'birth_date': '1992-08-04'}, {'age': 99, 'birth_date': '1922-09-27'}, {'age': 52, 'birth_date': '1968-11-11'}, {'age': 71, 'birth_date': '1950-03-23'}, {'age': 26, 'birth_date': '1995-01-25'}, {'age': 28, 'birth_date': '1993-10-21'}, {'age': 56, 'birth_date': '1965-08-30'}, {'age': 32, 'birth_date': '1989-08-11'}, {'age': 69, 'birth_date': '1951-12-24'}, {'age': 23, 'birth_date': '1998-02-14'}, {'age': 31, 'birth_date': '1990-02-24'}, {'age': 46, 'birth_date': '1975-03-04'}, {'age': 99, 'birth_date': '1922-09-27'}, {'age': 106, 'birth_date': '1915-04-02'}, {'age': 30, 'birth_date': '1991-09-07'}, {'age': 21, 'birth_date': '2000-10-13'}, {'age': 67, 'birth_date': '1953-12-24'}, {'age': 67, 'birth_date': '1954-07-17'}, {'age': 106, 'birth_date': '1915-04-02'}, {'age': 99, 'birth_date': '1922-09-27'}, {'age': 106, 'birth_date': '1915-04-02'}, {'age': 99, 'birth_date': '1922-09-27'}, {'age': 106, 'birth_date': '1915-04-02'}, {'age': 106, 'birth_date': '1915-04-02'}, {'age': 99, 'birth_date': '1922-09-27'}, {'age': 106, 'birth_date': '1915-04-02'}, {'age': 99, 'birth_date': '1922-09-27'}, {'age': 99, 'birth_date': '1922-09-27'}, {'age': 99, 'birth_date': '1922-09-27'}, {'age': 99, 'birth_date': '1922-09-27'}, {'age': 90, 'birth_date': '1931-03-16'}, {'age': 90, 'birth_date': '1931-03-16'}] . # filtering based on age min_age = 18 max_age = 79 patients = [ {'birth_date': row[1], 'age': get_age(row[1])} for row in rows if get_age(row[1]) &gt;= min_age and get_age(row[1]) &lt;= max_age ] pprint(patients) . [{'age': 18, 'birth_date': '2003-04-15'}, {'age': 55, 'birth_date': '1966-10-02'}, {'age': 20, 'birth_date': '2001-09-12'}, {'age': 53, 'birth_date': '1967-11-05'}, {'age': 23, 'birth_date': '1998-05-27'}, {'age': 18, 'birth_date': '2003-06-22'}, {'age': 64, 'birth_date': '1957-02-03'}, {'age': 39, 'birth_date': '1981-11-24'}, {'age': 34, 'birth_date': '1987-04-23'}, {'age': 19, 'birth_date': '2002-05-04'}, {'age': 18, 'birth_date': '2003-07-13'}, {'age': 26, 'birth_date': '1995-04-13'}, {'age': 28, 'birth_date': '1993-08-19'}, {'age': 29, 'birth_date': '1992-02-09'}, {'age': 71, 'birth_date': '1950-02-06'}, {'age': 32, 'birth_date': '1989-05-19'}, {'age': 41, 'birth_date': '1980-08-01'}, {'age': 51, 'birth_date': '1970-10-17'}, {'age': 40, 'birth_date': '1981-09-17'}, {'age': 53, 'birth_date': '1967-11-06'}, {'age': 60, 'birth_date': '1961-07-15'}, {'age': 65, 'birth_date': '1956-04-05'}, {'age': 21, 'birth_date': '2000-09-13'}, {'age': 32, 'birth_date': '1989-04-14'}, {'age': 52, 'birth_date': '1969-08-30'}, {'age': 52, 'birth_date': '1968-11-10'}, {'age': 30, 'birth_date': '1991-04-14'}, {'age': 69, 'birth_date': '1951-12-13'}, {'age': 24, 'birth_date': '1997-09-25'}, {'age': 51, 'birth_date': '1970-01-22'}, {'age': 27, 'birth_date': '1994-10-02'}, {'age': 76, 'birth_date': '1945-05-24'}, {'age': 78, 'birth_date': '1943-02-09'}, {'age': 29, 'birth_date': '1992-01-30'}, {'age': 69, 'birth_date': '1951-12-13'}, {'age': 34, 'birth_date': '1987-04-26'}, {'age': 78, 'birth_date': '1943-02-09'}, {'age': 58, 'birth_date': '1963-03-09'}, {'age': 76, 'birth_date': '1945-05-24'}, {'age': 65, 'birth_date': '1955-11-30'}, {'age': 70, 'birth_date': '1950-12-18'}, {'age': 60, 'birth_date': '1961-05-19'}, {'age': 50, 'birth_date': '1971-07-29'}, {'age': 32, 'birth_date': '1988-11-14'}, {'age': 78, 'birth_date': '1943-02-09'}, {'age': 65, 'birth_date': '1955-11-30'}, {'age': 48, 'birth_date': '1973-09-21'}, {'age': 66, 'birth_date': '1955-03-13'}, {'age': 77, 'birth_date': '1944-05-19'}, {'age': 25, 'birth_date': '1996-09-26'}, {'age': 18, 'birth_date': '2002-11-08'}, {'age': 47, 'birth_date': '1974-11-01'}, {'age': 79, 'birth_date': '1942-06-19'}, {'age': 32, 'birth_date': '1989-07-17'}, {'age': 74, 'birth_date': '1947-05-03'}, {'age': 61, 'birth_date': '1960-06-11'}, {'age': 62, 'birth_date': '1958-11-22'}, {'age': 59, 'birth_date': '1962-06-28'}, {'age': 67, 'birth_date': '1954-08-17'}, {'age': 50, 'birth_date': '1971-07-25'}, {'age': 70, 'birth_date': '1951-09-17'}, {'age': 61, 'birth_date': '1960-06-11'}, {'age': 62, 'birth_date': '1959-03-18'}, {'age': 43, 'birth_date': '1978-06-04'}, {'age': 18, 'birth_date': '2003-10-09'}, {'age': 71, 'birth_date': '1949-11-25'}, {'age': 29, 'birth_date': '1992-08-04'}, {'age': 52, 'birth_date': '1968-11-11'}, {'age': 71, 'birth_date': '1950-03-23'}, {'age': 26, 'birth_date': '1995-01-25'}, {'age': 28, 'birth_date': '1993-10-21'}, {'age': 56, 'birth_date': '1965-08-30'}, {'age': 32, 'birth_date': '1989-08-11'}, {'age': 69, 'birth_date': '1951-12-24'}, {'age': 23, 'birth_date': '1998-02-14'}, {'age': 31, 'birth_date': '1990-02-24'}, {'age': 46, 'birth_date': '1975-03-04'}, {'age': 30, 'birth_date': '1991-09-07'}, {'age': 21, 'birth_date': '2000-10-13'}, {'age': 67, 'birth_date': '1953-12-24'}, {'age': 67, 'birth_date': '1954-07-17'}] . # another way to do it, add the age to the end for row in rows: row.append(get_age(row[1])) . print(rows[0]) . ['3287bb9c-e395-4146-8dd5-1fc3d887d220', '2015-07-12', '', '999-82-9751', '', '', '', 'Victor265', 'Kilback373', '', '', '', 'white', 'polish', 'M', 'Newton Massachusetts US', '657 Heathcote Divide', 'Fitchburg', 'Massachusetts', '01420', 6] . min_age = 18 max_age = 79 patients = [ {'birth_date': row[1], 'age': row[-1]} for row in rows if row[20] &gt;= min_age and row[-1] &lt;= max_age ] pprint(patients) . [{'age': 18, 'birth_date': '2003-04-15'}, {'age': 55, 'birth_date': '1966-10-02'}, {'age': 20, 'birth_date': '2001-09-12'}, {'age': 53, 'birth_date': '1967-11-05'}, {'age': 23, 'birth_date': '1998-05-27'}, {'age': 18, 'birth_date': '2003-06-22'}, {'age': 64, 'birth_date': '1957-02-03'}, {'age': 39, 'birth_date': '1981-11-24'}, {'age': 34, 'birth_date': '1987-04-23'}, {'age': 19, 'birth_date': '2002-05-04'}, {'age': 18, 'birth_date': '2003-07-13'}, {'age': 26, 'birth_date': '1995-04-13'}, {'age': 28, 'birth_date': '1993-08-19'}, {'age': 29, 'birth_date': '1992-02-09'}, {'age': 71, 'birth_date': '1950-02-06'}, {'age': 32, 'birth_date': '1989-05-19'}, {'age': 41, 'birth_date': '1980-08-01'}, {'age': 51, 'birth_date': '1970-10-17'}, {'age': 40, 'birth_date': '1981-09-17'}, {'age': 53, 'birth_date': '1967-11-06'}, {'age': 60, 'birth_date': '1961-07-15'}, {'age': 65, 'birth_date': '1956-04-05'}, {'age': 21, 'birth_date': '2000-09-13'}, {'age': 32, 'birth_date': '1989-04-14'}, {'age': 52, 'birth_date': '1969-08-30'}, {'age': 52, 'birth_date': '1968-11-10'}, {'age': 30, 'birth_date': '1991-04-14'}, {'age': 69, 'birth_date': '1951-12-13'}, {'age': 24, 'birth_date': '1997-09-25'}, {'age': 51, 'birth_date': '1970-01-22'}, {'age': 27, 'birth_date': '1994-10-02'}, {'age': 76, 'birth_date': '1945-05-24'}, {'age': 78, 'birth_date': '1943-02-09'}, {'age': 29, 'birth_date': '1992-01-30'}, {'age': 69, 'birth_date': '1951-12-13'}, {'age': 34, 'birth_date': '1987-04-26'}, {'age': 78, 'birth_date': '1943-02-09'}, {'age': 58, 'birth_date': '1963-03-09'}, {'age': 76, 'birth_date': '1945-05-24'}, {'age': 65, 'birth_date': '1955-11-30'}, {'age': 70, 'birth_date': '1950-12-18'}, {'age': 60, 'birth_date': '1961-05-19'}, {'age': 50, 'birth_date': '1971-07-29'}, {'age': 32, 'birth_date': '1988-11-14'}, {'age': 78, 'birth_date': '1943-02-09'}, {'age': 65, 'birth_date': '1955-11-30'}, {'age': 48, 'birth_date': '1973-09-21'}, {'age': 66, 'birth_date': '1955-03-13'}, {'age': 77, 'birth_date': '1944-05-19'}, {'age': 25, 'birth_date': '1996-09-26'}, {'age': 18, 'birth_date': '2002-11-08'}, {'age': 47, 'birth_date': '1974-11-01'}, {'age': 79, 'birth_date': '1942-06-19'}, {'age': 32, 'birth_date': '1989-07-17'}, {'age': 74, 'birth_date': '1947-05-03'}, {'age': 61, 'birth_date': '1960-06-11'}, {'age': 62, 'birth_date': '1958-11-22'}, {'age': 59, 'birth_date': '1962-06-28'}, {'age': 67, 'birth_date': '1954-08-17'}, {'age': 50, 'birth_date': '1971-07-25'}, {'age': 70, 'birth_date': '1951-09-17'}, {'age': 61, 'birth_date': '1960-06-11'}, {'age': 62, 'birth_date': '1959-03-18'}, {'age': 43, 'birth_date': '1978-06-04'}, {'age': 18, 'birth_date': '2003-10-09'}, {'age': 71, 'birth_date': '1949-11-25'}, {'age': 29, 'birth_date': '1992-08-04'}, {'age': 52, 'birth_date': '1968-11-11'}, {'age': 71, 'birth_date': '1950-03-23'}, {'age': 26, 'birth_date': '1995-01-25'}, {'age': 28, 'birth_date': '1993-10-21'}, {'age': 56, 'birth_date': '1965-08-30'}, {'age': 32, 'birth_date': '1989-08-11'}, {'age': 69, 'birth_date': '1951-12-24'}, {'age': 23, 'birth_date': '1998-02-14'}, {'age': 31, 'birth_date': '1990-02-24'}, {'age': 46, 'birth_date': '1975-03-04'}, {'age': 30, 'birth_date': '1991-09-07'}, {'age': 21, 'birth_date': '2000-10-13'}, {'age': 67, 'birth_date': '1953-12-24'}, {'age': 67, 'birth_date': '1954-07-17'}] . # todo we might want to shift all dates (e.g. birthday, encounter date) . fake_names = '''Veronique Tippetts Sarah Santiago Eustolia Bushard Emanuel Riker Maybelle Denney Lilia Gobel Clarine Vandermeer Felicidad Joynes Rod Pixley Rashad Fukushima Marci Bakley Melvina Cichon Susy Sibert Oma Hoskins Lance Curnutte Mei Wooldridge Jillian Mccroy Darby Castellon Raul Pickney Loni Kaur''' print(fake_names) . Veronique Tippetts Sarah Santiago Eustolia Bushard Emanuel Riker Maybelle Denney Lilia Gobel Clarine Vandermeer Felicidad Joynes Rod Pixley Rashad Fukushima Marci Bakley Melvina Cichon Susy Sibert Oma Hoskins Lance Curnutte Mei Wooldridge Jillian Mccroy Darby Castellon Raul Pickney Loni Kaur . fake_names.split('\\n') . ['Veronique Tippetts', 'Sarah Santiago', 'Eustolia Bushard', 'Emanuel Riker', 'Maybelle Denney', 'Lilia Gobel', 'Clarine Vandermeer', 'Felicidad Joynes', 'Rod Pixley', 'Rashad Fukushima', 'Marci Bakley', 'Melvina Cichon', 'Susy Sibert', 'Oma Hoskins', 'Lance Curnutte', 'Mei Wooldridge', 'Jillian Mccroy', 'Darby Castellon', 'Raul Pickney', 'Loni Kaur'] . # deidentify the name import random def fake_name_generator(): fake_names = \\ \"\"\"Veronique Tippetts Sarah Santiago Eustolia Bushard Emanuel Riker Maybelle Denney Lilia Gobel Clarine Vandermeer Felicidad Joynes Rod Pixley Rashad Fukushima Marci Bakley Melvina Cichon Susy Sibert Oma Hoskins Lance Curnutte Mei Wooldridge Jillian Mccroy Darby Castellon Raul Pickney Loni Kaur\"\"\".split('\\n') return random.choice(fake_names).strip() . print(fake_name_generator()) print(fake_name_generator()) print(fake_name_generator()) . Lance Curnutte Darby Castellon Loni Kaur . # don't reuse names fake_names = \\ \"\"\"Veronique Tippetts Sarah Santiago Eustolia Bushard Emanuel Riker Maybelle Denney Lilia Gobel Clarine Vandermeer Felicidad Joynes Rod Pixley Rashad Fukushima Marci Bakley Melvina Cichon Susy Sibert Oma Hoskins Lance Curnutte Mei Wooldridge Jillian Mccroy Darby Castellon Raul Pickney Loni Kaur\"\"\".split('\\n') def get_fake_name(): if len(fake_names) == 0: return None r = random.randint(0, len(fake_names) - 1) n = fake_names[r].strip() del fake_names[r] return n . for i in range(21): print(get_fake_name()) . Rashad Fukushima Raul Pickney Marci Bakley Melvina Cichon Mei Wooldridge Maybelle Denney Emanuel Riker Veronique Tippetts Rod Pixley Lance Curnutte Susy Sibert Loni Kaur Sarah Santiago Clarine Vandermeer Jillian Mccroy Felicidad Joynes Darby Castellon Oma Hoskins Eustolia Bushard Lilia Gobel None . min_age = 18 max_age = 79 patients = [ {'birth_date': row[1], 'age': row[20], 'name': get_fake_name() } for row in rows if row[20] &gt;= min_age and row[20] &lt;= max_age ] pprint(patients) . [{'age': 18, 'birth_date': '2003-04-15', 'name': None}, {'age': 55, 'birth_date': '1966-10-02', 'name': None}, {'age': 20, 'birth_date': '2001-09-12', 'name': None}, {'age': 53, 'birth_date': '1967-11-05', 'name': None}, {'age': 23, 'birth_date': '1998-05-27', 'name': None}, {'age': 18, 'birth_date': '2003-06-22', 'name': None}, {'age': 64, 'birth_date': '1957-02-03', 'name': None}, {'age': 39, 'birth_date': '1981-11-24', 'name': None}, {'age': 34, 'birth_date': '1987-04-23', 'name': None}, {'age': 19, 'birth_date': '2002-05-04', 'name': None}, {'age': 18, 'birth_date': '2003-07-13', 'name': None}, {'age': 26, 'birth_date': '1995-04-13', 'name': None}, {'age': 28, 'birth_date': '1993-08-19', 'name': None}, {'age': 29, 'birth_date': '1992-02-09', 'name': None}, {'age': 71, 'birth_date': '1950-02-06', 'name': None}, {'age': 32, 'birth_date': '1989-05-19', 'name': None}, {'age': 41, 'birth_date': '1980-08-01', 'name': None}, {'age': 51, 'birth_date': '1970-10-17', 'name': None}, {'age': 40, 'birth_date': '1981-09-17', 'name': None}, {'age': 53, 'birth_date': '1967-11-06', 'name': None}, {'age': 60, 'birth_date': '1961-07-15', 'name': None}, {'age': 65, 'birth_date': '1956-04-05', 'name': None}, {'age': 21, 'birth_date': '2000-09-13', 'name': None}, {'age': 32, 'birth_date': '1989-04-14', 'name': None}, {'age': 52, 'birth_date': '1969-08-30', 'name': None}, {'age': 52, 'birth_date': '1968-11-10', 'name': None}, {'age': 30, 'birth_date': '1991-04-14', 'name': None}, {'age': 69, 'birth_date': '1951-12-13', 'name': None}, {'age': 24, 'birth_date': '1997-09-25', 'name': None}, {'age': 51, 'birth_date': '1970-01-22', 'name': None}, {'age': 27, 'birth_date': '1994-10-02', 'name': None}, {'age': 76, 'birth_date': '1945-05-24', 'name': None}, {'age': 78, 'birth_date': '1943-02-09', 'name': None}, {'age': 29, 'birth_date': '1992-01-30', 'name': None}, {'age': 69, 'birth_date': '1951-12-13', 'name': None}, {'age': 34, 'birth_date': '1987-04-26', 'name': None}, {'age': 78, 'birth_date': '1943-02-09', 'name': None}, {'age': 58, 'birth_date': '1963-03-09', 'name': None}, {'age': 76, 'birth_date': '1945-05-24', 'name': None}, {'age': 65, 'birth_date': '1955-11-30', 'name': None}, {'age': 70, 'birth_date': '1950-12-18', 'name': None}, {'age': 60, 'birth_date': '1961-05-19', 'name': None}, {'age': 50, 'birth_date': '1971-07-29', 'name': None}, {'age': 32, 'birth_date': '1988-11-14', 'name': None}, {'age': 78, 'birth_date': '1943-02-09', 'name': None}, {'age': 65, 'birth_date': '1955-11-30', 'name': None}, {'age': 48, 'birth_date': '1973-09-21', 'name': None}, {'age': 66, 'birth_date': '1955-03-13', 'name': None}, {'age': 77, 'birth_date': '1944-05-19', 'name': None}, {'age': 25, 'birth_date': '1996-09-26', 'name': None}, {'age': 18, 'birth_date': '2002-11-08', 'name': None}, {'age': 47, 'birth_date': '1974-11-01', 'name': None}, {'age': 79, 'birth_date': '1942-06-19', 'name': None}, {'age': 32, 'birth_date': '1989-07-17', 'name': None}, {'age': 74, 'birth_date': '1947-05-03', 'name': None}, {'age': 61, 'birth_date': '1960-06-11', 'name': None}, {'age': 62, 'birth_date': '1958-11-22', 'name': None}, {'age': 59, 'birth_date': '1962-06-28', 'name': None}, {'age': 67, 'birth_date': '1954-08-17', 'name': None}, {'age': 50, 'birth_date': '1971-07-25', 'name': None}, {'age': 70, 'birth_date': '1951-09-17', 'name': None}, {'age': 61, 'birth_date': '1960-06-11', 'name': None}, {'age': 62, 'birth_date': '1959-03-18', 'name': None}, {'age': 43, 'birth_date': '1978-06-04', 'name': None}, {'age': 18, 'birth_date': '2003-10-09', 'name': None}, {'age': 71, 'birth_date': '1949-11-25', 'name': None}, {'age': 29, 'birth_date': '1992-08-04', 'name': None}, {'age': 52, 'birth_date': '1968-11-11', 'name': None}, {'age': 71, 'birth_date': '1950-03-23', 'name': None}, {'age': 26, 'birth_date': '1995-01-25', 'name': None}, {'age': 28, 'birth_date': '1993-10-21', 'name': None}, {'age': 56, 'birth_date': '1965-08-30', 'name': None}, {'age': 32, 'birth_date': '1989-08-11', 'name': None}, {'age': 69, 'birth_date': '1951-12-24', 'name': None}, {'age': 23, 'birth_date': '1998-02-14', 'name': None}, {'age': 31, 'birth_date': '1990-02-24', 'name': None}, {'age': 46, 'birth_date': '1975-03-04', 'name': None}, {'age': 30, 'birth_date': '1991-09-07', 'name': None}, {'age': 21, 'birth_date': '2000-10-13', 'name': None}, {'age': 67, 'birth_date': '1953-12-24', 'name': None}, {'age': 67, 'birth_date': '1954-07-17', 'name': None}] . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.a%20-%20Cleaning%20and%20Transforming%20Data.html",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.a%20-%20Cleaning%20and%20Transforming%20Data.html"
  },"40": {
    "doc": "01.a - File Processing",
    "title": "Files",
    "content": "Python File I/O . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html#files",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html#files"
  },"41": {
    "doc": "01.a - File Processing",
    "title": "Open Files",
    "content": "f = open('file.txt') . Python File Modes . | Mode | Description | . | ‘r’ | Open a file for reading. (default) | . | ‘w’ | Open a file for writing. Creates a new file if it does not exist or truncates the file if it exists. | . | ‘x’ | Open a file for exclusive creation. If the file already exists, the operation fails. | . | ‘a’ | Open for appending at the end of the file without truncating it. Creates a new file if it does not exist. | . | ‘t’ | Open in text mode. (default) | . | ‘b’ | Open in binary mode. | . | ’+’ | Open a file for updating (reading and writing) | . f = open(\"test.txt\") # equivalent to 'r' or 'rt' . f = open(\"test.txt\", 'w') # write in text mode . f = open(\"python.png\",'r+b') # read and write in binary mode . File Encodings . Unlike other languages, the character ‘a’ does not imply the number 97 until it is encoded using ASCII (or other equivalent encodings). The default encoding is platform dependent. | In Windows it is ‘cp1252’ | In Linux it is ‘utf-8’ | . You cannot rely on the default encoding or else our code will behave differently in different platforms. It is highly recommended to specify the encoding type when working with text files. f = open(\"file.txt\", mode='r', encoding='utf-8') . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html#open-files",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html#open-files"
  },"42": {
    "doc": "01.a - File Processing",
    "title": "Closing Files",
    "content": "f = open(\"file.txt\", encoding='utf-8') # perform file operations f.close() . In the last example exceptions could crash your program. Exception handlig is a topic in itself, but here we’ll simply wrap the code in a try…catch block. Now the file handle will be closed regardless of whether or not there was an exception. In the example above, an exeption might result in the file remaining open (and potentially locked) from other applications in the event of an exception. try: f = open(\"file.txt\",encoding = 'utf-8') # perform file operations finally: f.close() . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html#closing-files",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html#closing-files"
  },"43": {
    "doc": "01.a - File Processing",
    "title": "Using with",
    "content": "The best way to ensure that you close your files when the code block terminates is by using the with statement. When using the with statement you do not have to explicitly call the file’s close() method. with open('file.txt') as f: text = f.read() print(text) # f.close() # not necessary . hello class python is fun let's get started . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html#using-with",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html#using-with"
  },"44": {
    "doc": "01.a - File Processing",
    "title": "Writing Files",
    "content": "with open(\"file.txt\", 'w', encoding='utf-8') as f: print(type(f)) f.write(\"hello class\\n\" ) f.write(\"python is fun\\n\\n\") f.write(\"let's get started\\n\") . &lt;class '_io.TextIOWrapper'&gt; . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html#writing-files",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html#writing-files"
  },"45": {
    "doc": "01.a - File Processing",
    "title": "Reading Files",
    "content": "f = open(\"file.txt\", 'r', encoding='utf-8') print('1:', f.read(3)) # read the 1st 3 characters print('2:', f.read(3)) # read the next 3 characters print('3:', f.read()) # read until the end print('4:', f.read()) # return's an empty string . 1: hel 2: lo 3: class python is fun let's get started 4: . f.tell() # get the current file position . 45 . f.seek(0) # bring file cursor to initial position . 0 . f.read() # read the entire file . \"hello class\\npython is fun\\n\\nlet's get started\\n\" . f.seek(0) # reset the cursor . 0 . # read line by line for line in f: print(line, end='') . hello class python is fun let's get started . f.seek(0) # reset the cursor . 0 . # read line by line f.readline() . 'hello class\\n' . f.readline() . 'python is fun\\n' . f.readline() . '\\n' . line = f.readline() print(type(line), line) . &lt;class 'str'&gt; let's get started . f.readline() . '' . f.seek(0) # reset the cursor . 0 . # read all lines at once f.readlines() . ['hello class\\n', 'python is fun\\n', '\\n', \"let's get started\\n\"] . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html#reading-files",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html#reading-files"
  },"46": {
    "doc": "01.a - File Processing",
    "title": "Some Notes on Printing",
    "content": "print('abcd') print('efgh') . abcd efgh . print('abcd', end='') print('efgh') . abcdefgh . text = ' abcd ' print(text, end='') print(text) . abcd abcd . text = ' abcd ' text = text.strip() print(text, end='') print(text) . abcdabcd . print? . \u001b[0;31mDocstring:\u001b[0m print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream. \u001b[0;31mType:\u001b[0m builtin_function_or_method . print('abc', 'def', 'xyz', sep=',') . abc,def,xyz . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html#some-notes-on-printing",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html#some-notes-on-printing"
  },"47": {
    "doc": "01.a - File Processing",
    "title": "01.a - File Processing",
    "content": " ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.a%20-%20File%20Processing.html"
  },"48": {
    "doc": "01.b - CSV Files",
    "title": "Reading CSV files",
    "content": " ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#reading-csv-files",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#reading-csv-files"
  },"49": {
    "doc": "01.b - CSV Files",
    "title": "Reading with open",
    "content": "# open and print line by line with open('mock.csv') as f: for line in f: print(line) . id,first_name,last_name,email,gender,ip_address 1,Finn,Burchmore,fburchmore0@epa.gov,Male,74.20.172.212 2,Coral,Grigorey,cgrigorey1@cdc.gov,Female,151.91.212.169 3,Harold,McCormack,hmccormack2@marketwatch.com,Male,72.36.41.150 4,Thacher,Woodvine,twoodvine3@marketwatch.com,Male,66.50.122.48 5,Querida,Allmann,qallmann4@people.com.cn,Female,236.145.83.165 6,Irving,Loughman,iloughman5@globo.com,Male,19.142.188.98 7,Meghann,Vittet,mvittet6@cyberchimps.com,Female,235.59.102.124 8,Wolfgang,Mishow,wmishow7@gnu.org,Male,189.36.37.25 9,Babbie,Reide,breide8@jiathis.com,Female,129.226.63.38 10,Carson,Vowdon,cvowdon9@slashdot.org,Male,51.228.124.19 . with open('mock.csv') as f: # use enumerate to get the index as you iterate for index, line in enumerate(f): print(index, line) . 0 id,first_name,last_name,email,gender,ip_address 1 1,Finn,Burchmore,fburchmore0@epa.gov,Male,74.20.172.212 2 2,Coral,Grigorey,cgrigorey1@cdc.gov,Female,151.91.212.169 3 3,Harold,McCormack,hmccormack2@marketwatch.com,Male,72.36.41.150 4 4,Thacher,Woodvine,twoodvine3@marketwatch.com,Male,66.50.122.48 5 5,Querida,Allmann,qallmann4@people.com.cn,Female,236.145.83.165 6 6,Irving,Loughman,iloughman5@globo.com,Male,19.142.188.98 7 7,Meghann,Vittet,mvittet6@cyberchimps.com,Female,235.59.102.124 8 8,Wolfgang,Mishow,wmishow7@gnu.org,Male,189.36.37.25 9 9,Babbie,Reide,breide8@jiathis.com,Female,129.226.63.38 10 10,Carson,Vowdon,cvowdon9@slashdot.org,Male,51.228.124.19 . index = 0 processed_header = False with open('mock.csv') as f: for line in f: # handle the header differently if processed_header == False: print(\"header\", index, line) processed_header = True else: print(\"row\", index, line) index += 1 . header 0 id,first_name,last_name,email,gender,ip_address row 1 1,Finn,Burchmore,fburchmore0@epa.gov,Male,74.20.172.212 row 2 2,Coral,Grigorey,cgrigorey1@cdc.gov,Female,151.91.212.169 row 3 3,Harold,McCormack,hmccormack2@marketwatch.com,Male,72.36.41.150 row 4 4,Thacher,Woodvine,twoodvine3@marketwatch.com,Male,66.50.122.48 row 5 5,Querida,Allmann,qallmann4@people.com.cn,Female,236.145.83.165 row 6 6,Irving,Loughman,iloughman5@globo.com,Male,19.142.188.98 row 7 7,Meghann,Vittet,mvittet6@cyberchimps.com,Female,235.59.102.124 row 8 8,Wolfgang,Mishow,wmishow7@gnu.org,Male,189.36.37.25 row 9 9,Babbie,Reide,breide8@jiathis.com,Female,129.226.63.38 row 10 10,Carson,Vowdon,cvowdon9@slashdot.org,Male,51.228.124.19 . with open('mock.csv') as f: for index, line in enumerate(f): # handle the header differently if index == 0: print(\"header\", index, line) else: print(\"row\", index, line) . header 0 id,first_name,last_name,email,gender,ip_address row 1 1,Finn,Burchmore,fburchmore0@epa.gov,Male,74.20.172.212 row 2 2,Coral,Grigorey,cgrigorey1@cdc.gov,Female,151.91.212.169 row 3 3,Harold,McCormack,hmccormack2@marketwatch.com,Male,72.36.41.150 row 4 4,Thacher,Woodvine,twoodvine3@marketwatch.com,Male,66.50.122.48 row 5 5,Querida,Allmann,qallmann4@people.com.cn,Female,236.145.83.165 row 6 6,Irving,Loughman,iloughman5@globo.com,Male,19.142.188.98 row 7 7,Meghann,Vittet,mvittet6@cyberchimps.com,Female,235.59.102.124 row 8 8,Wolfgang,Mishow,wmishow7@gnu.org,Male,189.36.37.25 row 9 9,Babbie,Reide,breide8@jiathis.com,Female,129.226.63.38 row 10 10,Carson,Vowdon,cvowdon9@slashdot.org,Male,51.228.124.19 . headers = [] with open('mock.csv') as f: for index, line in enumerate(f): line = line.strip() if index == 0: # split the headers headers = line.split(',') else: pass print('header fields', headers) . header fields ['id', 'first_name', 'last_name', 'email', 'gender', 'ip_address'] . headers = [] rows = [] with open('mock.csv') as f: for index, line in enumerate(f): line = line.strip() if index == 0: headers = line.split(',') else: # append the rows rows.append(line.split(',')) print('header fields', headers) print('rows fields', rows) . header fields ['id', 'first_name', 'last_name', 'email', 'gender', 'ip_address'] rows fields [['1', 'Finn', 'Burchmore', 'fburchmore0@epa.gov', 'Male', '74.20.172.212'], ['2', 'Coral', 'Grigorey', 'cgrigorey1@cdc.gov', 'Female', '151.91.212.169'], ['3', 'Harold', 'McCormack', 'hmccormack2@marketwatch.com', 'Male', '72.36.41.150'], ['4', 'Thacher', 'Woodvine', 'twoodvine3@marketwatch.com', 'Male', '66.50.122.48'], ['5', 'Querida', 'Allmann', 'qallmann4@people.com.cn', 'Female', '236.145.83.165'], ['6', 'Irving', 'Loughman', 'iloughman5@globo.com', 'Male', '19.142.188.98'], ['7', 'Meghann', 'Vittet', 'mvittet6@cyberchimps.com', 'Female', '235.59.102.124'], ['8', 'Wolfgang', 'Mishow', 'wmishow7@gnu.org', 'Male', '189.36.37.25'], ['9', 'Babbie', 'Reide', 'breide8@jiathis.com', 'Female', '129.226.63.38'], ['10', 'Carson', 'Vowdon', 'cvowdon9@slashdot.org', 'Male', '51.228.124.19']] . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#reading-with-open",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#reading-with-open"
  },"50": {
    "doc": "01.b - CSV Files",
    "title": "Reading with csv",
    "content": "Reading and Writing CSV Files in Python . import csv . with open('mock.csv') as f: reader = csv.reader(f, delimiter=',') for row in reader: print(row) . ['id', 'first_name', 'last_name', 'email', 'gender', 'ip_address'] ['1', 'Finn', 'Burchmore', 'fburchmore0@epa.gov', 'Male', '74.20.172.212'] ['2', 'Coral', 'Grigorey', 'cgrigorey1@cdc.gov', 'Female', '151.91.212.169'] ['3', 'Harold', 'McCormack', 'hmccormack2@marketwatch.com', 'Male', '72.36.41.150'] ['4', 'Thacher', 'Woodvine', 'twoodvine3@marketwatch.com', 'Male', '66.50.122.48'] ['5', 'Querida', 'Allmann', 'qallmann4@people.com.cn', 'Female', '236.145.83.165'] ['6', 'Irving', 'Loughman', 'iloughman5@globo.com', 'Male', '19.142.188.98'] ['7', 'Meghann', 'Vittet', 'mvittet6@cyberchimps.com', 'Female', '235.59.102.124'] ['8', 'Wolfgang', 'Mishow', 'wmishow7@gnu.org', 'Male', '189.36.37.25'] ['9', 'Babbie', 'Reide', 'breide8@jiathis.com', 'Female', '129.226.63.38'] ['10', 'Carson', 'Vowdon', 'cvowdon9@slashdot.org', 'Male', '51.228.124.19'] . with open('mock.csv') as f: reader = csv.reader(f, delimiter=',') for index, row in enumerate(reader): if index == 0: print(f'{\", \".join(row)}') else: print(f'id: {row[0]} first_name: {row[1]} last_name: {row[2]} email: {row[3]}') . id, first_name, last_name, email, gender, ip_address id: 1 first_name: Finn last_name: Burchmore email: fburchmore0@epa.gov id: 2 first_name: Coral last_name: Grigorey email: cgrigorey1@cdc.gov id: 3 first_name: Harold last_name: McCormack email: hmccormack2@marketwatch.com id: 4 first_name: Thacher last_name: Woodvine email: twoodvine3@marketwatch.com id: 5 first_name: Querida last_name: Allmann email: qallmann4@people.com.cn id: 6 first_name: Irving last_name: Loughman email: iloughman5@globo.com id: 7 first_name: Meghann last_name: Vittet email: mvittet6@cyberchimps.com id: 8 first_name: Wolfgang last_name: Mishow email: wmishow7@gnu.org id: 9 first_name: Babbie last_name: Reide email: breide8@jiathis.com id: 10 first_name: Carson last_name: Vowdon email: cvowdon9@slashdot.org . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#reading-with-csv",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#reading-with-csv"
  },"51": {
    "doc": "01.b - CSV Files",
    "title": "Reading CSV Files Into a Dictionary With csv",
    "content": "with open('mock.csv') as f: reader = csv.DictReader(f) for index, row in enumerate(reader): print(f'id: {row[\"id\"]} first_name: {row[\"first_name\"]} last_name: {row[\"last_name\"]} email: {row[\"email\"]}') . id: 1 first_name: Finn last_name: Burchmore email: fburchmore0@epa.gov id: 2 first_name: Coral last_name: Grigorey email: cgrigorey1@cdc.gov id: 3 first_name: Harold last_name: McCormack email: hmccormack2@marketwatch.com id: 4 first_name: Thacher last_name: Woodvine email: twoodvine3@marketwatch.com id: 5 first_name: Querida last_name: Allmann email: qallmann4@people.com.cn id: 6 first_name: Irving last_name: Loughman email: iloughman5@globo.com id: 7 first_name: Meghann last_name: Vittet email: mvittet6@cyberchimps.com id: 8 first_name: Wolfgang last_name: Mishow email: wmishow7@gnu.org id: 9 first_name: Babbie last_name: Reide email: breide8@jiathis.com id: 10 first_name: Carson last_name: Vowdon email: cvowdon9@slashdot.org . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#reading-csv-files-into-a-dictionary-with-csv",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#reading-csv-files-into-a-dictionary-with-csv"
  },"52": {
    "doc": "01.b - CSV Files",
    "title": "Optional Python CSV reader Parameters",
    "content": ". | delimiter specifies the character used to separate each field. The default is the comma (‘,’). | quotechar specifies the character used to surround fields that contain the delimiter character. The default is a double quote (‘ “ ‘). | escapechar specifies the character used to escape the delimiter character, in case quotes aren’t used. The default is no escape character. | . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#optional-python-csv-reader-parameters",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#optional-python-csv-reader-parameters"
  },"53": {
    "doc": "01.b - CSV Files",
    "title": "Writing CSV Files With csv",
    "content": "with open('scores.csv', mode='w') as employee_file: writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) writer.writerow(['2019-05-01', 'Pirates', 0, 'Cubs', 10]) writer.writerow(['2019-05-15', 'Reds', 7, 'Pirates', 0]) . | If quoting is set to csv.QUOTE_MINIMAL, then .writerow() will quote fields only if they contain the delimiter or the quotechar. This is the default case. | If quoting is set to csv.QUOTE_ALL, then .writerow() will quote all fields. | If quoting is set to csv.QUOTE_NONNUMERIC, then .writerow() will quote all fields containing text data and convert all numeric fields to the float data type. | If quoting is set to csv.QUOTE_NONE, then .writerow() will escape delimiters instead of quoting them. In this case, you also must provide a value for the escapechar optional parameter. | . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#writing-csv-files-with-csv",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#writing-csv-files-with-csv"
  },"54": {
    "doc": "01.b - CSV Files",
    "title": "Writing CSV File From a Dictionary With csv",
    "content": "with open('scores.csv', mode='w') as csv_file: fieldnames = ['date', 'home_team', 'home_score', 'away_team', 'away_score'] writer = csv.DictWriter(csv_file, fieldnames=fieldnames) writer.writeheader() writer.writerow({'date': '2019-05-01', 'home_team': 'Pirates', 'home_score': 0, 'away_team': 'Cubs', 'away_score': 10}) writer.writerow({'date': '2019-05-15', 'home_team': 'Reds', 'home_score': 7, 'away_team': 'Pirates', 'away_score': 0}) . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#writing-csv-file-from-a-dictionary-with-csv",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#writing-csv-file-from-a-dictionary-with-csv"
  },"55": {
    "doc": "01.b - CSV Files",
    "title": "Some Notes on typecasting",
    "content": "x = \"1\" print(x + str(3)) . 13 . x = \"1\" n = float(x) print(n + n) print(n * 3) . 2.0 3.0 . x = \"blah\" print(x) x = 125 print(x) . blah 125 . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#some-notes-on-typecasting",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html#some-notes-on-typecasting"
  },"56": {
    "doc": "01.b - CSV Files",
    "title": "01.b - CSV Files",
    "content": " ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.b%20-%20CSV%20Files.html"
  },"57": {
    "doc": "01.b - Grouping Data",
    "title": "Grouping data using python",
    "content": "In this tutorial we’re going to learn how to use basic python funtionality to group datasets. import csv from pprint import pprint . # recall, opening the file and reading the data with open('../data/csv/patients.csv') as f: reader = csv.DictReader(f) for i, row in enumerate(reader): pprint(row) if i &gt;= 1: break . {'ADDRESS': '657 Heathcote Divide', 'BIRTHDATE': '2015-07-12', 'BIRTHPLACE': 'Newton Massachusetts US', 'CITY': 'Fitchburg', 'DEATHDATE': '', 'DRIVERS': '', 'ETHNICITY': 'polish', 'FIRST': 'Victor265', 'GENDER': 'M', 'Id': '3287bb9c-e395-4146-8dd5-1fc3d887d220', 'LAST': 'Kilback373', 'MAIDEN': '', 'MARITAL': '', 'PASSPORT': '', 'PREFIX': '', 'RACE': 'white', 'SSN': '999-82-9751', 'STATE': 'Massachusetts', 'SUFFIX': '', 'ZIP': '01420'} {'ADDRESS': '419 Mraz Ranch Apt 13', 'BIRTHDATE': '2003-04-15', 'BIRTHPLACE': 'Ipswich Massachusetts US', 'CITY': 'Saugus', 'DEATHDATE': '', 'DRIVERS': 'S99990332', 'ETHNICITY': 'puerto_rican', 'FIRST': 'Reena181', 'GENDER': 'F', 'Id': 'e118c014-fef7-4fbd-8c5d-fe42e9e16d84', 'LAST': 'Dooley940', 'MAIDEN': '', 'MARITAL': '', 'PASSPORT': '', 'PREFIX': '', 'RACE': 'hispanic', 'SSN': '999-57-7583', 'STATE': 'Massachusetts', 'SUFFIX': '', 'ZIP': '01906'} . # then we're going to parse out a group, e.g. Gender # recall, opening the file and reading the data with open('../data/csv/patients.csv') as f: reader = csv.DictReader(f) for i, row in enumerate(reader): patient_gender = row['GENDER'] print(patient_gender) if i &gt;= 2: break . M F F . # how do we know what the unique genders are? # let's iterate over them and create a set patient_genders = set() with open('../data/csv/patients.csv') as f: reader = csv.DictReader(f) for i, row in enumerate(reader): patient_gender = row['GENDER'] patient_genders.add(patient_gender) print(patient_genders) . {'F', 'M'} . # okay, we have 2 genders let's create 2 lists male_patients = [] female_patients = [] with open('../data/csv/patients.csv') as f: reader = csv.DictReader(f) for i, row in enumerate(reader): patient_gender = row['GENDER'] if patient_gender == 'M': male_patients.append(row) elif patient_gender == 'F': female_patients.append(row) else: raise Exception('Unknown Gender') pprint(male_patients[0]) pprint(female_patients[0]) . {'ADDRESS': '657 Heathcote Divide', 'BIRTHDATE': '2015-07-12', 'BIRTHPLACE': 'Newton Massachusetts US', 'CITY': 'Fitchburg', 'DEATHDATE': '', 'DRIVERS': '', 'ETHNICITY': 'polish', 'FIRST': 'Victor265', 'GENDER': 'M', 'Id': '3287bb9c-e395-4146-8dd5-1fc3d887d220', 'LAST': 'Kilback373', 'MAIDEN': '', 'MARITAL': '', 'PASSPORT': '', 'PREFIX': '', 'RACE': 'white', 'SSN': '999-82-9751', 'STATE': 'Massachusetts', 'SUFFIX': '', 'ZIP': '01420'} {'ADDRESS': '419 Mraz Ranch Apt 13', 'BIRTHDATE': '2003-04-15', 'BIRTHPLACE': 'Ipswich Massachusetts US', 'CITY': 'Saugus', 'DEATHDATE': '', 'DRIVERS': 'S99990332', 'ETHNICITY': 'puerto_rican', 'FIRST': 'Reena181', 'GENDER': 'F', 'Id': 'e118c014-fef7-4fbd-8c5d-fe42e9e16d84', 'LAST': 'Dooley940', 'MAIDEN': '', 'MARITAL': '', 'PASSPORT': '', 'PREFIX': '', 'RACE': 'hispanic', 'SSN': '999-57-7583', 'STATE': 'Massachusetts', 'SUFFIX': '', 'ZIP': '01906'} . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.b%20-%20Grouping%20Data.html#grouping-data-using-python",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.b%20-%20Grouping%20Data.html#grouping-data-using-python"
  },"58": {
    "doc": "01.b - Grouping Data",
    "title": "Can we do better?",
    "content": "What’s wrong with the code above? . | Multiple iterations over the file | Brittle… not resilient to new genders | . What can we do? . | Use a dictionary to store the groupings | Make the code case insensitive | . # patients by gender patients_by_gender = {} patients_by_gender['F'] = ['patient1', 'patient3'] patients_by_gender['M'] = ['patient2', 'patient4'] pprint(patients_by_gender) . {'F': ['patient1', 'patient3'], 'M': ['patient2', 'patient4']} . print(patients_by_gender.keys()) . dict_keys(['F', 'M']) . print(patients_by_gender.values()) . dict_values([['patient1', 'patient3'], ['patient2', 'patient4']]) . print(patients_by_gender.items()) . dict_items([('F', ['patient1', 'patient3']), ('M', ['patient2', 'patient4'])]) . # check if a gender is in the dictionary print('F' in patients_by_gender) . True . print('f' in patients_by_gender) . False . # let's group using a dictionary patients_by_gender = {} with open('../data/csv/patients.csv') as f: reader = csv.DictReader(f) for row in reader: patient_gender = row['GENDER'].upper() # let's store the keys as uppercase # check to see if the key exists, if not if patient_gender not in patients_by_gender: # add the key patients_by_gender[patient_gender] = [] # create an empty list # append the patient as a new row to the correct grouping patients_by_gender[patient_gender].append(row) print(patients_by_gender.keys()) . dict_keys(['M', 'F']) . pprint(patients_by_gender['F'][0:2]) . [{'ADDRESS': '419 Mraz Ranch Apt 13', 'BIRTHDATE': '2003-04-15', 'BIRTHPLACE': 'Ipswich Massachusetts US', 'CITY': 'Saugus', 'DEATHDATE': '', 'DRIVERS': 'S99990332', 'ETHNICITY': 'puerto_rican', 'FIRST': 'Reena181', 'GENDER': 'F', 'Id': 'e118c014-fef7-4fbd-8c5d-fe42e9e16d84', 'LAST': 'Dooley940', 'MAIDEN': '', 'MARITAL': '', 'PASSPORT': '', 'PREFIX': '', 'RACE': 'hispanic', 'SSN': '999-57-7583', 'STATE': 'Massachusetts', 'SUFFIX': '', 'ZIP': '01906'}, {'ADDRESS': '570 Gleichner Branch Suite 81', 'BIRTHDATE': '2008-06-26', 'BIRTHPLACE': 'Danvers Massachusetts US', 'CITY': 'Lynn', 'DEATHDATE': '', 'DRIVERS': '', 'ETHNICITY': 'english', 'FIRST': 'Brinda322', 'GENDER': 'F', 'Id': '4a2a4a0b-0c4a-47b3-9b93-ce82048a41e6', 'LAST': 'Harber290', 'MAIDEN': '', 'MARITAL': '', 'PASSPORT': '', 'PREFIX': '', 'RACE': 'white', 'SSN': '999-57-3145', 'STATE': 'Massachusetts', 'SUFFIX': '', 'ZIP': '01901'}] . print(patients_by_gender['F'][0]['LAST']) . Dooley940 . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.b%20-%20Grouping%20Data.html#can-we-do-better",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.b%20-%20Grouping%20Data.html#can-we-do-better"
  },"59": {
    "doc": "01.b - Grouping Data",
    "title": "Let’s make the code reusable",
    "content": ". | Create a function | Externalize parameters that will change from the function | . What’s common? . | The csv processing | The grouping logic | . What’s different? . | The file name | The groupby parameters | . # define the function def group_patient(file_name, groupby): pass . # call the function patients_by_gender = group_patient('../data/csv/patients.csv', 'GENDER') print(patients_by_gender) . None . # now let's implement the function def group_patient(file_name, groupby): patients_by_group = {} with open(file_name) as f: reader = csv.DictReader(f) for row in reader: # note : we renamed the variable to _attribute patient_attribute = row[groupby].upper() # check to see if the key exists, if not if patient_attribute not in patients_by_group: # add the key patients_by_group[patient_attribute] = [] # create an empty list # append the patient as a new row to the correct grouping patients_by_group[patient_attribute].append(row) return patients_by_group . patients_by_gender = group_patient('../data/csv/patients.csv', 'GENDER') pprint(patients_by_gender['F'][0:2]) . [{'ADDRESS': '419 Mraz Ranch Apt 13', 'BIRTHDATE': '2003-04-15', 'BIRTHPLACE': 'Ipswich Massachusetts US', 'CITY': 'Saugus', 'DEATHDATE': '', 'DRIVERS': 'S99990332', 'ETHNICITY': 'puerto_rican', 'FIRST': 'Reena181', 'GENDER': 'F', 'Id': 'e118c014-fef7-4fbd-8c5d-fe42e9e16d84', 'LAST': 'Dooley940', 'MAIDEN': '', 'MARITAL': '', 'PASSPORT': '', 'PREFIX': '', 'RACE': 'hispanic', 'SSN': '999-57-7583', 'STATE': 'Massachusetts', 'SUFFIX': '', 'ZIP': '01906'}, {'ADDRESS': '570 Gleichner Branch Suite 81', 'BIRTHDATE': '2008-06-26', 'BIRTHPLACE': 'Danvers Massachusetts US', 'CITY': 'Lynn', 'DEATHDATE': '', 'DRIVERS': '', 'ETHNICITY': 'english', 'FIRST': 'Brinda322', 'GENDER': 'F', 'Id': '4a2a4a0b-0c4a-47b3-9b93-ce82048a41e6', 'LAST': 'Harber290', 'MAIDEN': '', 'MARITAL': '', 'PASSPORT': '', 'PREFIX': '', 'RACE': 'white', 'SSN': '999-57-3145', 'STATE': 'Massachusetts', 'SUFFIX': '', 'ZIP': '01901'}] . # but wait... is there anything patient specific??? # let's refactor the code to make it more extensible # and easier to read def group(file_name, groupby): grouped_data = {} with open(file_name) as f: reader = csv.DictReader(f) for row in reader: attribute = row[groupby].upper() if attribute not in grouped_data: grouped_data[attribute] = [] grouped_data[attribute].append(row) return grouped_data . patients_by_gender = group('../data/csv/patients.csv', 'GENDER') pprint(patients_by_gender['F'][0]) . {'ADDRESS': '419 Mraz Ranch Apt 13', 'BIRTHDATE': '2003-04-15', 'BIRTHPLACE': 'Ipswich Massachusetts US', 'CITY': 'Saugus', 'DEATHDATE': '', 'DRIVERS': 'S99990332', 'ETHNICITY': 'puerto_rican', 'FIRST': 'Reena181', 'GENDER': 'F', 'Id': 'e118c014-fef7-4fbd-8c5d-fe42e9e16d84', 'LAST': 'Dooley940', 'MAIDEN': '', 'MARITAL': '', 'PASSPORT': '', 'PREFIX': '', 'RACE': 'hispanic', 'SSN': '999-57-7583', 'STATE': 'Massachusetts', 'SUFFIX': '', 'ZIP': '01906'} . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.b%20-%20Grouping%20Data.html#lets-make-the-code-reusable",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.b%20-%20Grouping%20Data.html#lets-make-the-code-reusable"
  },"60": {
    "doc": "01.b - Grouping Data",
    "title": "What if we want multiple grouping levels?",
    "content": "# single level grouping by gender { \"F\": [ 'patient1', 'patient2'], \"M\": [ 'patient3', 'patient4'] } . {'F': ['patient1', 'patient2'], 'M': ['patient3', 'patient4']} . # multi level grouping by gender and race { \"F\": { \"WHITE\": ['patient1', 'patient2'], \"HISPANIC\": ['patient3'] }, \"M\": { \"WHITE\": [], \"HISPANIC\": ['patient4', 'patient5'] } } . {'F': {'WHITE': ['patient1', 'patient2'], 'HISPANIC': ['patient3']}, 'M': {'WHITE': [], 'HISPANIC': ['patient4', 'patient5']}} . def group_file(file_name, groupby): with open(file_name) as f: reader = csv.DictReader(f) return group(reader, groupby) def group(iterable, groupby): grouped_data = {} for item in iterable: attribute = item[groupby].upper() if attribute not in grouped_data: grouped_data[attribute] = [] grouped_data[attribute].append(item) return grouped_data . patients_by_gender = group_file('../data/csv/patients.csv', 'GENDER') pprint(patients_by_gender['F'][0]) . {'ADDRESS': '419 Mraz Ranch Apt 13', 'BIRTHDATE': '2003-04-15', 'BIRTHPLACE': 'Ipswich Massachusetts US', 'CITY': 'Saugus', 'DEATHDATE': '', 'DRIVERS': 'S99990332', 'ETHNICITY': 'puerto_rican', 'FIRST': 'Reena181', 'GENDER': 'F', 'Id': 'e118c014-fef7-4fbd-8c5d-fe42e9e16d84', 'LAST': 'Dooley940', 'MAIDEN': '', 'MARITAL': '', 'PASSPORT': '', 'PREFIX': '', 'RACE': 'hispanic', 'SSN': '999-57-7583', 'STATE': 'Massachusetts', 'SUFFIX': '', 'ZIP': '01906'} . # patients by gender and race # here we'll use a dictionary to represent the 2nd level patients_by_gender_and_race = group_file('../data/csv/patients.csv', 'GENDER') for gender in patients_by_gender_and_race.keys(): patients_by_gender_and_race[gender] = {} pprint(patients_by_gender_and_race) . {'F': {}, 'M': {}} . # now let's perform the groupings patients_by_gender_and_race = group_file('../data/csv/patients.csv', 'GENDER') # print(patients_by_gender_and_race) for gender in patients_by_gender_and_race.keys(): patients_by_gender_and_race[gender] = group(patients_by_gender_and_race[gender], 'RACE') # print(patients_by_gender) . # finally let's print the unique genders and races for gender in patients_by_gender_and_race.keys(): for race in patients_by_gender_and_race[gender].keys(): print(gender, race) . M WHITE M ASIAN M NATIVE M HISPANIC M BLACK F HISPANIC F WHITE F BLACK F NATIVE F ASIAN . # as a sanity check let's look over the dataset again gender = set() race = set() with open('../data/csv/patients.csv') as f: reader = csv.DictReader(f) for row in reader: gender.add(row['GENDER']) race.add(row['RACE']) print(gender, race) . {'F', 'M'} {'black', 'white', 'asian', 'native', 'hispanic'} . # what about 3 levels of grouping? grouped_patients = group_file('../data/csv/patients.csv', 'GENDER') for gender in grouped_patients.keys(): grouped_patients[gender] = group(grouped_patients[gender], 'RACE') for race in grouped_patients[gender].keys(): grouped_patients[gender][race] = group(grouped_patients[gender][race], 'ETHNICITY') # print(grouped_patients) . # let's print the unique genders and races for gender in grouped_patients.keys(): for race in grouped_patients[gender].keys(): for ethnicity in grouped_patients[gender][race].keys(): print(gender, race, ethnicity) . M WHITE POLISH M WHITE PORTUGUESE M WHITE IRISH M WHITE FRENCH_CANADIAN M WHITE ITALIAN M WHITE GREEK M WHITE SCOTTISH M WHITE ENGLISH M WHITE AMERICAN M WHITE FRENCH M WHITE GERMAN M ASIAN CHINESE M ASIAN ASIAN_INDIAN M NATIVE AMERICAN_INDIAN M HISPANIC PUERTO_RICAN M HISPANIC SOUTH_AMERICAN M HISPANIC MEXICAN M BLACK DOMINICAN M BLACK WEST_INDIAN F HISPANIC PUERTO_RICAN F WHITE ENGLISH F WHITE FRENCH F WHITE ITALIAN F WHITE GERMAN F WHITE FRENCH_CANADIAN F WHITE IRISH F WHITE PORTUGUESE F WHITE SCOTTISH F WHITE POLISH F BLACK DOMINICAN F NATIVE AMERICAN_INDIAN F ASIAN CHINESE . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.b%20-%20Grouping%20Data.html#what-if-we-want-multiple-grouping-levels",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.b%20-%20Grouping%20Data.html#what-if-we-want-multiple-grouping-levels"
  },"61": {
    "doc": "01.b - Grouping Data",
    "title": "What about 4 levels of grouping?",
    "content": "I think we should refactor again. This time using recursion. This time we’ll support any number of groupings. def group_file_by_list(file_name, groupings): with open(file_name) as f: reader = csv.DictReader(f) grouped_data = group(reader, groupings[0]) if len(groupings) &gt; 1: group_by_list(grouped_data.keys(), grouped_data, groupings[1:]) return grouped_data def group_by_list(iterable, grouped_data, groupings): for item in iterable: grouped_data[item] = group(grouped_data[item], groupings[0]) if len(groupings) &gt; 1: group_by_list(grouped_data[item].keys(), grouped_data[item], groupings[1:]) return grouped_data . groupings = ['GENDER', 'RACE', 'ETHNICITY', 'BIRTHPLACE'] grouped_patients = group_file_by_list('../data/csv/patients.csv', groupings) . for gender in grouped_patients.keys(): for race in grouped_patients[gender].keys(): for ethnicity in grouped_patients[gender][race].keys(): print(gender, race, ethnicity) . M WHITE POLISH M WHITE PORTUGUESE M WHITE IRISH M WHITE FRENCH_CANADIAN M WHITE ITALIAN M WHITE GREEK M WHITE SCOTTISH M WHITE ENGLISH M WHITE AMERICAN M WHITE FRENCH M WHITE GERMAN M ASIAN CHINESE M ASIAN ASIAN_INDIAN M NATIVE AMERICAN_INDIAN M HISPANIC PUERTO_RICAN M HISPANIC SOUTH_AMERICAN M HISPANIC MEXICAN M BLACK DOMINICAN M BLACK WEST_INDIAN F HISPANIC PUERTO_RICAN F WHITE ENGLISH F WHITE FRENCH F WHITE ITALIAN F WHITE GERMAN F WHITE FRENCH_CANADIAN F WHITE IRISH F WHITE PORTUGUESE F WHITE SCOTTISH F WHITE POLISH F BLACK DOMINICAN F NATIVE AMERICAN_INDIAN F ASIAN CHINESE . # let's create a recursive print def print_keys(d): for k, v in d.items(): print_keys2(v, [k]) def print_keys2(d, l): if isinstance(d, dict): for k, v in d.items(): print_keys2(v, l + [k]) else: print(' '.join(l)) l = [] print_keys(grouped_patients) . M WHITE POLISH NEWTON MASSACHUSETTS US M WHITE POLISH ADAMS MASSACHUSETTS US M WHITE POLISH BOSTON MASSACHUSETTS US M WHITE POLISH HOLYOKE MASSACHUSETTS US M WHITE PORTUGUESE PORTO DOURO LITORAL PT M WHITE PORTUGUESE BRAGA MINHO PT M WHITE PORTUGUESE YARMOUTH MASSACHUSETTS US M WHITE IRISH CHICOPEE MASSACHUSETTS US M WHITE IRISH WORCESTER MASSACHUSETTS US M WHITE IRISH WESTPORT MASSACHUSETTS US M WHITE IRISH WEYMOUTH TOWN MASSACHUSETTS US M WHITE IRISH MASHPEE MASSACHUSETTS US M WHITE IRISH PITTSFIELD MASSACHUSETTS US M WHITE FRENCH_CANADIAN LENOX MASSACHUSETTS US M WHITE FRENCH_CANADIAN BOSTON MASSACHUSETTS US M WHITE FRENCH_CANADIAN SHERBORN MASSACHUSETTS US M WHITE FRENCH_CANADIAN HUDSON MASSACHUSETTS US M WHITE FRENCH_CANADIAN SANDWICH MASSACHUSETTS US M WHITE FRENCH_CANADIAN WORCESTER MASSACHUSETTS US M WHITE ITALIAN FALL RIVER MASSACHUSETTS US M WHITE ITALIAN HAVERHILL MASSACHUSETTS US M WHITE ITALIAN BOSTON MASSACHUSETTS US M WHITE ITALIAN SUDBURY MASSACHUSETTS US M WHITE ITALIAN SOMERVILLE MASSACHUSETTS US M WHITE ITALIAN MANSFIELD MASSACHUSETTS US M WHITE ITALIAN BELCHERTOWN MASSACHUSETTS US M WHITE ITALIAN NORTH ATTLEBOROUGH MASSACHUSETTS US M WHITE ITALIAN FAIRHAVEN MASSACHUSETTS US M WHITE ITALIAN MIDDLETON MASSACHUSETTS US M WHITE ITALIAN BEVERLY MASSACHUSETTS US M WHITE GREEK ARLINGTON MASSACHUSETTS US M WHITE GREEK WORCESTER MASSACHUSETTS US M WHITE SCOTTISH PITTSFIELD MASSACHUSETTS US M WHITE ENGLISH HAVERHILL MASSACHUSETTS US M WHITE ENGLISH BOSTON MASSACHUSETTS US M WHITE ENGLISH SHARON MASSACHUSETTS US M WHITE ENGLISH BEVERLY MASSACHUSETTS US M WHITE ENGLISH ATTLEBORO MASSACHUSETTS US M WHITE ENGLISH NEW BEDFORD MASSACHUSETTS US M WHITE ENGLISH WORCESTER MASSACHUSETTS US M WHITE ENGLISH SOMERVILLE MASSACHUSETTS US M WHITE ENGLISH CHICOPEE MASSACHUSETTS US M WHITE ENGLISH REVERE MASSACHUSETTS US M WHITE ENGLISH RUTLAND MASSACHUSETTS US M WHITE ENGLISH PLYMOUTH MASSACHUSETTS US M WHITE ENGLISH NORTHAMPTON MASSACHUSETTS US M WHITE ENGLISH DARTMOUTH MASSACHUSETTS US M WHITE ENGLISH NATICK MASSACHUSETTS US M WHITE ENGLISH WHATELY MASSACHUSETTS US M WHITE ENGLISH SOUTHWICK MASSACHUSETTS US M WHITE AMERICAN SPRINGFIELD MASSACHUSETTS US M WHITE FRENCH SHREWSBURY MASSACHUSETTS US M WHITE GERMAN WALTHAM MASSACHUSETTS US M ASIAN CHINESE QUINCY MASSACHUSETTS US M ASIAN ASIAN_INDIAN ROCKLAND MASSACHUSETTS US M NATIVE AMERICAN_INDIAN NEW BEDFORD MASSACHUSETTS US M HISPANIC PUERTO_RICAN TEMPLETON MASSACHUSETTS US M HISPANIC PUERTO_RICAN LOWELL MASSACHUSETTS US M HISPANIC PUERTO_RICAN BAYAMON PUERTO RICO PR M HISPANIC PUERTO_RICAN PONCE PUERTO RICO PR M HISPANIC PUERTO_RICAN CAROLINA PUERTO RICO PR M HISPANIC SOUTH_AMERICAN RIO DE JANERIO RIO DE JANERIO BR M HISPANIC MEXICAN LA PAZ BAJA CALIFORNIA MX M BLACK DOMINICAN UXBRIDGE MASSACHUSETTS US M BLACK DOMINICAN DARTMOUTH MASSACHUSETTS US M BLACK WEST_INDIAN SANTIAGO DE LOS CABALLEROS SANTIAGO DO M BLACK WEST_INDIAN KINGSTON KINGSTON JM F HISPANIC PUERTO_RICAN IPSWICH MASSACHUSETTS US F HISPANIC PUERTO_RICAN BAYAMON PUERTO RICO PR F HISPANIC PUERTO_RICAN CAROLINA PUERTO RICO PR F WHITE ENGLISH DANVERS MASSACHUSETTS US F WHITE ENGLISH BELCHERTOWN MASSACHUSETTS US F WHITE ENGLISH HARVARD MASSACHUSETTS US F WHITE ENGLISH STERLING MASSACHUSETTS US F WHITE ENGLISH NATICK MASSACHUSETTS US F WHITE ENGLISH HOPKINTON MASSACHUSETTS US F WHITE FRENCH CHICOPEE MASSACHUSETTS US F WHITE FRENCH STOUGHTON MASSACHUSETTS US F WHITE FRENCH BOSTON MASSACHUSETTS US F WHITE FRENCH SALEM MASSACHUSETTS US F WHITE FRENCH CONCORD MASSACHUSETTS US F WHITE ITALIAN MILFORD MASSACHUSETTS US F WHITE ITALIAN SPRINGFIELD MASSACHUSETTS US F WHITE ITALIAN MILTON MASSACHUSETTS US F WHITE ITALIAN DRACUT MASSACHUSETTS US F WHITE ITALIAN PALERMO SICILY IT F WHITE ITALIAN NEW BEDFORD MASSACHUSETTS US F WHITE ITALIAN SOUTHWICK MASSACHUSETTS US F WHITE GERMAN NORTH ADAMS MASSACHUSETTS US F WHITE GERMAN ATTLEBORO MASSACHUSETTS US F WHITE GERMAN EASTHAMPTON TOWN MASSACHUSETTS US F WHITE GERMAN QUINCY MASSACHUSETTS US F WHITE GERMAN REVERE MASSACHUSETTS US F WHITE FRENCH_CANADIAN BILLERICA MASSACHUSETTS US F WHITE FRENCH_CANADIAN STONEHAM MASSACHUSETTS US F WHITE FRENCH_CANADIAN HOLYOKE MASSACHUSETTS US F WHITE FRENCH_CANADIAN ROCKLAND MASSACHUSETTS US F WHITE FRENCH_CANADIAN AGAWAM TOWN MASSACHUSETTS US F WHITE FRENCH_CANADIAN WEST SPRINGFIELD TOWN MASSACHUSETTS US F WHITE FRENCH_CANADIAN BOSTON MASSACHUSETTS US F WHITE FRENCH_CANADIAN LOWELL MASSACHUSETTS US F WHITE FRENCH_CANADIAN PALMER TOWN MASSACHUSETTS US F WHITE IRISH LYNN MASSACHUSETTS US F WHITE IRISH FRAMINGHAM MASSACHUSETTS US F WHITE IRISH PALMER TOWN MASSACHUSETTS US F WHITE IRISH BOSTON MASSACHUSETTS US F WHITE IRISH BRAINTREE TOWN MASSACHUSETTS US F WHITE IRISH SOMERSET MASSACHUSETTS US F WHITE PORTUGUESE FUNCHAL MADEIRA PT F WHITE PORTUGUESE BELMONT MASSACHUSETTS US F WHITE PORTUGUESE PORTO DOURO LITORAL PT F WHITE PORTUGUESE MARSHFIELD MASSACHUSETTS US F WHITE PORTUGUESE FARO ALGARVE PT F WHITE SCOTTISH EAST LONGMEADOW MASSACHUSETTS US F WHITE SCOTTISH WINDSOR MASSACHUSETTS US F WHITE SCOTTISH BOSTON MASSACHUSETTS US F WHITE POLISH TAUNTON MASSACHUSETTS US F WHITE POLISH CONCORD MASSACHUSETTS US F BLACK DOMINICAN SALISBURY SAINT JOSEPH PARISH DM F BLACK DOMINICAN FALL RIVER MASSACHUSETTS US F BLACK DOMINICAN MARIGOT SAINT ANDREW PARISH DM F NATIVE AMERICAN_INDIAN STONEHAM MASSACHUSETTS US F ASIAN CHINESE WENHAM MASSACHUSETTS US . # lets try it out on a few examples . grouped_patients = group_file_by_list('../data/csv/patients.csv', ['GENDER']) print_keys(grouped_patients) . M F . grouped_patients = group_file_by_list('../data/csv/patients.csv', ['RACE']) print_keys(grouped_patients) . WHITE HISPANIC ASIAN NATIVE BLACK . grouped_patients = group_file_by_list('../data/csv/patients.csv', ['GENDER', 'RACE']) print_keys(grouped_patients) . M WHITE M ASIAN M NATIVE M HISPANIC M BLACK F HISPANIC F WHITE F BLACK F NATIVE F ASIAN . grouped_patients = group_file_by_list('../data/csv/patients.csv', ['RACE', 'GENDER']) print_keys(grouped_patients) . WHITE M WHITE F HISPANIC F HISPANIC M ASIAN M ASIAN F NATIVE M NATIVE F BLACK M BLACK F . grouped_patients = group_file_by_list('../data/csv/patients.csv', ['ETHNICITY']) print_keys(grouped_patients) . POLISH PUERTO_RICAN ENGLISH FRENCH ITALIAN PORTUGUESE IRISH GERMAN CHINESE FRENCH_CANADIAN AMERICAN_INDIAN SCOTTISH DOMINICAN SOUTH_AMERICAN GREEK AMERICAN WEST_INDIAN MEXICAN ASIAN_INDIAN . grouped_patients = group_file_by_list('../data/csv/patients.csv', ['RACE', 'ETHNICITY']) print_keys(grouped_patients) . WHITE POLISH WHITE ENGLISH WHITE FRENCH WHITE ITALIAN WHITE PORTUGUESE WHITE IRISH WHITE GERMAN WHITE FRENCH_CANADIAN WHITE SCOTTISH WHITE GREEK WHITE AMERICAN HISPANIC PUERTO_RICAN HISPANIC SOUTH_AMERICAN HISPANIC MEXICAN ASIAN CHINESE ASIAN ASIAN_INDIAN NATIVE AMERICAN_INDIAN BLACK DOMINICAN BLACK WEST_INDIAN . grouped_patients = group_file_by_list('../data/csv/patients.csv', ['RACE', 'ETHNICITY', 'BIRTHPLACE', 'GENDER']) print_keys(grouped_patients) . WHITE POLISH NEWTON MASSACHUSETTS US M WHITE POLISH TAUNTON MASSACHUSETTS US F WHITE POLISH ADAMS MASSACHUSETTS US M WHITE POLISH BOSTON MASSACHUSETTS US M WHITE POLISH CONCORD MASSACHUSETTS US F WHITE POLISH HOLYOKE MASSACHUSETTS US M WHITE ENGLISH DANVERS MASSACHUSETTS US F WHITE ENGLISH BELCHERTOWN MASSACHUSETTS US F WHITE ENGLISH HAVERHILL MASSACHUSETTS US M WHITE ENGLISH BOSTON MASSACHUSETTS US M WHITE ENGLISH SHARON MASSACHUSETTS US M WHITE ENGLISH BEVERLY MASSACHUSETTS US M WHITE ENGLISH HARVARD MASSACHUSETTS US F WHITE ENGLISH ATTLEBORO MASSACHUSETTS US M WHITE ENGLISH NEW BEDFORD MASSACHUSETTS US M WHITE ENGLISH STERLING MASSACHUSETTS US F WHITE ENGLISH WORCESTER MASSACHUSETTS US M WHITE ENGLISH SOMERVILLE MASSACHUSETTS US M WHITE ENGLISH NATICK MASSACHUSETTS US F WHITE ENGLISH NATICK MASSACHUSETTS US M WHITE ENGLISH HOPKINTON MASSACHUSETTS US F WHITE ENGLISH CHICOPEE MASSACHUSETTS US M WHITE ENGLISH REVERE MASSACHUSETTS US M WHITE ENGLISH RUTLAND MASSACHUSETTS US M WHITE ENGLISH PLYMOUTH MASSACHUSETTS US M WHITE ENGLISH NORTHAMPTON MASSACHUSETTS US M WHITE ENGLISH DARTMOUTH MASSACHUSETTS US M WHITE ENGLISH WHATELY MASSACHUSETTS US M WHITE ENGLISH SOUTHWICK MASSACHUSETTS US M WHITE FRENCH CHICOPEE MASSACHUSETTS US F WHITE FRENCH STOUGHTON MASSACHUSETTS US F WHITE FRENCH SHREWSBURY MASSACHUSETTS US M WHITE FRENCH BOSTON MASSACHUSETTS US F WHITE FRENCH SALEM MASSACHUSETTS US F WHITE FRENCH CONCORD MASSACHUSETTS US F WHITE ITALIAN MILFORD MASSACHUSETTS US F WHITE ITALIAN SPRINGFIELD MASSACHUSETTS US F WHITE ITALIAN FALL RIVER MASSACHUSETTS US M WHITE ITALIAN HAVERHILL MASSACHUSETTS US M WHITE ITALIAN BOSTON MASSACHUSETTS US M WHITE ITALIAN SUDBURY MASSACHUSETTS US M WHITE ITALIAN SOMERVILLE MASSACHUSETTS US M WHITE ITALIAN MANSFIELD MASSACHUSETTS US M WHITE ITALIAN BELCHERTOWN MASSACHUSETTS US M WHITE ITALIAN NORTH ATTLEBOROUGH MASSACHUSETTS US M WHITE ITALIAN FAIRHAVEN MASSACHUSETTS US M WHITE ITALIAN MIDDLETON MASSACHUSETTS US M WHITE ITALIAN MILTON MASSACHUSETTS US F WHITE ITALIAN BEVERLY MASSACHUSETTS US M WHITE ITALIAN DRACUT MASSACHUSETTS US F WHITE ITALIAN PALERMO SICILY IT F WHITE ITALIAN NEW BEDFORD MASSACHUSETTS US F WHITE ITALIAN SOUTHWICK MASSACHUSETTS US F WHITE PORTUGUESE PORTO DOURO LITORAL PT M WHITE PORTUGUESE PORTO DOURO LITORAL PT F WHITE PORTUGUESE BRAGA MINHO PT M WHITE PORTUGUESE FUNCHAL MADEIRA PT F WHITE PORTUGUESE BELMONT MASSACHUSETTS US F WHITE PORTUGUESE MARSHFIELD MASSACHUSETTS US F WHITE PORTUGUESE FARO ALGARVE PT F WHITE PORTUGUESE YARMOUTH MASSACHUSETTS US M WHITE IRISH CHICOPEE MASSACHUSETTS US M WHITE IRISH WORCESTER MASSACHUSETTS US M WHITE IRISH LYNN MASSACHUSETTS US F WHITE IRISH WESTPORT MASSACHUSETTS US M WHITE IRISH FRAMINGHAM MASSACHUSETTS US F WHITE IRISH PALMER TOWN MASSACHUSETTS US F WHITE IRISH BOSTON MASSACHUSETTS US F WHITE IRISH WEYMOUTH TOWN MASSACHUSETTS US M WHITE IRISH MASHPEE MASSACHUSETTS US M WHITE IRISH PITTSFIELD MASSACHUSETTS US M WHITE IRISH BRAINTREE TOWN MASSACHUSETTS US F WHITE IRISH SOMERSET MASSACHUSETTS US F WHITE GERMAN NORTH ADAMS MASSACHUSETTS US F WHITE GERMAN ATTLEBORO MASSACHUSETTS US F WHITE GERMAN EASTHAMPTON TOWN MASSACHUSETTS US F WHITE GERMAN QUINCY MASSACHUSETTS US F WHITE GERMAN REVERE MASSACHUSETTS US F WHITE GERMAN WALTHAM MASSACHUSETTS US M WHITE FRENCH_CANADIAN BILLERICA MASSACHUSETTS US F WHITE FRENCH_CANADIAN LENOX MASSACHUSETTS US M WHITE FRENCH_CANADIAN STONEHAM MASSACHUSETTS US F WHITE FRENCH_CANADIAN BOSTON MASSACHUSETTS US M WHITE FRENCH_CANADIAN BOSTON MASSACHUSETTS US F WHITE FRENCH_CANADIAN SHERBORN MASSACHUSETTS US M WHITE FRENCH_CANADIAN HUDSON MASSACHUSETTS US M WHITE FRENCH_CANADIAN SANDWICH MASSACHUSETTS US M WHITE FRENCH_CANADIAN WORCESTER MASSACHUSETTS US M WHITE FRENCH_CANADIAN HOLYOKE MASSACHUSETTS US F WHITE FRENCH_CANADIAN ROCKLAND MASSACHUSETTS US F WHITE FRENCH_CANADIAN AGAWAM TOWN MASSACHUSETTS US F WHITE FRENCH_CANADIAN WEST SPRINGFIELD TOWN MASSACHUSETTS US F WHITE FRENCH_CANADIAN LOWELL MASSACHUSETTS US F WHITE FRENCH_CANADIAN PALMER TOWN MASSACHUSETTS US F WHITE SCOTTISH EAST LONGMEADOW MASSACHUSETTS US F WHITE SCOTTISH PITTSFIELD MASSACHUSETTS US M WHITE SCOTTISH WINDSOR MASSACHUSETTS US F WHITE SCOTTISH BOSTON MASSACHUSETTS US F WHITE GREEK ARLINGTON MASSACHUSETTS US M WHITE GREEK WORCESTER MASSACHUSETTS US M WHITE AMERICAN SPRINGFIELD MASSACHUSETTS US M HISPANIC PUERTO_RICAN IPSWICH MASSACHUSETTS US F HISPANIC PUERTO_RICAN TEMPLETON MASSACHUSETTS US M HISPANIC PUERTO_RICAN BAYAMON PUERTO RICO PR F HISPANIC PUERTO_RICAN BAYAMON PUERTO RICO PR M HISPANIC PUERTO_RICAN CAROLINA PUERTO RICO PR F HISPANIC PUERTO_RICAN CAROLINA PUERTO RICO PR M HISPANIC PUERTO_RICAN LOWELL MASSACHUSETTS US M HISPANIC PUERTO_RICAN PONCE PUERTO RICO PR M HISPANIC SOUTH_AMERICAN RIO DE JANERIO RIO DE JANERIO BR M HISPANIC MEXICAN LA PAZ BAJA CALIFORNIA MX M ASIAN CHINESE QUINCY MASSACHUSETTS US M ASIAN CHINESE WENHAM MASSACHUSETTS US F ASIAN ASIAN_INDIAN ROCKLAND MASSACHUSETTS US M NATIVE AMERICAN_INDIAN NEW BEDFORD MASSACHUSETTS US M NATIVE AMERICAN_INDIAN STONEHAM MASSACHUSETTS US F BLACK DOMINICAN UXBRIDGE MASSACHUSETTS US M BLACK DOMINICAN SALISBURY SAINT JOSEPH PARISH DM F BLACK DOMINICAN FALL RIVER MASSACHUSETTS US F BLACK DOMINICAN DARTMOUTH MASSACHUSETTS US M BLACK DOMINICAN MARIGOT SAINT ANDREW PARISH DM F BLACK WEST_INDIAN SANTIAGO DE LOS CABALLEROS SANTIAGO DO M BLACK WEST_INDIAN KINGSTON KINGSTON JM M . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.b%20-%20Grouping%20Data.html#what-about-4-levels-of-grouping",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.b%20-%20Grouping%20Data.html#what-about-4-levels-of-grouping"
  },"62": {
    "doc": "01.b - Grouping Data",
    "title": "01.b - Grouping Data",
    "content": " ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.b%20-%20Grouping%20Data.html",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.b%20-%20Grouping%20Data.html"
  },"63": {
    "doc": "01.c - Descriptive Statistics",
    "title": "Descriptive Statistics",
    "content": ". | Min | Max | Mean | Median | Standard Deviation | Variance | . First we’re going to define a method to get the patient’s age from their birthdate and then use that function to create a list of patient ages. import csv from datetime import date, timedelta from dateutil import parser def get_age(birth_date): if isinstance(birth_date, str): birth_date = parser.parse(birth_date).date() age = (date.today() - birth_date) // timedelta(days=365) return age . # calculate a list of patient ages patient_ages = [] with open('../data/csv/patients.csv') as f: reader = csv.DictReader(f) for i, row in enumerate(reader): date_of_birth = row['BIRTHDATE'] age = get_age(date_of_birth) patient_ages.append(age) print(patient_ages) . [6, 18, 13, 17, 8, 9, 55, 84, 14, 20, 53, 23, 18, 64, 11, 39, 34, 5, 82, 19, 18, 15, 26, 28, 29, 8, 71, 32, 82, 41, 51, 17, 40, 53, 60, 65, 11, 21, 32, 52, 3, 52, 82, 30, 69, 24, 99, 12, 51, 99, 27, 76, 78, 29, 16, 69, 34, 78, 58, 76, 99, 65, 70, 60, 50, 32, 4, 78, 65, 48, 3, 66, 77, 99, 25, 18, 47, 79, 99, 32, 10, 74, 61, 99, 62, 59, 67, 2, 50, 99, 3, 70, 61, 62, 43, 18, 71, 29, 99, 52, 71, 26, 28, 56, 32, 69, 23, 31, 46, 99, 106, 30, 21, 67, 67, 106, 99, 106, 99, 106, 106, 99, 106, 99, 99, 99, 99, 90, 90] . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.c%20-%20Descriptive%20Statistics.html#descriptive-statistics",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.c%20-%20Descriptive%20Statistics.html#descriptive-statistics"
  },"64": {
    "doc": "01.c - Descriptive Statistics",
    "title": "Min &amp; Max",
    "content": "# find min and max with a for loop: min_age = 0 max_age = 0 for i, age in enumerate(patient_ages): if i == 0: min_age = age max_age = age if age &lt; min_age: min_age = age if age &gt; max_age: max_age = age print(f'Min Age: {min_age}, Max Age: {max_age}') . Min Age: 2, Max Age: 106 . # find min and max with a for loop: min_age = patient_ages[0] max_age = patient_ages[0] for age in patient_ages: if age &lt; min_age: min_age = age if age &gt; max_age: max_age = age print(f'Min Age: {min_age}, Max Age: {max_age}') . Min Age: 2, Max Age: 106 . # find min and max by sorting sorted_ages = sorted(patient_ages) min_age = sorted_ages[0] max_age = sorted_ages[-1] print(f'Min Age: {min_age}, Max Age: {max_age}') . Min Age: 2, Max Age: 106 . # find min and max with reduce from functools import reduce min_age = reduce(lambda a, b : a if a &lt; b else b, patient_ages) max_age = reduce(lambda a, b : a if a &gt; b else b, patient_ages) print(f'Min Age: {min_age}, Max Age: {max_age}') . Min Age: 2, Max Age: 106 . # find min and max with min and max min_age = min(patient_ages) max_age = max(patient_ages) print(f'Min Age: {min_age}, Max Age: {max_age}') . Min Age: 2, Max Age: 106 . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.c%20-%20Descriptive%20Statistics.html#min--max",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.c%20-%20Descriptive%20Statistics.html#min--max"
  },"65": {
    "doc": "01.c - Descriptive Statistics",
    "title": "Mean",
    "content": "# calculate mean with a for loop total_age = 0 age_count = 0 for age in patient_ages: total_age += age age_count += 1 mean_age = total_age / age_count print(f'Mean Age: {mean_age}') . Mean Age: 52.62015503875969 . # calculate mean age with reduce import functools total_age = functools.reduce(lambda a, b : a + b, patient_ages) age_count = len(patient_ages) mean_age = total_age / age_count print(f'Mean Age: {mean_age}') . Mean Age: 52.62015503875969 . # calculate mean age with sum and len mean_age = sum(patient_ages) / len(patient_ages) print(f'Mean Age: {mean_age}') . Mean Age: 52.62015503875969 . # calculate mean with statistics package from statistics import mean mean_age = mean(patient_ages) print(f'Mean Age: {mean_age}') . Mean Age: 52.62015503875969 . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.c%20-%20Descriptive%20Statistics.html#mean",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.c%20-%20Descriptive%20Statistics.html#mean"
  },"66": {
    "doc": "01.c - Descriptive Statistics",
    "title": "Median",
    "content": "# calculate median with a for loop from math import floor sorted_ages = sorted(patient_ages) midpoint = len(sorted_ages) / 2 if midpoint.is_integer(): high_index = floor(midpoint) low_index = high_index - 1 median_age = (sorted_ages[high_index] + sorted_ages[low_index]) / 2 else: median_age = sorted_ages[floor(midpoint)] print(f'Median Age: {median_age}') . Median Age: 52 . # calculate median with statistics package from statistics import median median_age = median(patient_ages) print(f'Median Age: {median_age}') . Median Age: 52 . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.c%20-%20Descriptive%20Statistics.html#median",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.c%20-%20Descriptive%20Statistics.html#median"
  },"67": {
    "doc": "01.c - Descriptive Statistics",
    "title": "Variance",
    "content": "# calculate variance with a for loop mean_age = mean(patient_ages) squares = [] for age in patient_ages: squares.append((age - mean_age)**2) age_variance = mean(squares) print(f'Variance: {age_variance}') . Variance: 995.2433147046452 . # calculate variance with a list comprehension mean_age = mean(patient_ages) squares = [(age - mean_age)**2 for age in patient_ages] age_variance = mean(squares) print(f'Variance: {age_variance}') . Variance: 995.2433147046452 . # calculate variance with map and reduce from functools import reduce mean_age = mean(patient_ages) squares = map(lambda x: (x - mean_age) ** 2, patient_ages) age_variance = reduce(lambda a, b: a + b, squares) / len(patient_ages) print(f'Variance: {age_variance}') . Variance: 995.2433147046452 . # calculate variance with statistics package from statistics import pvariance age_variance = pvariance(patient_ages) print(f'Populaiton Variance: {age_variance}') . Populaiton Variance: 995.2433147046452 . # calculate variance with statistics package from statistics import variance age_variance = variance(patient_ages) print(f'Saple Variance: {age_variance}') . Saple Variance: 1003.0186531007752 . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.c%20-%20Descriptive%20Statistics.html#variance",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.c%20-%20Descriptive%20Statistics.html#variance"
  },"68": {
    "doc": "01.c - Descriptive Statistics",
    "title": "Standard Deviation",
    "content": "# calculate variance with statistics package import math from statistics import pvariance age_variance = pvariance(patient_ages) standard_devation = math.sqrt(age_variance) print(f'Standard Deviation: {standard_devation}') . Standard Deviation: 31.547477152771584 . # calculate variance with statistics package from statistics import pstdev standard_devation = pstdev(patient_ages) print(f'Standard Deviation: {standard_devation}') . Standard Deviation: 31.547477152771584 . # calculate standard deviation with statistics package from statistics import pstdev standard_deviation = pstdev(patient_ages) print(f'Standard Deviation: {standard_deviation}') . Standard Deviation: 31.547477152771584 . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.c%20-%20Descriptive%20Statistics.html#standard-deviation",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.c%20-%20Descriptive%20Statistics.html#standard-deviation"
  },"69": {
    "doc": "01.c - Descriptive Statistics",
    "title": "01.c - Descriptive Statistics",
    "content": " ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.c%20-%20Descriptive%20Statistics.html",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/01.c%20-%20Descriptive%20Statistics.html"
  },"70": {
    "doc": "01.c - JSON Files",
    "title": "JSON Files",
    "content": "Working With JSON Data in Python . from pprint import pprint # scores dictionary games = { 'games':[ { 'date': '2019-05-01', 'home_team': 'Pirates', 'home_score': 0, 'away_team': 'Cubs', 'away_score': 10 }, { 'date': '2019-05-15', 'home_team': 'Reds', 'home_score': 7, 'away_team': 'Pirates', 'away_score': 0 }, { 'date': '2019-05-17', 'home_team': 'Reds', 'home_score': 12, 'away_team': 'Pirates', 'away_score': 0 }, { 'date': '2019-05-18', 'home_team': 'Reds', 'home_score': 8, 'away_team': 'Pirates', 'away_score': 0 }, { 'date': '2019-05-21', 'home_team': 'Pirates', 'home_score': 1, 'away_team': 'Brewers', 'away_score': 11 } ] } pprint(games) . {'games': [{'away_score': 10, 'away_team': 'Cubs', 'date': '2019-05-01', 'home_score': 0, 'home_team': 'Pirates'}, {'away_score': 0, 'away_team': 'Pirates', 'date': '2019-05-15', 'home_score': 7, 'home_team': 'Reds'}, {'away_score': 0, 'away_team': 'Pirates', 'date': '2019-05-17', 'home_score': 12, 'home_team': 'Reds'}, {'away_score': 0, 'away_team': 'Pirates', 'date': '2019-05-18', 'home_score': 8, 'home_team': 'Reds'}, {'away_score': 11, 'away_team': 'Brewers', 'date': '2019-05-21', 'home_score': 1, 'home_team': 'Pirates'}]} . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#json-files",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#json-files"
  },"71": {
    "doc": "01.c - JSON Files",
    "title": "Python Serialization",
    "content": "In this instance, serialization is the process of converting the python objects to JSON. Python object types get serialized to the following JSON types. | Python | JSON | . | dict | object | . | list, tuple | array | . | str | string | . | int, long, float | number | . | True | true | . | False | false | . | None | null | . import json pprint(games) print() games_json = json.dumps(games) pprint(games_json) . {'games': [{'away_score': 10, 'away_team': 'Cubs', 'date': '2019-05-01', 'home_score': 0, 'home_team': 'Pirates'}, {'away_score': 0, 'away_team': 'Pirates', 'date': '2019-05-15', 'home_score': 7, 'home_team': 'Reds'}, {'away_score': 0, 'away_team': 'Pirates', 'date': '2019-05-17', 'home_score': 12, 'home_team': 'Reds'}, {'away_score': 0, 'away_team': 'Pirates', 'date': '2019-05-18', 'home_score': 8, 'home_team': 'Reds'}, {'away_score': 11, 'away_team': 'Brewers', 'date': '2019-05-21', 'home_score': 1, 'home_team': 'Pirates'}]} ('{\"games\": [{\"date\": \"2019-05-01\", \"home_team\": \"Pirates\", \"home_score\": 0, ' '\"away_team\": \"Cubs\", \"away_score\": 10}, {\"date\": \"2019-05-15\", \"home_team\": ' '\"Reds\", \"home_score\": 7, \"away_team\": \"Pirates\", \"away_score\": 0}, {\"date\": ' '\"2019-05-17\", \"home_team\": \"Reds\", \"home_score\": 12, \"away_team\": \"Pirates\", ' '\"away_score\": 0}, {\"date\": \"2019-05-18\", \"home_team\": \"Reds\", \"home_score\": ' '8, \"away_team\": \"Pirates\", \"away_score\": 0}, {\"date\": \"2019-05-21\", ' '\"home_team\": \"Pirates\", \"home_score\": 1, \"away_team\": \"Brewers\", ' '\"away_score\": 11}]}') . # changing whitespace games_json = json.dumps(games, indent=4) print(games_json) . { \"games\": [ { \"date\": \"2019-05-01\", \"home_team\": \"Pirates\", \"home_score\": 0, \"away_team\": \"Cubs\", \"away_score\": 10 }, { \"date\": \"2019-05-15\", \"home_team\": \"Reds\", \"home_score\": 7, \"away_team\": \"Pirates\", \"away_score\": 0 }, { \"date\": \"2019-05-17\", \"home_team\": \"Reds\", \"home_score\": 12, \"away_team\": \"Pirates\", \"away_score\": 0 }, { \"date\": \"2019-05-18\", \"home_team\": \"Reds\", \"home_score\": 8, \"away_team\": \"Pirates\", \"away_score\": 0 }, { \"date\": \"2019-05-21\", \"home_team\": \"Pirates\", \"home_score\": 1, \"away_team\": \"Brewers\", \"away_score\": 11 } ] } . # compacting games_json = json.dumps(games, separators=(',', ':')) print(games_json) . {\"games\":[{\"date\":\"2019-05-01\",\"home_team\":\"Pirates\",\"home_score\":0,\"away_team\":\"Cubs\",\"away_score\":10},{\"date\":\"2019-05-15\",\"home_team\":\"Reds\",\"home_score\":7,\"away_team\":\"Pirates\",\"away_score\":0},{\"date\":\"2019-05-17\",\"home_team\":\"Reds\",\"home_score\":12,\"away_team\":\"Pirates\",\"away_score\":0},{\"date\":\"2019-05-18\",\"home_team\":\"Reds\",\"home_score\":8,\"away_team\":\"Pirates\",\"away_score\":0},{\"date\":\"2019-05-21\",\"home_team\":\"Pirates\",\"home_score\":1,\"away_team\":\"Brewers\",\"away_score\":11}]} . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#python-serialization",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#python-serialization"
  },"72": {
    "doc": "01.c - JSON Files",
    "title": "Writing JSON files",
    "content": "with open(\"games.json\", \"w\") as f: json.dump(games, f) . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#writing-json-files",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#writing-json-files"
  },"73": {
    "doc": "01.c - JSON Files",
    "title": "Reading JSON files",
    "content": "# open and print line by line with open('games.json', \"r\") as f: games = json.load(f) print(games) . {'games': [{'date': '2019-05-01', 'home_team': 'Pirates', 'home_score': 0, 'away_team': 'Cubs', 'away_score': 10}, {'date': '2019-05-15', 'home_team': 'Reds', 'home_score': 7, 'away_team': 'Pirates', 'away_score': 0}, {'date': '2019-05-17', 'home_team': 'Reds', 'home_score': 12, 'away_team': 'Pirates', 'away_score': 0}, {'date': '2019-05-18', 'home_team': 'Reds', 'home_score': 8, 'away_team': 'Pirates', 'away_score': 0}, {'date': '2019-05-21', 'home_team': 'Pirates', 'home_score': 1, 'away_team': 'Brewers', 'away_score': 11}]} . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#reading-json-files",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#reading-json-files"
  },"74": {
    "doc": "01.c - JSON Files",
    "title": "Using JSONPath",
    "content": "Source . One of the biggest strengths of XML is XPath, the query-oriented language to query subsections of an XML document. In the same line, JSONPath is a query language with features similar to XPath that lets you extract just the bits of a JSON document your application needs. ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#using-jsonpath",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#using-jsonpath"
  },"75": {
    "doc": "01.c - JSON Files",
    "title": "JSONPath Syntax",
    "content": "As XPath, JSONPath also has syntax to follow: . | $ – symbol refers to the root object or element. | @ – symbol refers to the current object or element. | . – operator is the dot-child operator, which you use to denote a child element of the current element. | [ ] – is the subscript operator, which you use to denote a child element of the current element (by name or index). | * – operator is a wildcard, returning all objects or elements regardless of their names. | , – operator is the union operator, which returns the union of the children or indexes indicated. | : – operator is the array slice operator, so you can slice collections using the syntax [start:end:step] to return a subcollection of a collection. | ( ) – operator lets you pass a script expression in the underlying implementation’s script language. It’s not supported by every implementation of JSONPath, however. | ? ( ) – to query all items that meet a certain criteria. | . There are many online jsonpath validators out there, and I encourage you to try a few. ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#jsonpath-syntax",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#jsonpath-syntax"
  },"76": {
    "doc": "01.c - JSON Files",
    "title": "Using JSONPath in Python",
    "content": "To use JSONPath, you will need to include its dependency and then use it. The library we’ll use is jsonpath-rw-ext . pip install jsonpath-rw-ext . Once installed we’re ready to use it. ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#using-jsonpath-in-python",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#using-jsonpath-in-python"
  },"77": {
    "doc": "01.c - JSON Files",
    "title": "Finding Information about Pirate Games",
    "content": "Lets try to parse a JSON file a few different ways. First, we need to load the data as a JSON object into memory. with open('games.json', \"r\") as f: games = json.load(f) pprint(games) . {'games': [{'away_score': 10, 'away_team': 'Cubs', 'date': '2019-05-01', 'home_score': 0, 'home_team': 'Pirates'}, {'away_score': 0, 'away_team': 'Pirates', 'date': '2019-05-15', 'home_score': 7, 'home_team': 'Reds'}, {'away_score': 0, 'away_team': 'Pirates', 'date': '2019-05-17', 'home_score': 12, 'home_team': 'Reds'}, {'away_score': 0, 'away_team': 'Pirates', 'date': '2019-05-18', 'home_score': 8, 'home_team': 'Reds'}, {'away_score': 11, 'away_team': 'Brewers', 'date': '2019-05-21', 'home_score': 1, 'home_team': 'Pirates'}]} . Now that the data is loaded, we can try to answer a few questions. What dates were the games played on? . # json parsing game_dates = [] for game in games['games']: game_date = game['date'] game_dates.append(game_date) print(game_dates) . ['2019-05-01', '2019-05-15', '2019-05-17', '2019-05-18', '2019-05-21'] . # jsonpath parsing # import the library import jsonpath_rw_ext as jp . # create a jsonpath expression and match game_dates = jp.match(\"$.games[*].date\", games) print(game_dates) . ['2019-05-01', '2019-05-15', '2019-05-17', '2019-05-18', '2019-05-21'] . # this also works game_dates = jp.match(\"$.games..date\", games) print(game_dates) . ['2019-05-01', '2019-05-15', '2019-05-17', '2019-05-18', '2019-05-21'] . What were the scores for the Pirates home games? . # json parsing home_scores = [] for game in games['games']: if game['home_team'] == 'Pirates': home_scores.append(game) pprint(home_scores) . [{'away_score': 10, 'away_team': 'Cubs', 'date': '2019-05-01', 'home_score': 0, 'home_team': 'Pirates'}, {'away_score': 11, 'away_team': 'Brewers', 'date': '2019-05-21', 'home_score': 1, 'home_team': 'Pirates'}] . # jsonpath parsing home_scores = jp.match(\"$.games[?(@.home_team=='Pirates')]\", games) pprint(home_scores) . [{'away_score': 10, 'away_team': 'Cubs', 'date': '2019-05-01', 'home_score': 0, 'home_team': 'Pirates'}, {'away_score': 11, 'away_team': 'Brewers', 'date': '2019-05-21', 'home_score': 1, 'home_team': 'Pirates'}] . What were the scores after May 15th . # json parsing game_scores = [] for game in games['games']: if game['date'] &gt; '2019-05-15': game_scores.append(game) pprint(game_scores) . [{'away_score': 0, 'away_team': 'Pirates', 'date': '2019-05-17', 'home_score': 12, 'home_team': 'Reds'}, {'away_score': 0, 'away_team': 'Pirates', 'date': '2019-05-18', 'home_score': 8, 'home_team': 'Reds'}, {'away_score': 11, 'away_team': 'Brewers', 'date': '2019-05-21', 'home_score': 1, 'home_team': 'Pirates'}] . # jsonpath parsing game_scores = jp.match(\"$.games[?(@.date &gt; '2019-05-15')]\", games) pprint(game_scores) . [{'away_score': 0, 'away_team': 'Pirates', 'date': '2019-05-17', 'home_score': 12, 'home_team': 'Reds'}, {'away_score': 0, 'away_team': 'Pirates', 'date': '2019-05-18', 'home_score': 8, 'home_team': 'Reds'}, {'away_score': 11, 'away_team': 'Brewers', 'date': '2019-05-21', 'home_score': 1, 'home_team': 'Pirates'}] . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#finding-information-about-pirate-games",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html#finding-information-about-pirate-games"
  },"78": {
    "doc": "01.c - JSON Files",
    "title": "01.c - JSON Files",
    "content": " ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.c%20-%20JSON%20Files.html"
  },"79": {
    "doc": "01.d - XML Files",
    "title": "XML Files",
    "content": "Python XML with ElementTree: Beginner’s Guide . # scores dictionary scores_xml = \"\"\" &lt;?xml version=\"1.0\"?&gt; &lt;scores&gt; &lt;score&gt; &lt;date&gt;2019-05-01&lt;/date&gt; &lt;home-team&gt;Pirates&lt;/home-team&gt; &lt;home-score&gt;0&lt;/home-score&gt; &lt;away-team&gt;Cubs&lt;/away-team&gt; &lt;away-score&gt;10&lt;/away-score&gt; &lt;/score&gt; &lt;score&gt; &lt;date&gt;2019-05-15&lt;/date&gt; &lt;home-team&gt;Reds&lt;/home-team&gt; &lt;home-score&gt;7&lt;/home-score&gt; &lt;away-team&gt;Pirates&lt;/away-team&gt; &lt;away-score&gt;0&lt;/away-score&gt; &lt;/score&gt; &lt;/scores&gt; \"\"\" print(scores_xml) . &lt;?xml version=\"1.0\"?&gt; &lt;scores&gt; &lt;score&gt; &lt;date&gt;2019-05-01&lt;/date&gt; &lt;home-team&gt;Pirates&lt;/home-team&gt; &lt;home-score&gt;0&lt;/home-score&gt; &lt;away-team&gt;Cubs&lt;/away-team&gt; &lt;away-score&gt;10&lt;/away-score&gt; &lt;/score&gt; &lt;score&gt; &lt;date&gt;2019-05-15&lt;/date&gt; &lt;home-team&gt;Reds&lt;/home-team&gt; &lt;home-score&gt;7&lt;/home-score&gt; &lt;away-team&gt;Pirates&lt;/away-team&gt; &lt;away-score&gt;0&lt;/away-score&gt; &lt;/score&gt; &lt;/scores&gt; . scores_xsd = \"\"\" &lt;xs:schema attributeFormDefault=\"unqualified\" elementFormDefault=\"qualified\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"&gt; &lt;xs:element name=\"scores\"&gt; &lt;xs:complexType&gt; &lt;xs:sequence&gt; &lt;xs:element name=\"score\" maxOccurs=\"unbounded\" minOccurs=\"0\"&gt; &lt;xs:complexType&gt; &lt;xs:sequence&gt; &lt;xs:element type=\"xs:date\" name=\"date\"/&gt; &lt;xs:element type=\"xs:string\" name=\"home-team\"/&gt; &lt;xs:element type=\"xs:int\" name=\"home-score\"/&gt; &lt;xs:element type=\"xs:string\" name=\"away-team\"/&gt; &lt;xs:element type=\"xs:int\" name=\"away-score\"/&gt; &lt;/xs:sequence&gt; &lt;/xs:complexType&gt; &lt;/xs:element&gt; &lt;/xs:sequence&gt; &lt;/xs:complexType&gt; &lt;/xs:element&gt; &lt;/xs:schema&gt; \"\"\" print(scores_xsd) . &lt;xs:schema attributeFormDefault=\"unqualified\" elementFormDefault=\"qualified\" xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"&gt; &lt;xs:element name=\"scores\"&gt; &lt;xs:complexType&gt; &lt;xs:sequence&gt; &lt;xs:element name=\"score\" maxOccurs=\"unbounded\" minOccurs=\"0\"&gt; &lt;xs:complexType&gt; &lt;xs:sequence&gt; &lt;xs:element type=\"xs:date\" name=\"date\"/&gt; &lt;xs:element type=\"xs:string\" name=\"home-team\"/&gt; &lt;xs:element type=\"xs:int\" name=\"home-score\"/&gt; &lt;xs:element type=\"xs:string\" name=\"away-team\"/&gt; &lt;xs:element type=\"xs:int\" name=\"away-score\"/&gt; &lt;/xs:sequence&gt; &lt;/xs:complexType&gt; &lt;/xs:element&gt; &lt;/xs:sequence&gt; &lt;/xs:complexType&gt; &lt;/xs:element&gt; &lt;/xs:schema&gt; . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.d%20-%20XML%20Files.html#xml-files",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.d%20-%20XML%20Files.html#xml-files"
  },"80": {
    "doc": "01.d - XML Files",
    "title": "Parsing using ElementTree",
    "content": "import xml.etree.ElementTree as ET . tree = ET.parse('scores.xml') root = tree.getroot() . print(root.tag) . scores . print(root.attrib) . {'{http://www.w3.org/2001/XMLSchema-instance}noNamespaceSchemaLocation': 'scores.xsd'} . for loops . for child in root: print(child.tag, child.attrib) . score {} score {} . Typically it is helpful to know all the elements in the entire tree. A useful function for doing that is root.iter(). You can put this function into a “for” loop and it will iterate over the entire tree. for elem in root.iter(): print(elem.tag) . scores score date home-team home-score away-team away-score score date home-team home-score away-team away-score . If you need to print the entire string you can use ElementTree.tostring(). However, you must specify both the encoding and decoding of the document you are displaying as the string because ElementTree is a powerful library that can interpret more than just XML, For XMLs, use utf8 - this is the typical document format type for an XML. print(ET.tostring(root, encoding='utf8').decode('utf8')) . &lt;?xml version='1.0' encoding='utf8'?&gt; &lt;scores xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"scores.xsd\"&gt; &lt;score&gt; &lt;date&gt;2019-05-01&lt;/date&gt; &lt;home-team&gt;Pirates&lt;/home-team&gt; &lt;home-score&gt;0&lt;/home-score&gt; &lt;away-team&gt;Cubs&lt;/away-team&gt; &lt;away-score&gt;10&lt;/away-score&gt; &lt;/score&gt; &lt;score&gt; &lt;date&gt;2019-05-15&lt;/date&gt; &lt;home-team&gt;Reds&lt;/home-team&gt; &lt;home-score&gt;7&lt;/home-score&gt; &lt;away-team&gt;Pirates&lt;/away-team&gt; &lt;away-score&gt;0&lt;/away-score&gt; &lt;/score&gt; &lt;/scores&gt; . Iterating over specific elements . for game_date in root.iter('date'): print(game_date.text) . 2019-05-01 2019-05-15 . XPath Expressions . for score in root.findall('./score[date=\"2019-05-01\"]/date'): print(score.text) . 2019-05-01 . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.d%20-%20XML%20Files.html#parsing-using-elementtree",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.d%20-%20XML%20Files.html#parsing-using-elementtree"
  },"81": {
    "doc": "01.d - XML Files",
    "title": "Writing xml files",
    "content": "tree.write(\"scores2.xml\") . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.d%20-%20XML%20Files.html#writing-xml-files",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.d%20-%20XML%20Files.html#writing-xml-files"
  },"82": {
    "doc": "01.d - XML Files",
    "title": "01.d - XML Files",
    "content": " ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/01.d%20-%20XML%20Files.html",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/01.d%20-%20XML%20Files.html"
  },"83": {
    "doc": "02 - Basic Medical Data Visualization",
    "title": "Basic Medical Data Exploration Visualization  Heart Diseases",
    "content": "Source . In this lecture we’re going to learn how to use matplotlib and seaborn by following along with the following example. As always, the source author’s link is listed for reference. This page will evolve over time. ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#basic-medical-data-exploration-visualization-heart-diseases",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#basic-medical-data-exploration-visualization-heart-diseases"
  },"84": {
    "doc": "02 - Basic Medical Data Visualization",
    "title": "Dataset",
    "content": "The dataset we’ll use here is the Heart Disease Data Set containing 302 patient data each with 75 attributes. However, this example only uses 14 of them which can be seen below. The columns used include: . | age: age in years | sex: sex . | 1 = male | 0 = female | . | cp: chest pain type . | Value 1: typical angina | Value 2: atypical angina | Value 3: non-anginal pain | Value 4: asymptomatic | . | trestbps: resting blood pressure (in mm Hg on admission to the hospital) | chol: serum cholestoral in mg/dl | fbs: fasting blood sugar &gt; 120 mg/dl . | 1 = true | 0 = false | . | restecg: restecg: resting electrocardiographic results . | Value 0: normal | Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV) | Value 2: showing probable or definite left ventricular hypertrophy by Estes’ criteria | . | thalach: maximum heart rate achieved | exang: exercise induced angina . | 1 = yes | 0 = no | . | oldpeak: ST depression induced by exercise relative to rest | slope: the slope of the peak exercise ST segment . | Value 1: upsloping | Value 2: flat | Value 3: downsloping | . | ca: number of major vessels (0-3) colored by flourosopy | thal: . | 3 = normal | 6 = fixed defect | 7 = reversable defect | . | num: diagnosis of heart disease (angiographic disease status) . | Value 0: &lt; 50% diameter narrowing | Value 1: &gt; 50% diameter narrowing | . | . columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"] . # disable warnings for lecture import warnings warnings.filterwarnings('ignore') . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#dataset",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#dataset"
  },"85": {
    "doc": "02 - Basic Medical Data Visualization",
    "title": "Overview of the Data Set , Cleaning, and Viewing",
    "content": "import pandas as pd # import the data and see the basic description df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\") df.columns = columns . print(\"---- Describe ----\") print(df.describe()) . ---- Describe ---- age sex cp trestbps chol fbs \\ count 302.000000 302.000000 302.000000 302.000000 302.000000 302.000000 mean 54.410596 0.678808 3.165563 131.645695 246.738411 0.145695 std 9.040163 0.467709 0.953612 17.612202 51.856829 0.353386 min 29.000000 0.000000 1.000000 94.000000 126.000000 0.000000 25% 48.000000 0.000000 3.000000 120.000000 211.000000 0.000000 50% 55.500000 1.000000 3.000000 130.000000 241.500000 0.000000 75% 61.000000 1.000000 4.000000 140.000000 275.000000 0.000000 max 77.000000 1.000000 4.000000 200.000000 564.000000 1.000000 restecg thalach exang oldpeak slope num count 302.000000 302.000000 302.000000 302.000000 302.000000 302.000000 mean 0.986755 149.605960 0.327815 1.035430 1.596026 0.940397 std 0.994916 22.912959 0.470196 1.160723 0.611939 1.229384 min 0.000000 71.000000 0.000000 0.000000 1.000000 0.000000 25% 0.000000 133.250000 0.000000 0.000000 1.000000 0.000000 50% 0.500000 153.000000 0.000000 0.800000 2.000000 0.000000 75% 2.000000 166.000000 1.000000 1.600000 2.000000 2.000000 max 2.000000 202.000000 1.000000 6.200000 3.000000 4.000000 . print('---- Info -----') print(df.info()) . ---- Info ----- &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 302 entries, 0 to 301 Data columns (total 14 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 age 302 non-null float64 1 sex 302 non-null float64 2 cp 302 non-null float64 3 trestbps 302 non-null float64 4 chol 302 non-null float64 5 fbs 302 non-null float64 6 restecg 302 non-null float64 7 thalach 302 non-null float64 8 exang 302 non-null float64 9 oldpeak 302 non-null float64 10 slope 302 non-null float64 11 ca 302 non-null object 12 thal 302 non-null object 13 num 302 non-null int64 dtypes: float64(11), int64(1), object(2) memory usage: 33.2+ KB None . We notice above that the ca and thal data elements are objects which we’ll likely want to remap. Let’s take a look at the data. df['thal'].unique() . array(['3.0', '7.0', '6.0', '?'], dtype=object) . df['ca'].unique() . array(['3.0', '2.0', '0.0', '1.0', '?'], dtype=object) . From the codebook above we see these are coded values that we can remap. # Replace Every Number greater than 0 to 1 to mark heart disease df.loc[df['num'] &gt; 0 , 'num'] = 1 df.ca = pd.to_numeric(df.ca, errors='coerce').fillna(0) df.thal = pd.to_numeric(df.thal, errors='coerce').fillna(0) . df['thal'].unique() . array([3., 7., 6., 0.]) . df['ca'].unique() . array([3., 2., 0., 1.]) . Now we can view the datatypes of the remapped data to float64 and int64. print('---- Dtype ----') print(df.dtypes) . ---- Dtype ---- age float64 sex float64 cp float64 trestbps float64 chol float64 fbs float64 restecg float64 thalach float64 exang float64 oldpeak float64 slope float64 ca float64 thal float64 num int64 dtype: object . Next we’ll want to . print('---- Null Data ----') # count how many null values exist print(df.isnull().sum()) . ---- Null Data ---- age 0 sex 0 cp 0 trestbps 0 chol 0 fbs 0 restecg 0 thalach 0 exang 0 oldpeak 0 slope 0 ca 0 thal 0 num 0 dtype: int64 . # quickly check to see if there are any null values print(df.isnull().values.any()) . False . After doing simple clean up, changing non-numerical value to NaN and replacing NaN with 0 we can safely say our data is somewhat clean. ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#overview-of-the-data-set--cleaning-and-viewing",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#overview-of-the-data-set--cleaning-and-viewing"
  },"86": {
    "doc": "02 - Basic Medical Data Visualization",
    "title": "First / Last 10 Rows",
    "content": "# print the first 10 and last 10 print('------ First 10 -------') df.head(10) . ------ First 10 ------- . | | age | sex | cp | trestbps | chol | fbs | restecg | thalach | exang | oldpeak | slope | ca | thal | num | . | 0 | 67.0 | 1.0 | 4.0 | 160.0 | 286.0 | 0.0 | 2.0 | 108.0 | 1.0 | 1.5 | 2.0 | 3.0 | 3.0 | 1 | . | 1 | 67.0 | 1.0 | 4.0 | 120.0 | 229.0 | 0.0 | 2.0 | 129.0 | 1.0 | 2.6 | 2.0 | 2.0 | 7.0 | 1 | . | 2 | 37.0 | 1.0 | 3.0 | 130.0 | 250.0 | 0.0 | 0.0 | 187.0 | 0.0 | 3.5 | 3.0 | 0.0 | 3.0 | 0 | . | 3 | 41.0 | 0.0 | 2.0 | 130.0 | 204.0 | 0.0 | 2.0 | 172.0 | 0.0 | 1.4 | 1.0 | 0.0 | 3.0 | 0 | . | 4 | 56.0 | 1.0 | 2.0 | 120.0 | 236.0 | 0.0 | 0.0 | 178.0 | 0.0 | 0.8 | 1.0 | 0.0 | 3.0 | 0 | . | 5 | 62.0 | 0.0 | 4.0 | 140.0 | 268.0 | 0.0 | 2.0 | 160.0 | 0.0 | 3.6 | 3.0 | 2.0 | 3.0 | 1 | . | 6 | 57.0 | 0.0 | 4.0 | 120.0 | 354.0 | 0.0 | 0.0 | 163.0 | 1.0 | 0.6 | 1.0 | 0.0 | 3.0 | 0 | . | 7 | 63.0 | 1.0 | 4.0 | 130.0 | 254.0 | 0.0 | 2.0 | 147.0 | 0.0 | 1.4 | 2.0 | 1.0 | 7.0 | 1 | . | 8 | 53.0 | 1.0 | 4.0 | 140.0 | 203.0 | 1.0 | 2.0 | 155.0 | 1.0 | 3.1 | 3.0 | 0.0 | 7.0 | 1 | . | 9 | 57.0 | 1.0 | 4.0 | 140.0 | 192.0 | 0.0 | 0.0 | 148.0 | 0.0 | 0.4 | 2.0 | 0.0 | 6.0 | 0 | . # Last 10 print('------ Last 10 -------') df.tail(10) . ------ Last 10 ------- . | | age | sex | cp | trestbps | chol | fbs | restecg | thalach | exang | oldpeak | slope | ca | thal | num | . | 292 | 63.0 | 1.0 | 4.0 | 140.0 | 187.0 | 0.0 | 2.0 | 144.0 | 1.0 | 4.0 | 1.0 | 2.0 | 7.0 | 1 | . | 293 | 63.0 | 0.0 | 4.0 | 124.0 | 197.0 | 0.0 | 0.0 | 136.0 | 1.0 | 0.0 | 2.0 | 0.0 | 3.0 | 1 | . | 294 | 41.0 | 1.0 | 2.0 | 120.0 | 157.0 | 0.0 | 0.0 | 182.0 | 0.0 | 0.0 | 1.0 | 0.0 | 3.0 | 0 | . | 295 | 59.0 | 1.0 | 4.0 | 164.0 | 176.0 | 1.0 | 2.0 | 90.0 | 0.0 | 1.0 | 2.0 | 2.0 | 6.0 | 1 | . | 296 | 57.0 | 0.0 | 4.0 | 140.0 | 241.0 | 0.0 | 0.0 | 123.0 | 1.0 | 0.2 | 2.0 | 0.0 | 7.0 | 1 | . | 297 | 45.0 | 1.0 | 1.0 | 110.0 | 264.0 | 0.0 | 0.0 | 132.0 | 0.0 | 1.2 | 2.0 | 0.0 | 7.0 | 1 | . | 298 | 68.0 | 1.0 | 4.0 | 144.0 | 193.0 | 1.0 | 0.0 | 141.0 | 0.0 | 3.4 | 2.0 | 2.0 | 7.0 | 1 | . | 299 | 57.0 | 1.0 | 4.0 | 130.0 | 131.0 | 0.0 | 0.0 | 115.0 | 1.0 | 1.2 | 2.0 | 1.0 | 7.0 | 1 | . | 300 | 57.0 | 0.0 | 2.0 | 130.0 | 236.0 | 0.0 | 2.0 | 174.0 | 0.0 | 0.0 | 2.0 | 1.0 | 3.0 | 1 | . | 301 | 38.0 | 1.0 | 3.0 | 138.0 | 175.0 | 0.0 | 0.0 | 173.0 | 0.0 | 0.0 | 1.0 | 0.0 | 3.0 | 0 | . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#first--last-10-rows",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#first--last-10-rows"
  },"87": {
    "doc": "02 - Basic Medical Data Visualization",
    "title": "Plotting Histograms",
    "content": "After reviewing the data in tabular form we want to visualize all of the data across the variables. We can do this easily with a histogram. # import matplotlib import matplotlib.pyplot as plt %matplotlib inline . # using pandas to generate the plots df.hist() # using matplotlib to render (or show) the plot plt.show() . # get the histogram of every data points fig = plt.figure(figsize = (18, 18)) ax = fig.gca() df.hist(ax=ax, bins=30) plt.show() . With simple histogram of our data, we can easily observe the distribution of different attributes. One thing to note here is the fact that it is extremely easy for us to see which attributes are categorical values and which are not. We can inspect a little bit more closely and take a look at the distribution of ages and fbs (fasting blood sugar). We can see that the age distribution is closely resembling of Gaussian distribution while fbs is a categorical value. # import seaborn import seaborn as sns # a closer look at age plt.figure(figsize=(8, 8)) sns.distplot(df.age) plt.show() plt.close('all') . # a closer look at fbs plt.figure(figsize=(8, 8)) sns.distplot(df.fbs) plt.show() . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#plotting-histograms",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#plotting-histograms"
  },"88": {
    "doc": "02 - Basic Medical Data Visualization",
    "title": "Variance-Covariance Matrix",
    "content": "We can calculate variance-covariance matrices in a number of ways. First we’ll use Numpy and then we’ll use the built-in Dataframe functrion. Once calculated, we can observe that most attributes do not have a strong covariance relationship. import numpy as np from numpy import dot # calculate the Variance-Covariance Matrix sample = df.values sample = sample - dot(np.ones((sample.shape[0],sample.shape[0])),sample)/(len(sample)-1) covv = dot(sample.T,sample)/(len(sample)-1) plt.figure(figsize=(8,8)) sns.heatmap(covv) plt.show() . # compare with built in plt.figure(figsize=(8,8)) sns.heatmap(df.cov()) plt.show() . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#variance-covariance-matrix",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#variance-covariance-matrix"
  },"89": {
    "doc": "02 - Basic Medical Data Visualization",
    "title": "Correlation matrix",
    "content": "Similarly, the first image is created by manual numpy calculation and the second using the bulit-in method. Ee can observe that among the attributes there are actually strong correlation with one another. (especially heart disease and thal). # calculate correaltion matrix sample = df.values certering_mat = np.diag(np.ones((302))) - np.ones((302,302))/302 std_matrix = np.diag(np.std(sample,0)) temp = dot(certering_mat,dot(sample, np.linalg.inv(std_matrix) )) temp = dot(temp.T,temp)/len(sample) # plot plt.figure(figsize=(13, 13)) sns.heatmap(np.around(temp,2),annot=True,fmt=\".2f\",cmap=\"Blues\",annot_kws={\"size\": 15}) plt.show() . # correaltion matrix sns.set(font_scale=2) plt.figure(figsize=(13,13)) sns.heatmap(df.corr().round(2),annot=True,fmt=\".2f\",cmap=\"Blues\",annot_kws={\"size\": 15}) plt.show() . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#correlation-matrix",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#correlation-matrix"
  },"90": {
    "doc": "02 - Basic Medical Data Visualization",
    "title": "Interactive Histogram",
    "content": "# plot the people who have heart vs not plt.figure(figsize=(13, 13)) sns.distplot(df.age[df.num==0], label='No Disease', color='blue') sns.distplot(df.age[df.num==1], label='Disease', color='Red') sns.distplot(df.trestbps[df.num==0],label= 'No Disease', color='Green') sns.distplot(df.trestbps[df.num==1], label='Disease', color='violet') plt.legend() plt.show() . %matplotlib inline import pygal from IPython.display import SVG, HTML html_pygal = \"\"\" &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;script type=\"text/javascript\" src=\"http://kozea.github.com/pygal.js/javascripts/svg.jquery.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\" src=\"http://kozea.github.com/pygal.js/javascripts/pygal-tooltips.js\"&gt;&lt;/script&gt; &lt;!-- ... --&gt; &lt;/head&gt; &lt;body&gt; &lt;figure&gt; {pygal_render} &lt;/figure&gt; &lt;/body&gt; &lt;/html&gt; \"\"\" hist = pygal.Histogram() count, division = np.histogram(df.age[df.num==0].values,bins=100) temp = [] for c,div in zip(count,division): temp.append((c,div,div+1)) count, division = np.histogram(df.age[df.num==1].values,bins=100) temp1 = [] for c,div in zip(count,division): temp1.append((c,div,div+1)) count, division = np.histogram(df.trestbps[df.num==0].values,bins=100) temp2 = [] for c,div in zip(count,division): temp2.append((c,div,div+1)) count, division = np.histogram(df.trestbps[df.num==1].values,bins=100) temp3 = [] for c,div in zip(count,division): temp3.append((c,div,div+1)) hist.add('No Disease age', temp) hist.add('Disease age', temp1) hist.add('No Disease ', temp2) hist.add('Disease', temp3) hist.render() HTML(html_pygal.format(pygal_render=hist.render())) . &lt;!DOCTYPE html&gt; . b'\\n . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#interactive-histogram",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html#interactive-histogram"
  },"91": {
    "doc": "02 - Basic Medical Data Visualization",
    "title": "02 - Basic Medical Data Visualization",
    "content": " ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/02%20-%20Basic%20Medical%20Data%20Visualization.html"
  },"92": {
    "doc": "02 - Lists and List Comprehensions",
    "title": "Python List Comprehensions",
    "content": "Python List Comprehension Tutorial . When doing data science, you might find yourself wanting to read lists of lists, filtering column names, removing vowels from a list or flattening a matrix. You can easily use a lambda function or a for loop; As you well know, there are multiple ways to go about this. One other way to do this is by using list comprehensions. ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html#python-list-comprehensions",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html#python-list-comprehensions"
  },"93": {
    "doc": "02 - Lists and List Comprehensions",
    "title": "Python List Comprehension",
    "content": "List comprehensions are used to create lists programatically. List comprehensions in Python are constructed as follows: . list_variable = [x for x in iterable] . List Comprehension in Python: The Mathematics . Remember in maths, the common ways to describe lists (or sets, or tuples, or vectors) are: . S = {x² : x in {0 ... 9}} V = (1, 2, 4, 8, ..., 2¹²) M = {x | x in S and x even} . In other words, you’ll find that the above definitions actually tell you the following: . | The sequence S is actually a sequence that contains values between 0 and 9 included that are raised to the power of two. | The sequence V, on the other hand, contains the value 2 that is raised to a certain power. For the first element in the sequence, this is 0, for the second this is 1, and so on, until you reach 12. | Lastly, the sequence M contains elements from the sequence S, but only the even ones. | . If the above definitions give you a headache, take a look at the actual lists that these definitions would produce: . S = {0, 1, 4, 9, 16, 25, 36, 49, 64, 81} V = {1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096} M = {0, 4, 16, 36, 64} . # S = {x² : x in {0 ... 9}} S = [] for x in range(10): S.append(x**2) print(S) . [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] . # S = {x² : x in {0 ... 9}} S = [x**2 for x in range(10)] print(S) . [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] . The list S is built up with the square brackets that you read above in the first section. In those brackets, you see that there is an element x, which is raised to the power of 10. Now, you just need to know for how many values (and which values!) you need to raise to the power of 2. This is determined in range(10). Considering all of this, you can derive that you’ll raise all numbers, going from 0 to 9, to the power of 2. # V = (1, 2, 4, 8, ..., 2¹²) V = [] for i in range(13): V.append(2**i) print(V) . [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096] . # V = (1, 2, 4, 8, ..., 2¹²) V = [2**i for i in range(13)] print(V) . [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096] . The list V contains the base value 2, which is raised to a certain power. Just like before, now you need to know which power or i is exactly going to be used to do this. You see that i in this case is part of range(13), which means that you start from 0 and go until 12. All of this means that your list is going to have 13 values - those values will be 2 raised to the power 0, 1, 2, … all the way up to 12. # M = {x | x in S and x even} M = [] for x in S: if x % 2 == 0: M.append(x) print(M) . [0, 4, 16, 36, 64] . # M = {x | x in S and x even} M = [x for x in S if x % 2 == 0] print(M) . [0, 4, 16, 36, 64] . Lastly, the list M contains elements that are part of S if -and only if- they can be divided by 2 without having any leftovers. The modulo needs to be 0. In other words, the list M is built up with the equal values that are stored in list S. ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html#python-list-comprehension",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html#python-list-comprehension"
  },"94": {
    "doc": "02 - Lists and List Comprehensions",
    "title": "List Comprehension as an Alternative to…",
    "content": "List comprehension is a complete substitute to for loops, lambda function as well as the functions map(), filter() and reduce(). For Loops . List comprehensions are actually good alternatives to for loops, as they are more compact. For Loop Example . numbers = range(0, 10) # Initialize `new_list` new_list = [] # Add values to `new_list` for n in numbers: if n % 2 == 0: new_list.append(n**2) # Print `new_list` print(new_list) . [0, 4, 16, 36, 64] . List Comprehension Alternative to For Loop . numbers = range(0, 10) # Create `new_list` new_list = [n**2 for n in numbers if n%2==0] # Print `new_list` print(new_list) . [0, 4, 16, 36, 64] . Which is faster? . Let’s study the difference in performance between the list comprehension and the for loop with a small test: you can set this up very quickly with the timeit library, which you can use to time small bits of Python code in a simple way. In this case, the small pieces of code that you will test are the for loop, which you will put in a function called power_two() for your convenience, and the exact list comprehension which you have formulated above. # Import `timeit` import timeit . # Print the execution time print(timeit.timeit('[n**2 for n in range(10) if n%2==0]', number=10000)) . 0.04181958400000063 . # Define `power_two()` def power_two(numbers): for n in numbers: if n%2==0: new_list.append(n**2) return new_list . print(timeit.timeit('power_two(numbers)', globals=globals(), number=10000)) . 0.044310624999999604 . Lambda Functions with map(), filter() and reduce() . Lambda functions are also called “anonymous functions” or “functions without name”. That means that you only use this type of functions when they are created. Lambda functions borrow their name from the lambda keyword in Python, which is used to declare these functions instead of the standard def keyword. You usually use these functions together with the map(), filter(), and reduce() functions. Replace map() and Lambda Functions with List Comprehensions . Map Example . # Initialize the `kilometer` list kilometer = [39.2, 36.5, 37.3, 37.8] # Construct `feet` with `map()` feet = map(lambda x: 3280.8399*x, kilometer) # Print `feet` as a list print(type(feet), list(feet)) . &lt;class 'map'&gt; [128608.92408000001, 119750.65635, 122375.32826999998, 124015.74822] . List Comprehension Alternative to Map . # Convert `kilometer` to `feet` feet = [] for x in kilometer: feet.append(3280.8399*x) print(feet) . [128608.92408000001, 119750.65635, 122375.32826999998, 124015.74822] . # Convert `kilometer` to `feet` feet = [3280.8399*x for x in kilometer] # Print `feet` print(feet) . [128608.92408000001, 119750.65635, 122375.32826999998, 124015.74822] . Replace filter() and Lambda Functions with List Comprehensions . Filter Example . # Filter `feet` to only include uneven distances uneven = filter(lambda x: x % 2, feet) # Check the type of `uneven` type(uneven) # Print `uneven` as a list print(list(uneven)) . [128608.92408000001, 119750.65635, 122375.32826999998, 124015.74822] . List Comprehension Alternative to Filter . # Constructing `feet` feet = [int(x) for x in feet] # Print `feet` print(feet) # Get all uneven distances uneven = [True if x%2 else False for x in feet] # Print `uneven` print(uneven) . [128608, 119750, 122375, 124015] [False, False, True, True] . Replace reduce() and Lambda Functions with List Comprehensions . Reduce Example . # Import `reduce` from `functools` from functools import reduce # Reduce `feet` to `reduced_feet` reduced_feet = reduce(lambda x,y: x+y, feet) # Print `reduced_feet` print(reduced_feet) . 494748 . List Comprehension Alternative to Reduce . # Construct `reduced_feet` reduced_feet = sum([float(3280.8399)*x for x in kilometer]) # Print `reduced_feet` print(reduced_feet) . 494750.65692000004 . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html#list-comprehension-as-an-alternative-to",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html#list-comprehension-as-an-alternative-to"
  },"95": {
    "doc": "02 - Lists and List Comprehensions",
    "title": "List Comprehensions with Conditionals",
    "content": "# Define `uneven` uneven = [int(x) for x in feet if x%2] # Print `uneven` print(uneven) . [122375, 124015] . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html#list-comprehensions-with-conditionals",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html#list-comprehensions-with-conditionals"
  },"96": {
    "doc": "02 - Lists and List Comprehensions",
    "title": "Multiple If Conditions",
    "content": "Example . divided = [] for x in range(100): if x%2 == 0 : if x%6 == 0: divided.append(x) print(divided) . [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96] . List Comprehension Alternative . divided = [x for x in range(100) if x % 2 == 0 if x % 6 == 0] print(divided) . [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96] . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html#multiple-if-conditions",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html#multiple-if-conditions"
  },"97": {
    "doc": "02 - Lists and List Comprehensions",
    "title": "if…else Conditions",
    "content": "Example . values = [] for x in feet: if x &gt;= 120000: x + 1 else: x + 5 values.append(x) print(values) . [128608, 119750, 122375, 124015] . List Comprehension Alternative . values = [x+1 if x &gt;= 120000 else x+5 for x in feet] print(values) . [128609, 119755, 122376, 124016] . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html#ifelse-conditions",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html#ifelse-conditions"
  },"98": {
    "doc": "02 - Lists and List Comprehensions",
    "title": "Nested List Comprehensions",
    "content": "Apart from conditionals, you can also adjust your list comprehensions by nesting them within other list comprehensions. This is handy when you want to work with lists of lists: generating lists of lists, transposing lists of lists or flattening lists of lists to regular lists, for example, becomes extremely easy with nested list comprehensions. You see that most of the keywords and elements that are used in the example of the nested list comprehension are similar to the ones that you used in the simple list comprehension examples: . | Square brackets | Two for keywords, followed by a variable that symbolizes an item of the list of lists (x) and a list item of a nested list (y); And | Two in keywords, followed by a list of lists (list_of_list) and a list item (x). | . list_of_list = [[1,2,3],[4,5,6],[7,8]] print(list_of_list) print(list_of_list[0]) print(list_of_list[1]) print(list_of_list[1][1]) print(list_of_list[2]) . [[1, 2, 3], [4, 5, 6], [7, 8]] [1, 2, 3] [4, 5, 6] 5 [7, 8] . # Flatten `list_of_list` flat = [] for x in list_of_list: for y in x: flat.append(y) print(flat) . [1, 2, 3, 4, 5, 6, 7, 8] . # Flatten `list_of_list` flat = [y for x in list_of_list for y in x] print(flat) . [1, 2, 3, 4, 5, 6, 7, 8] . Let’s now consider another example, where you see that you can also use two pairs of square brackets to change the logic of your nested list comprehension: . matrix = [[1,2,3], [4,5,6],[7,8,9]] [[row[i] for row in matrix] for i in range(len(matrix))] . [[1, 4, 7], [2, 5, 8], [3, 6, 9]] . Now practice: rewrite the code chunk above to a nested for loop. If you need some pointers on how to tackle this exercise, go to one of the previous sections of this tutorial. transposed = [] for i in range(len(matrix)): transposed_row = [] for row in matrix: transposed_row.append(row[i]) transposed.append(transposed_row) print(transposed) . [[1, 4, 7], [2, 5, 8], [3, 6, 9]] . You can also use nested list comprehensions when you need to create a list of lists that is actually a matrix. Check out the following example: . matrix = [] for x in range(3): nested = [] matrix.append(nested) for row in range(4): nested.append(0) print(matrix) . [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]] . # rewritten as a list comprehension matrix = [[0 for col in range(4)] for row in range(3)] matrix . [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]] . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html#nested-list-comprehensions",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html#nested-list-comprehensions"
  },"99": {
    "doc": "02 - Lists and List Comprehensions",
    "title": "02 - Lists and List Comprehensions",
    "content": " ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/02%20-%20Lists%20and%20List%20Comprehensions.html"
  },"100": {
    "doc": "02 - Python Tuples",
    "title": "Python Tuple",
    "content": "Python Tuple . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/02%20-%20Python%20Tuples.html#python-tuple",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/02%20-%20Python%20Tuples.html#python-tuple"
  },"101": {
    "doc": "02 - Python Tuples",
    "title": "What is tuple?",
    "content": "In Python programming, a tuple is similar to a list. The difference between the two is that we cannot change the elements of a tuple once it is assigned whereas in a list, elements can be changed. Advantages of Tuple over List . Since, tuples are quite similiar to lists, both of them are used in similar situations as well. However, there are certain advantages of implementing a tuple over a list. Below listed are some of the main advantages: . | We generally use tuple for heterogeneous (different) datatypes and list for homogeneous (similar) datatypes. | Since tuple are immutable, iterating through tuple is faster than with list. So there is a slight performance boost. | Tuples that contain immutable elements can be used as key for a dictionary. With list, this is not possible. | If you have data that doesn’t change, implementing it as tuple will guarantee that it remains write-protected. | . Creating a Tuple . A tuple is created by placing all the items (elements) inside a parentheses (), separated by comma. The parentheses are optional but is a good practice to write it. A tuple can have any number of items and they may be of different types (integer, float, list, string etc.). # empty tuple # Output: () my_tuple = () print(type(my_tuple), my_tuple) . &lt;class 'tuple'&gt; () . # tuple having integers # Output: (1, 2, 3) my_tuple = (1, 2, 3) print(my_tuple) . (1, 2, 3) . # tuple with mixed datatypes # Output: (1, \"Hello\", 3.4) my_tuple = (1, \"Hello\", 3.4) print(my_tuple) . (1, 'Hello', 3.4) . # nested tuple # Output: (\"mouse\", [8, 4, 6], (1, 2, 3)) my_tuple = (\"mouse\", [8, 4, 6], (1, 2, 3)) print(my_tuple) . ('mouse', [8, 4, 6], (1, 2, 3)) . # tuple can be created without parentheses # also called tuple packing # Output: 3, 4.6, \"dog\" my_tuple = 3, 4.6, \"dog\" print(my_tuple) . (3, 4.6, 'dog') . Creating a tuple with one element is a bit tricky. Having one element within parentheses is not enough. We will need a trailing comma to indicate that it is in fact a tuple. # only parentheses is not enough # Output: &lt;class 'str'&gt; my_tuple = (\"hello\") print(type(my_tuple)) . &lt;class 'str'&gt; . # need a comma at the end # Output: &lt;class 'tuple'&gt; my_tuple = (\"hello\") print(type(my_tuple), my_tuple) . &lt;class 'str'&gt; hello . # what if i forget a comma # Output: &lt;class 'tuple'&gt; my_tuple = (\"hello\" \"bye\") print(type(my_tuple), my_tuple) . &lt;class 'str'&gt; hellobye . # parentheses is optional # Output: &lt;class 'tuple'&gt; my_tuple = \"hello\", print(type(my_tuple)) . &lt;class 'tuple'&gt; . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/02%20-%20Python%20Tuples.html#what-is-tuple",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/02%20-%20Python%20Tuples.html#what-is-tuple"
  },"102": {
    "doc": "02 - Python Tuples",
    "title": "Accessing Elements in a Tuple",
    "content": "Indexing . n_tuple = (\"mouse\", [8, 4, 6], (1, 2, 3)) print(n_tuple[1][2]) . 6 . n_tuple = (\"mouse\", [8, 4, 6], (1, 2, 3)) print(type(n_tuple[0][2]), n_tuple[0][2]) . &lt;class 'str'&gt; u . Negative indexing . n_tuple = (\"mouse\", [8, 4, 6], (1, 2, 3)) print(n_tuple[-1][-3], ' is the same as ', n_tuple[2][0]) . 1 is the same as 1 . n_tuple = (\"mouse\", [8, 4, 6], (1, 2, 3)) print(n_tuple[0][0], ' is the same as ', n_tuple[-3][-5]) . m is the same as m . n_tuple = (\"mouse\", [8, 4, 6], (1, 2, 3)) print(n_tuple[-1][1]) . 2 . Slicing . letter_tuple = ('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j') . # prints items at position 0, 1, and 2 but not 3 print(letter_tuple[0:3]) . ('a', 'b', 'c') . # prints items at position 3, 4, and 5 but not 6 print(letter_tuple[3:6]) . ('d', 'e', 'f') . # start at the beginning print(letter_tuple[:6]) . ('a', 'b', 'c', 'd', 'e', 'f') . # go until the end print(letter_tuple[3:]) . ('d', 'e', 'f', 'g', 'h', 'i', 'j') . stepping over a tuple (or list) . # every element print(letter_tuple[0:10:1]) . ('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j') . # every 3rd element print(letter_tuple[0:10:3]) . ('a', 'd', 'g', 'j') . # elements at even positions print(letter_tuple[0::2]) . ('a', 'c', 'e', 'g', 'i') . # elements at odd positions print(letter_tuple[1::2]) . ('b', 'd', 'f', 'h', 'j') . reversing a tuple (or list) . print(letter_tuple[len(letter_tuple)::-1]) . ('j', 'i', 'h', 'g', 'f', 'e', 'd', 'c', 'b', 'a') . print(letter_tuple[len(letter_tuple)::-2]) . ('j', 'h', 'f', 'd', 'b') . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/02%20-%20Python%20Tuples.html#accessing-elements-in-a-tuple",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/02%20-%20Python%20Tuples.html#accessing-elements-in-a-tuple"
  },"103": {
    "doc": "02 - Python Tuples",
    "title": "Changing a Tuple",
    "content": "Unlike lists, tuples are immutable. This means that elements of a tuple cannot be changed once it has been assigned. x_tuple = (4, 2, 3, [6, 5]) x_tuple[1] = 8 # throws an error . --------------------------------------------------------------------------- TypeError Traceback (most recent call last) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_91254/2014519070.py in &lt;module&gt; 1 x_tuple = (4, 2, 3, [6, 5]) 2 ----&gt; 3 x_tuple[1] = 8 # throws an error TypeError: 'tuple' object does not support item assignment . But, if the element is itself a mutable datatype like list, its nested items can be changed. x_tuple[3][0] = 9 # but item of mutable element can be changed print(x_tuple) . (4, 2, 3, [9, 5]) . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/02%20-%20Python%20Tuples.html#changing-a-tuple",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/02%20-%20Python%20Tuples.html#changing-a-tuple"
  },"104": {
    "doc": "02 - Python Tuples",
    "title": "Deleting a Tuple",
    "content": "We cannot change the elements in a tuple. That also means we cannot delete or remove items from a tuple. But deleting a tuple entirely is possible using the keyword del. a_tuple = (1, 2, 3) print(a_tuple) . (1, 2, 3) . del a_tuple print(a_tuple) . --------------------------------------------------------------------------- NameError Traceback (most recent call last) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_91254/1119046425.py in &lt;module&gt; 1 del a_tuple ----&gt; 2 print(a_tuple) NameError: name 'a_tuple' is not defined . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/02%20-%20Python%20Tuples.html#deleting-a-tuple",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/02%20-%20Python%20Tuples.html#deleting-a-tuple"
  },"105": {
    "doc": "02 - Python Tuples",
    "title": "Other Tuple Methods",
    "content": ". | count(x) - Return the number of items that is equal to x | index(x) - Return index of first item that is equal to x | . my_tuple = ('r', 'e', 'd', '', 'a', 'p', 'p', 'l', 'e',) . print(my_tuple.count('p')) . 2 . print(my_tuple.index('p')) . 5 . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/02%20-%20Python%20Tuples.html#other-tuple-methods",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/02%20-%20Python%20Tuples.html#other-tuple-methods"
  },"106": {
    "doc": "02 - Python Tuples",
    "title": "02 - Python Tuples",
    "content": " ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/02%20-%20Python%20Tuples.html",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/02%20-%20Python%20Tuples.html"
  },"107": {
    "doc": "02 - Running Python",
    "title": "Running Python",
    "content": " ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/02%20-%20Running%20Python.html#running-python",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/02%20-%20Running%20Python.html#running-python"
  },"108": {
    "doc": "02 - Running Python",
    "title": "Running the course Notebooks",
    "content": "Once the course git repository has been cloned locally you can run the course notebooks and follow along in class or try them at home. First, change the directory to the data-focused-python directory of your clonned repository,. cd data-focused-python . Then start Jupyter from your terminal . jupyter-lab . What is Jupyter? . The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. If you’ve installed Anaconda, Jupyter is packaged as part of the distribution. I encourage you to review the more detailed How To Use Jupyter Notebook – An Ultimate Guide. ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/02%20-%20Running%20Python.html#running-the-course-notebooks",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/02%20-%20Running%20Python.html#running-the-course-notebooks"
  },"109": {
    "doc": "02 - Running Python",
    "title": "Running the Python Interpreter",
    "content": "Python itself is an interpreted programming language which means the source code you type gets executed by the interpreter at runtime. You can start the python interpreter from any command line by executing the following in your terminal: . python . I encourage you to try it out. You can follow along the official python docs here . ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/02%20-%20Running%20Python.html#running-the-python-interpreter",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/02%20-%20Running%20Python.html#running-the-python-interpreter"
  },"110": {
    "doc": "02 - Running Python",
    "title": "Running the IPython Interpreter",
    "content": "IPython builds on the Python interpreter adding advanced read-eval-print-loop (REPL) functionality among other features. You can start the ipython interpreter by executing the following command in your terminal: . ipython . # in anaconda ## find the location of your virtual environment !which python . /Users/bk/opt/miniconda3/envs/cmu39/bin/python . ## run IDLE out of that location !/Users/bk/opt/miniconda3/envs/cmu39/bin/idle3.9 &amp; . ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/02%20-%20Running%20Python.html#running-the-ipython-interpreter",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/02%20-%20Running%20Python.html#running-the-ipython-interpreter"
  },"111": {
    "doc": "02 - Running Python",
    "title": "Executing Python Files",
    "content": "Using the Python interpreter is great for small problems, testing libraries, etc. but all of your code lives in memory inside the python interpreter. When the interpreter stops your code is gone forever. In order to get around this limitation you can create python script files and execute the files using the interpreter. Create a file on your harddrive called hello.py. Copy and paste this code into the file and save it. print('Hello class') print('Welcome to 95-888') . You can then execute the file by typing this command into your terminal. python hello.py . !python hello.py . ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/02%20-%20Running%20Python.html#executing-python-files",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/02%20-%20Running%20Python.html#executing-python-files"
  },"112": {
    "doc": "02 - Running Python",
    "title": "02 - Running Python",
    "content": " ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/02%20-%20Running%20Python.html",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/02%20-%20Running%20Python.html"
  },"113": {
    "doc": "02 - Using Faker",
    "title": "Using Faker",
    "content": "Source . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#using-faker",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#using-faker"
  },"114": {
    "doc": "02 - Using Faker",
    "title": "Basic Usage",
    "content": "# import the library from faker import Faker . # create an instance faker = Faker() . # generate a name faker.name() . 'Vincent Allen' . # generate another name faker.name() . 'Dawn Harvey' . # generate an address faker.address() . '11351 Elizabeth Canyon\\nLake Sarah, NE 29707' . # generate another address faker.address() . '1006 Jessica Mills\\nBrewerview, MO 39072' . # generate some random text faker.text() . 'Help politics watch space study beautiful. If focus firm again.\\nPay up subject. Will particularly have be article clear season. Participant huge name notice.' . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#basic-usage",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#basic-usage"
  },"115": {
    "doc": "02 - Using Faker",
    "title": "Name Related Data",
    "content": "faker.name() . 'Amy Roberson' . faker.first_name() . 'Antonio' . faker.last_name() . 'Odom' . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#name-related-data",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#name-related-data"
  },"116": {
    "doc": "02 - Using Faker",
    "title": "Faking Jobs",
    "content": "faker.job() . 'Microbiologist' . faker.job() . 'Health promotion specialist' . for _ in range(5): print(faker.job()) . Surveyor, insurance Engineer, chemical Cytogeneticist Drilling engineer Production assistant, radio . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-jobs",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-jobs"
  },"117": {
    "doc": "02 - Using Faker",
    "title": "Faking Locale Data",
    "content": "faker = Faker('cz_CZ') for i in range(3): name = faker.name() address = faker.address() phone = faker.phone_number() print(f'{name}, {address}, {phone}') . Miroslava Kratochvílová, Komenského Nám. 91 595 83 Kojetín, 601 108 913 Kryštof Bartoš, Lednická 347 649 34 Horní Jelení, 723 227 473 Bohuslav Urban, Na Staré Vinici 50 609 26 Hostomice, 603 178 388 . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-locale-data",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-locale-data"
  },"118": {
    "doc": "02 - Using Faker",
    "title": "Faking Currencies",
    "content": "faker = Faker() . faker.currency() . ('GNF', 'Guinean franc') . faker.currency_name() . 'Jersey pound' . faker.currency_code() . 'AWG' . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-currencies",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-currencies"
  },"119": {
    "doc": "02 - Using Faker",
    "title": "Faking words",
    "content": "faker.word() . 'care' . faker.words(10) . ['its', 'pattern', 'concern', 'road', 'couple', 'common', 'value', 'our', 'either', 'world'] . my_words = ['forrest', 'blue', 'cloud', 'sky', 'wood', 'falcon'] faker.words(3, my_words, True) . ['blue', 'cloud', 'wood'] . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-words",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-words"
  },"120": {
    "doc": "02 - Using Faker",
    "title": "Faking profiles",
    "content": "profile1 = faker.simple_profile() print(profile1) . {'username': 'churchjames', 'name': 'Victoria Ryan MD', 'sex': 'F', 'address': '90570 Hannah Stream\\nSouth Shelly, TN 77015', 'mail': 'kelli28@yahoo.com', 'birthdate': datetime.date(1949, 7, 30)} . import dumper dumper.dump(profile1) . &lt;dict at 0x7ffcc004d380&gt;: username: 'churchjames' name: 'Victoria Ryan MD' sex: 'F' address: '90570 Hannah Stream\\nSouth Shelly, TN 77015' mail: 'kelli28@yahoo.com' birthdate: &lt;str at 0x7ffcc00503a0&gt;: 'datetime.date(1949, 7, 30)' . profile2 = faker.simple_profile('F') dumper.dump(profile2) . &lt;dict at 0x7ffcc0047200&gt;: username: 'qjohnston' name: 'Jessica Gillespie' sex: 'F' address: '69654 Megan Rest\\nPatriciafurt, MD 35261' mail: 'jessicasmith@hotmail.com' birthdate: &lt;str at 0x7ffcc00509e0&gt;: 'datetime.date(1949, 4, 3)' . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-profiles",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-profiles"
  },"121": {
    "doc": "02 - Using Faker",
    "title": "Faking Numbers",
    "content": "faker.random_int() . 1717 . faker.random_int(18, 64) . 31 . faker.random_digit() . 1 . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-numbers",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-numbers"
  },"122": {
    "doc": "02 - Using Faker",
    "title": "Faking hashes and uids",
    "content": "faker.md5() . '1e7bfd5eb691a36a94af457b383f375e' . faker.sha1() . '0c83ab884516f9ef6f877a4d077a16630354b001' . faker.sha256() . 'b239734a398a4c8005beefefd98a772c896dc5966f6233d74a398988d98714d3' . faker.uuid4() . 'b914121e-63e4-4610-a1df-2555719ba7b8' . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-hashes-and-uids",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-hashes-and-uids"
  },"123": {
    "doc": "02 - Using Faker",
    "title": "Faking internet related data",
    "content": "faker.email() . 'kellylester@example.org' . faker.safe_email() . 'powellbenjamin@example.com' . faker.free_email() . 'mphillips@gmail.com' . faker.company_email() . 'seancarroll@murray.info' . faker.hostname() . 'srv-25.williamson-johnson.com' . faker.domain_name() . 'wilson.org' . faker.domain_word() . 'clark' . faker.tld() . 'org' . faker.ipv4() . '31.17.120.149' . faker.ipv6() . 'e4d1:cfad:4a05:b049:cb63:5889:8094:c571' . faker.slug() . 'occur-bed-work-yet' . faker.image_url() . 'https://placekitten.com/727/516' . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-internet-related-data",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-internet-related-data"
  },"124": {
    "doc": "02 - Using Faker",
    "title": "Faking date and time",
    "content": "faker.date_of_birth() . faker.century() . 'XV' . faker.year() . '1991' . faker.month() . '09' . faker.month_name() . 'November' . faker.day_of_week() . 'Sunday' . faker.day_of_month() . '01' . faker.timezone() . 'America/Paramaribo' . faker.am_pm() . 'AM' . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-date-and-time",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#faking-date-and-time"
  },"125": {
    "doc": "02 - Using Faker",
    "title": "Specific Date Time",
    "content": "faker.date_time_this_century() . datetime.datetime(2018, 2, 9, 20, 51, 37) . faker.date_time_this_decade() . datetime.datetime(2020, 1, 12, 23, 48, 1) . faker.date_time_this_year() . datetime.datetime(2021, 6, 11, 20, 16, 29) . faker.date_time_this_month() . datetime.datetime(2021, 10, 6, 18, 59, 38) . faker.date_this_century() . datetime.date(2013, 10, 11) . faker.date_this_decade() . datetime.date(2021, 4, 1) . faker.date_this_year() . datetime.date(2021, 8, 28) . faker.date_this_month() . datetime.date(2021, 10, 20) . TOTAL_SECONDS = 60*60*24*2 # two days series = faker.time_series(start_date='-12d', end_date='now', precision=TOTAL_SECONDS) for val in series: print(val[0]) . 2021-10-16 15:13:18 2021-10-18 15:13:18 2021-10-20 15:13:18 2021-10-22 15:13:18 2021-10-24 15:13:18 2021-10-26 15:13:18 . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#specific-date-time",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#specific-date-time"
  },"126": {
    "doc": "02 - Using Faker",
    "title": "More Date Time",
    "content": "faker.unix_time() . 1251085387 . faker.date_time() . datetime.datetime(1976, 7, 17, 20, 43, 28) . faker.iso8601() . '1982-11-03T07:46:42' . faker.date() . '1990-08-27' . faker.time() . '12:55:01' . print(f\"Datetime between: {faker.date_time_between(start_date='-15y', end_date='now')}\") print(f\"Date between: {faker.date_between()}\") . Datetime between: 2007-07-11 14:37:20 Date between: 1993-01-12 . faker.future_datetime() . datetime.datetime(2021, 11, 9, 6, 55, 23) . faker.future_date() . datetime.date(2021, 11, 18) . faker.past_datetime() . datetime.datetime(2021, 9, 29, 8, 18, 48) . faker.past_date() . datetime.date(2021, 10, 23) . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#more-date-time",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#more-date-time"
  },"127": {
    "doc": "02 - Using Faker",
    "title": "Generating XML Data",
    "content": "from jinja2 import Environment, FileSystemLoader . class User: def __init__(self, first_name, last_name, occupation): self.first_name = first_name self.last_name = last_name self.occupation = occupation . faker = Faker() . users = [] . for _ in range(10): first_name = faker.first_name() last_name = faker.last_name() occupation = faker.job() user = User(first_name, last_name, occupation) users.append(user) . file_loader = FileSystemLoader('templates') env = Environment(loader=file_loader) template = env.get_template('users.xml.j2') output = template.render(users=users) . print(output) . &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;users&gt; &lt;user id=\"1\"&gt; &lt;firstname&gt;Cheryl&lt;/firstname&gt; &lt;lastname&gt;Howard&lt;/lastname&gt; &lt;occupation&gt;Community arts worker&lt;/occupation&gt; &lt;/user&gt; &lt;user id=\"2\"&gt; &lt;firstname&gt;Jessica&lt;/firstname&gt; &lt;lastname&gt;Miller&lt;/lastname&gt; &lt;occupation&gt;Magazine features editor&lt;/occupation&gt; &lt;/user&gt; &lt;user id=\"3\"&gt; &lt;firstname&gt;Alexandra&lt;/firstname&gt; &lt;lastname&gt;Hill&lt;/lastname&gt; &lt;occupation&gt;Ceramics designer&lt;/occupation&gt; &lt;/user&gt; &lt;user id=\"4\"&gt; &lt;firstname&gt;Lindsay&lt;/firstname&gt; &lt;lastname&gt;Holland&lt;/lastname&gt; &lt;occupation&gt;Economist&lt;/occupation&gt; &lt;/user&gt; &lt;user id=\"5\"&gt; &lt;firstname&gt;Kevin&lt;/firstname&gt; &lt;lastname&gt;Michael&lt;/lastname&gt; &lt;occupation&gt;Nurse, children's&lt;/occupation&gt; &lt;/user&gt; &lt;user id=\"6\"&gt; &lt;firstname&gt;Amanda&lt;/firstname&gt; &lt;lastname&gt;Bush&lt;/lastname&gt; &lt;occupation&gt;IT trainer&lt;/occupation&gt; &lt;/user&gt; &lt;user id=\"7\"&gt; &lt;firstname&gt;Candice&lt;/firstname&gt; &lt;lastname&gt;Oliver&lt;/lastname&gt; &lt;occupation&gt;Quantity surveyor&lt;/occupation&gt; &lt;/user&gt; &lt;user id=\"8\"&gt; &lt;firstname&gt;Brian&lt;/firstname&gt; &lt;lastname&gt;Powell&lt;/lastname&gt; &lt;occupation&gt;Advice worker&lt;/occupation&gt; &lt;/user&gt; &lt;user id=\"9\"&gt; &lt;firstname&gt;Sarah&lt;/firstname&gt; &lt;lastname&gt;Graham&lt;/lastname&gt; &lt;occupation&gt;Heritage manager&lt;/occupation&gt; &lt;/user&gt; &lt;user id=\"10\"&gt; &lt;firstname&gt;Frank&lt;/firstname&gt; &lt;lastname&gt;Acevedo&lt;/lastname&gt; &lt;occupation&gt;Archivist&lt;/occupation&gt; &lt;/user&gt; &lt;/users&gt; . # write output to file print(output, file=open('users.xml', 'w')) . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#generating-xml-data",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html#generating-xml-data"
  },"128": {
    "doc": "02 - Using Faker",
    "title": "02 - Using Faker",
    "content": " ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/02%20-%20Using%20Faker.html"
  },"129": {
    "doc": "02. CSS Selectors",
    "title": "How CSS Selectors Work",
    "content": "source . Are you new to CSS? This article is for you! Perhaps the biggest key to understanding CSS is understanding selectors. Selectors are what allows you to target specific HTML elements and apply style to them. Let’s not think about style right now though, let’s just focus on the selecting. In the examples below, the CSS would be in a file called something like style.css that is referenced from an HTML document called something like index.html. They are separate files, which is the great thing about CSS, keeping the design away from the document. Here’s what that HTML file would be like: . &lt;!DOCTYPE html&gt; &lt;html lang=\"en\"&gt; &lt;head&gt; &lt;title&gt;We're learning selectors!&lt;/title&gt; &lt;link rel=\"stylesheet\" href=\"style.css\"&gt; &lt;/head&gt; &lt;body&gt; &lt;h1 id=\"yay\"&gt;Yay&lt;/h1&gt; &lt;body&gt; &lt;/html&gt; . And the CSS file would contain just the selector blocks like you’ll see below. CSS . #happy-cake { } . HTML . &lt;!-- WILL match --&gt; &lt;div id=\"happy-cake\"&gt;&lt;/div&gt; &lt;!-- WILL match --&gt; &lt;aside id=\"happy-cake\"&gt;&lt;/aside&gt; &lt;!-- Will NOT match --&gt; &lt;div id=\"sad-cake\"&gt;Wrong ID!&lt;/div&gt; &lt;!-- Will NOT match --&gt; &lt;div class=\"happy-cake\"&gt;That's not an ID!&lt;/div&gt; . ID selectors are the most powerful type of selector in terms of CSS specificity. Meaning that they beat out other types of selectors and the styles defined within win. That sounds good, but that’s typically considered bad, because it’s nice to have lower-specificity selectors that are easier to override when needed. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html#how-css-selectors-work",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html#how-css-selectors-work"
  },"130": {
    "doc": "02. CSS Selectors",
    "title": "Class Selector",
    "content": "CSS .module { } . HTML* . &lt;!-- WILL match --&gt; &lt;div class=\"module\"&gt;&lt;/div&gt; &lt;!-- WILL match --&gt; &lt;aside class=\"country module iceland\"&gt;&lt;/aside&gt; &lt;!-- Will NOT match --&gt; &lt;div class=\".module\"&gt;The dot is for CSS, not HTML&lt;/div&gt; &lt;!-- Will NOT match --&gt; &lt;div class=\"bigmodule\"&gt;Wrong class&lt;/div&gt; . Class selectors are your friend. They are probably the most useful and versatile selectors out there. In part because they are well supported in all browsers. In part because you can add multiple classes (just separated by a space) on HTML elements. In part because there are JavaScript things you can do specifically for manipulating classes. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html#class-selector",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html#class-selector"
  },"131": {
    "doc": "02. CSS Selectors",
    "title": "Tag Selector",
    "content": "CSS . h2 { } . HTML . &lt;!-- WILL match --&gt; &lt;h2&gt;Hi, Mom&lt;/h2&gt; &lt;main&gt; &lt;div&gt; &lt;!-- WILL match --&gt; &lt;h2&gt;Anywhere&lt;/h2&gt; &lt;/div&gt; &lt;/main&gt; &lt;!-- Will NOT match --&gt; &lt;div class=\"h2\"&gt;Wrong tag, can't trick it&lt;/div&gt; &lt;!-- Will NOT match --&gt; &lt;h2class=\"yolo\"&gt;Make sure that tag has a space after it!&lt;/h2&gt; . Tag selectors are at their most useful when changing properties that are unique to that HTML element. Like setting the list-style on a &lt;ul&gt; or tab-size on a &lt;pre&gt;. Also in reset stylesheets where you are specifically trying to unset styles that browsers apply to certain elements. Don’t rely on them too much though. It’s typically more useful to have a class define styling that you can use on any HTML element. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html#tag-selector",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html#tag-selector"
  },"132": {
    "doc": "02. CSS Selectors",
    "title": "Attribute Selector",
    "content": "CSS . [data-modal=\"open\"] { } . HTML . &lt;!-- WILL match --&gt; &lt;div data-modal=\"open\"&gt;&lt;/div&gt; &lt;!-- WILL match --&gt; &lt;aside class='closed' data-modal='open'&gt;&lt;/aside&gt; &lt;!-- Will NOT match --&gt; &lt;div data-modal=\"false\"&gt;Wrong value&lt;/div&gt; &lt;!-- Will NOT match --&gt; &lt;div data-modal&gt;No value&lt;/div&gt; &lt;!-- Will NOT match --&gt; &lt;div data-modal-open&gt;Wrong attribute&lt;/div&gt; . You might argue that attribute selectors are even more useful than classes because they have the same specificity value, but can be any attribute not just class, plus they can have a value you can select by. Hardly an issue anymore, but attribute selectors aren’t supported in IE 6. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html#attribute-selector",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html#attribute-selector"
  },"133": {
    "doc": "02. CSS Selectors",
    "title": "Positional Selectors",
    "content": "CSS . :nth-child(2) { } . HTML . &lt;ul&gt; &lt;li&gt;nope&lt;/li&gt; &lt;!-- WILL match --&gt; &lt;li&gt;yep, I'm #2&lt;/li&gt; &lt;li&gt;nope&lt;/li&gt; &lt;/ul&gt; . There are several different positional selectors beyond :nth-child. Using simple expressions (like 3n = “every third”) you can select elements based on their position in the HTML. You can play with that idea here or check out some useful recipes. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html#positional-selectors",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html#positional-selectors"
  },"134": {
    "doc": "02. CSS Selectors",
    "title": "Other Pseudo Selectors",
    "content": "CSS . :empty { } . HTML . &lt;!-- WILL match --&gt; &lt;div&gt;&lt;/div&gt; &lt;!-- WILL match --&gt; &lt;aside data-blah&gt;&lt;!-- nothin' --&gt;&lt;/aside&gt; &lt;!-- Will NOT match --&gt; &lt;div&gt; &lt;/div&gt; &lt;!-- Will NOT match --&gt; &lt;div&gt; &lt;/div&gt; . :empty is one of many pseudo selectors, which you can recognize by the colon (:) in them. They typically represent something that you couldn’t know by just the element and attributes alone. Note that these are slightly different than pseudo elements, which you can recognize by the double colon (::). They are responsible for adding things to the page by the things they select. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html#other-pseudo-selectors",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html#other-pseudo-selectors"
  },"135": {
    "doc": "02. CSS Selectors",
    "title": "More on Selectors",
    "content": "Selectors can be combined together. For instance: .module.news { /* Selects elements with BOTH of those classes */ } #site-footer::after { /* Adds content after an element with that ID */ } section[data-open] { /* Selects only section elements if they have this attribute */ } . There are also selector combinators like ~ and + and &gt; that affect selectors, like: .module &gt; h2 { /* Select h2 elements that are direct children of an element with that class */ } h2 + p { /* Select p elements that are directly following an h2 element */ } li ~ li { /* Select li elements that are siblings (and following) another li element. */ } . On CSS-Tricks there is an entire Almanac that covers all the selectors in CSS, as well as properties. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html#more-on-selectors",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html#more-on-selectors"
  },"136": {
    "doc": "02. CSS Selectors",
    "title": "02. CSS Selectors",
    "content": " ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/02.%20CSS%20Selectors.html"
  },"137": {
    "doc": "02.a - Pandas Data analysis Part 1",
    "title": "Importing Data with Pandas",
    "content": "The first step we’ll take is to read the data in. The data is stored as a comma-separated values, or csv, file, where each row is separated by a new line, and each column by a comma (,). Here are the first few rows of the ign.csv file: . ,score_phrase,title,url,platform,score,genre,editors_choice,release_year,release_month,release_day 0,Amazing,LittleBigPlanet PS Vita,/games/littlebigplanet-vita/vita-98907,PlayStation Vita,9.0,Platformer,Y,2012,9,12 1,Amazing,LittleBigPlanet PS Vita -- Marvel Super Hero Edition,/games/littlebigplanet-ps-vita-marvel-super-hero-edition/vita-20027059,PlayStation Vita,9.0,Platformer,Y,2012,9,12 2,Great,Splice: Tree of Life,/games/splice/ipad-141070,iPad,8.5,Puzzle,N,2012,9,12 3,Great,NHL 13,/games/nhl-13/xbox-360-128182,Xbox 360,8.5,Sports,N,2012,9,11 . As you can see above, each row in the data represents a single game that was reviewed by IGN. The columns contain information about that game: . | score_phrase — how IGN described the game in one word. This is linked to the score it received. | title — the name of the game. | url — the URL where you can see the full review. | platform — the platform the game was reviewed on (PC, PS4, etc). | score — the score for the game, from 1.0 to 10.0. | genre — the genre of the game. | editors_choice — N if the game wasn’t an editor’s choice, Y if it was. This is tied to score. | release_year — the year the game was released. | release_month — the month the game was released. | release_day — the day the game was released. | . There’s also a leading column that contains row index values. We can safely ignore this column, but we’ll dive into what index values are later on. In order to be able to work with the data in Python, we’ll need to read the csv file into a Pandas DataFrame. A DataFrame is a way to represent and work with tabular data. Tabular data has rows and columns, just like our csv file. In order to read in the data, we’ll need to use the pandas.read_csv function. This function will take in a csv file and return a DataFrame. The below code will: . | Import the pandas library. We rename it to pd so it’s faster to type out. | Read ign.csv into a DataFrame, and assign the result to reviews. | . # disable warnings for lecture import warnings warnings.filterwarnings('ignore') . import pandas as pd reviews = pd.read_csv(\"ign.csv\") . Once we read in a DataFrame, Pandas gives us two methods that make it fast to print out the data. These functions are: . | pandas.DataFrame.head – prints the first N rows of a DataFrame. By default 5. | pandas.DataFrame.tail – prints the last N rows of a DataFrame. By default 5. | . We’ll use the head method to see what’s in reviews: . reviews.head(3) . | | Unnamed: 0 | score_phrase | title | url | platform | score | genre | editors_choice | release_year | release_month | release_day | . | 0 | 0 | Amazing | LittleBigPlanet PS Vita | /games/littlebigplanet-vita/vita-98907 | PlayStation Vita | 9.0 | Platformer | Y | 2012 | 9 | 12 | . | 1 | 1 | Amazing | LittleBigPlanet PS Vita -- Marvel Super Hero E... | /games/littlebigplanet-ps-vita-marvel-super-he... | PlayStation Vita | 9.0 | Platformer | Y | 2012 | 9 | 12 | . | 2 | 2 | Great | Splice: Tree of Life | /games/splice/ipad-141070 | iPad | 8.5 | Puzzle | N | 2012 | 9 | 12 | . We can also access the pandas.DataFrame.shape property to see row many rows and columns are in reviews: . reviews.shape . (18625, 11) . As you can see, everything has been read in properly – we have 18625 rows and 11 columns. One of the big advantages of Pandas vs just using NumPy is that Pandas allows you to have columns with different data types. reviews has columns that store float values, like score, string values, like score_phrase, and integers, like release_year. Now that we’ve read the data in properly, let’s work on indexing reviews to get the rows and columns that we want. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#importing-data-with-pandas",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#importing-data-with-pandas"
  },"138": {
    "doc": "02.a - Pandas Data analysis Part 1",
    "title": "Indexing DataFrames with Pandas",
    "content": "Earlier, we used the head method to print the first 5 rows of reviews. We could accomplish the same thing using the pandas.DataFrame.iloc method. The iloc method allows us to retrieve rows and columns by position. In order to do that, we’ll need to specify the positions of the rows that we want, and the positions of the columns that we want as well. The below code will replicate reviews.head(): . reviews.iloc[0:5,:] . | | Unnamed: 0 | score_phrase | title | url | platform | score | genre | editors_choice | release_year | release_month | release_day | . | 0 | 0 | Amazing | LittleBigPlanet PS Vita | /games/littlebigplanet-vita/vita-98907 | PlayStation Vita | 9.0 | Platformer | Y | 2012 | 9 | 12 | . | 1 | 1 | Amazing | LittleBigPlanet PS Vita -- Marvel Super Hero E... | /games/littlebigplanet-ps-vita-marvel-super-he... | PlayStation Vita | 9.0 | Platformer | Y | 2012 | 9 | 12 | . | 2 | 2 | Great | Splice: Tree of Life | /games/splice/ipad-141070 | iPad | 8.5 | Puzzle | N | 2012 | 9 | 12 | . | 3 | 3 | Great | NHL 13 | /games/nhl-13/xbox-360-128182 | Xbox 360 | 8.5 | Sports | N | 2012 | 9 | 11 | . | 4 | 4 | Great | NHL 13 | /games/nhl-13/ps3-128181 | PlayStation 3 | 8.5 | Sports | N | 2012 | 9 | 11 | . As you can see above, we specified that we wanted rows 0:5. This means that we wanted the rows from position 0 up to, but not including, position 5. The first row is considered to be in position 0. This gives us the rows at positions 0, 1, 2, 3, and 4. If we leave off the first position value, like :5, it’s assumed we mean 0. If we leave off the last position value, like 0:, it’s assumed we mean the last row or column in the DataFrame. We wanted all of the columns, so we specified just a colon (:), without any positions. This gave us the columns from 0 to the last column. Here are some indexing examples, along with the results: . | reviews.iloc[:5,:] — the first 5 rows, and all of the columns for those rows. | reviews.iloc[:,:] — the entire DataFrame. | reviews.iloc[5:,5:] — rows from position 5 onwards, and columns from position 5 onwards. | reviews.iloc[:,0] — the first column, and all of the rows for the column. | reviews.iloc[9,:] — the 10th row, and all of the columns for that row. | . Indexing by position is very similar to NumPy indexing. Now that we know how to index by position, let’s remove the first column, which doesn’t have any useful information: . reviews = reviews.iloc[:,1:] reviews.head() . | | score_phrase | title | url | platform | score | genre | editors_choice | release_year | release_month | release_day | . | 0 | Amazing | LittleBigPlanet PS Vita | /games/littlebigplanet-vita/vita-98907 | PlayStation Vita | 9.0 | Platformer | Y | 2012 | 9 | 12 | . | 1 | Amazing | LittleBigPlanet PS Vita -- Marvel Super Hero E... | /games/littlebigplanet-ps-vita-marvel-super-he... | PlayStation Vita | 9.0 | Platformer | Y | 2012 | 9 | 12 | . | 2 | Great | Splice: Tree of Life | /games/splice/ipad-141070 | iPad | 8.5 | Puzzle | N | 2012 | 9 | 12 | . | 3 | Great | NHL 13 | /games/nhl-13/xbox-360-128182 | Xbox 360 | 8.5 | Sports | N | 2012 | 9 | 11 | . | 4 | Great | NHL 13 | /games/nhl-13/ps3-128181 | PlayStation 3 | 8.5 | Sports | N | 2012 | 9 | 11 | . reviews.shape . (18625, 10) . Indexing Using Labels in Pandas . Now that we know how to retrieve rows and columns by position, it’s worth looking into the other major way to work with DataFrames, which is to retrieve rows and columns by label. A major advantage of Pandas over NumPy is that each of the columns and rows has a label. Working with column positions is possible, but it can be hard to keep track of which number corresponds to which column. We can work with labels using the pandas.DataFrame.loc method, which allows us to index using labels instead of positions. We can display the first five rows of reviews using the loc method like this: . reviews.loc[0:5,:] . | | score_phrase | title | url | platform | score | genre | editors_choice | release_year | release_month | release_day | . | 0 | Amazing | LittleBigPlanet PS Vita | /games/littlebigplanet-vita/vita-98907 | PlayStation Vita | 9.0 | Platformer | Y | 2012 | 9 | 12 | . | 1 | Amazing | LittleBigPlanet PS Vita -- Marvel Super Hero E... | /games/littlebigplanet-ps-vita-marvel-super-he... | PlayStation Vita | 9.0 | Platformer | Y | 2012 | 9 | 12 | . | 2 | Great | Splice: Tree of Life | /games/splice/ipad-141070 | iPad | 8.5 | Puzzle | N | 2012 | 9 | 12 | . | 3 | Great | NHL 13 | /games/nhl-13/xbox-360-128182 | Xbox 360 | 8.5 | Sports | N | 2012 | 9 | 11 | . | 4 | Great | NHL 13 | /games/nhl-13/ps3-128181 | PlayStation 3 | 8.5 | Sports | N | 2012 | 9 | 11 | . | 5 | Good | Total War Battles: Shogun | /games/total-war-battles-shogun/mac-142565 | Macintosh | 7.0 | Strategy | N | 2012 | 9 | 11 | . The above doesn’t actually look much different from reviews.iloc[0:5,:]. This is because while row labels can take on any values, our row labels match the positions exactly. You can see the row labels on the very left of the table above (they’re in bold). You can also see them by accessing the index property of a DataFrame. We’ll display the row indexes for reviews: . reviews.index . RangeIndex(start=0, stop=18625, step=1) . # pull out the 1st twenty indexes list(reviews.index)[:20] . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] . Indexes don’t always have to match up with positions, though. In the below code cell, we’ll: . Get row 10 to row 20 of reviews, and assign the result to some_reviews. Display the first 5 rows of some_reviews. some_reviews = reviews.iloc[10:20,] some_reviews.head() . | | score_phrase | title | url | platform | score | genre | editors_choice | release_year | release_month | release_day | . | 10 | Good | Tekken Tag Tournament 2 | /games/tekken-tag-tournament-2/ps3-124584 | PlayStation 3 | 7.5 | Fighting | N | 2012 | 9 | 11 | . | 11 | Good | Tekken Tag Tournament 2 | /games/tekken-tag-tournament-2/xbox-360-124581 | Xbox 360 | 7.5 | Fighting | N | 2012 | 9 | 11 | . | 12 | Good | Wild Blood | /games/wild-blood/iphone-139363 | iPhone | 7.0 | NaN | N | 2012 | 9 | 10 | . | 13 | Amazing | Mark of the Ninja | /games/mark-of-the-ninja-135615/xbox-360-129276 | Xbox 360 | 9.0 | Action, Adventure | Y | 2012 | 9 | 7 | . | 14 | Amazing | Mark of the Ninja | /games/mark-of-the-ninja-135615/pc-143761 | PC | 9.0 | Action, Adventure | Y | 2012 | 9 | 7 | . As we mentioned earlier, column labels can make life much easier when you’re working with data. We can specify column labels in the loc method to retrieve columns by label instead of by position. reviews.loc[:5, \"score\"] . 0 9.0 1 9.0 2 8.5 3 8.5 4 8.5 5 7.0 Name: score, dtype: float64 . We can also specify more than one column at a time by passing in a list: . reviews.loc[:5, [\"score\", \"release_year\"]] . | | score | release_year | . | 0 | 9.0 | 2012 | . | 1 | 9.0 | 2012 | . | 2 | 8.5 | 2012 | . | 3 | 8.5 | 2012 | . | 4 | 8.5 | 2012 | . | 5 | 7.0 | 2012 | . reviews.loc[[0, 3], [\"score\", \"release_year\"]] . | | score | release_year | . | 0 | 9.0 | 2012 | . | 3 | 8.5 | 2012 | . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#indexing-dataframes-with-pandas",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#indexing-dataframes-with-pandas"
  },"139": {
    "doc": "02.a - Pandas Data analysis Part 1",
    "title": "Pandas Series Objects",
    "content": "We can retrieve an individual column in Pandas a few different ways. So far, we’ve seen two types of syntax for this: . | reviews.iloc[:,1] — will retrieve the second column. | reviews.loc[:,\"score_phrase\"] — will also retrieve the second column. | . There’s a third, even easier, way to retrieve a whole column. We can just specify the column name in square brackets, like with a dictionary: . reviews[\"score\"].head() . 0 9.0 1 9.0 2 8.5 3 8.5 4 8.5 Name: score, dtype: float64 . We can also use lists of columns with this method: . x = reviews[[\"score\", \"release_year\"]] x.head() . | | score | release_year | . | 0 | 9.0 | 2012 | . | 1 | 9.0 | 2012 | . | 2 | 8.5 | 2012 | . | 3 | 8.5 | 2012 | . | 4 | 8.5 | 2012 | . When we retrieve a single column, we’re actually retrieving a Pandas Series object. A DataFrame stores tabular data, but a Series stores a single column or row of data. We can verify that a single column is a Series: . type(reviews[\"score\"]) . pandas.core.series.Series . We can create a Series manually to better understand how it works. To create a Series, we pass a list or NumPy array into the Series object when we instantiate it: . s1 = pd.Series([1, 2]) s1 . 0 1 1 2 dtype: int64 . A Series can contain any type of data, including mixed types. Here, we create a Series that contains string objects: . s2 = pd.Series([\"Boris Yeltsin\", \"Mikhail Gorbachev\"]) s2 . 0 Boris Yeltsin 1 Mikhail Gorbachev dtype: object . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#pandas-series-objects",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#pandas-series-objects"
  },"140": {
    "doc": "02.a - Pandas Data analysis Part 1",
    "title": "Creating A DataFrame in Pandas",
    "content": "We can create a DataFrame by passing multiple Series into the DataFrame class. Here, we pass in the two Series objects we just created, s1 as the first row, and s2 as the second row: . # create a dataframe from two series x = pd.DataFrame() x['rank'] = s1 x['name'] = s2 x . | | rank | name | . | 0 | 1 | Boris Yeltsin | . | 1 | 2 | Mikhail Gorbachev | . # create a datafrme from a dictionary y = pd.DataFrame({ 'rank': s1, 'name': s2 }) y . | | rank | name | . | 0 | 1 | Boris Yeltsin | . | 1 | 2 | Mikhail Gorbachev | . # create a dataframe from a list pd.DataFrame([s1, s2]) . | | 0 | 1 | . | 0 | 1 | 2 | . | 1 | Boris Yeltsin | Mikhail Gorbachev | . We can also accomplish the same thing with a list of lists. Each inner list is treated as a row in the resulting DataFrame: . # create a dataframe from a list of lists pd.DataFrame( [ [1, \"Boris Yeltsin\"], [2, \"Mikhail Gorbachev\"] ] ) . | | 0 | 1 | . | 0 | 1 | Boris Yeltsin | . | 1 | 2 | Mikhail Gorbachev | . We can specify the column labels when we create a DataFrame: . # specify column labels pd.DataFrame( [ [1, \"Boris Yeltsin\"], [2, \"Mikhail Gorbachev\"] ], columns=[\"rank\", \"name\"] ) . | | rank | name | . | 0 | 1 | Boris Yeltsin | . | 1 | 2 | Mikhail Gorbachev | . As well as the row labels (the index): . # specify the index labels frame = pd.DataFrame( [ [1,2], [\"Boris Yeltsin\", \"Mikhail Gorbachev\"] ], index=[\"rank\", \"name\"], columns=[\"person1\", \"person2\"] ) frame . | | person1 | person2 | . | rank | 1 | 2 | . | name | Boris Yeltsin | Mikhail Gorbachev | . We’re then able index the DataFrame using the labels: . frame.loc[\"rank\":\"name\", \"person1\"] . rank 1 name Boris Yeltsin Name: person1, dtype: object . We can skip specifying the columns keyword argument if we pass a dictionary into the DataFrame constructor. This will automatically setup column names: . frame = pd.DataFrame( { \"person1\": [1, \"Boris Yeltsin\"], \"person2\": [2, \"Mikhail Gorbachev\"] } ) frame . | | person1 | person2 | . | 0 | 1 | 2 | . | 1 | Boris Yeltsin | Mikhail Gorbachev | . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#creating-a-dataframe-in-pandas",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#creating-a-dataframe-in-pandas"
  },"141": {
    "doc": "02.a - Pandas Data analysis Part 1",
    "title": "Pandas DataFrame Methods",
    "content": "As we mentioned earlier, each column in a DataFrame is a Series object: . type(reviews[\"title\"]) . pandas.core.series.Series . We can call most of the same methods on a Series object that we can on a DataFrame, including head: . reviews[\"title\"].head() . 0 LittleBigPlanet PS Vita 1 LittleBigPlanet PS Vita -- Marvel Super Hero E... 2 Splice: Tree of Life 3 NHL 13 4 NHL 13 Name: title, dtype: object . Pandas Series and DataFrame also have other methods that make calculations simpler. For example, we can use the pandas.Series.mean method to find the mean of a Series: . reviews[\"score\"].mean() . 6.950459060402685 . We can also call the similar pandas.DataFrame.mean method, which will find the mean of each numerical column in a DataFrame by default: . x = reviews.mean() x . score 6.950459 release_year 2006.515329 release_month 7.138470 release_day 15.603866 dtype: float64 . We can modify the axis keyword argument to mean in order to compute the mean of each row or of each column. By default, axis is equal to 0, and will compute the mean of each column. We can also set it to 1 to compute the mean of each row. Note that this will only compute the mean of the numerical values in each row: . reviews.mean(axis=1).head() . 0 510.500 1 510.500 2 510.375 3 510.125 4 510.125 dtype: float64 . There are quite a few methods on Series and DataFrame that behave like mean. Here are some handy ones: . | pandas.DataFrame.corr — finds the correlation between columns in a DataFrame. | pandas.DataFrame.count — counts the number of non-null values in each DataFrame column. | pandas.DataFrame.max — finds the highest value in each column. | pandas.DataFrame.min — finds the lowest value in each column. | pandas.DataFrame.median — finds the median of each column. | pandas.DataFrame.std — finds the standard deviation of each column. | . We can use the corr method to see if any columns correlation with score. For instance, this would tell us if games released more recently have been getting higher reviews (release_year), or if games released towards the end of the year score better (release_month): . reviews.corr() . | | score | release_year | release_month | release_day | . | score | 1.000000 | 0.062716 | 0.007632 | 0.020079 | . | release_year | 0.062716 | 1.000000 | -0.115515 | 0.016867 | . | release_month | 0.007632 | -0.115515 | 1.000000 | -0.067964 | . | release_day | 0.020079 | 0.016867 | -0.067964 | 1.000000 | . As you can see above, none of our numeric columns correlates with score, meaning that release timing doesn’t linearly relate to review score. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#pandas-dataframe-methods",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#pandas-dataframe-methods"
  },"142": {
    "doc": "02.a - Pandas Data analysis Part 1",
    "title": "DataFrame Math with Pandas",
    "content": "We can also perform math operations on Series or DataFrame objects. For example, we can divide every value in the score column by 2 to switch the scale from 0-10 to 0-5: . reviews[\"score\"] / 2 . 0 4.50 1 4.50 2 4.25 3 4.25 4 4.25 ... 18620 3.80 18621 4.50 18622 2.90 18623 5.00 18624 5.00 Name: score, Length: 18625, dtype: float64 . All the common mathematical operators that work in Python, like +, -, *, /, and ^ will work, and will apply to each element in a DataFrame or a Series. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#dataframe-math-with-pandas",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#dataframe-math-with-pandas"
  },"143": {
    "doc": "02.a - Pandas Data analysis Part 1",
    "title": "Boolean Indexing in Pandas",
    "content": "As we saw above, the mean of all the values in the score column of reviews is around 7. What if we wanted to find all the games that got an above average score? We could start by doing a comparison. The comparison compares each value in a Series to a specified value, then generate a Series full of Boolean values indicating the status of the comparison. For example, we can see which of the rows have a score value higher than 7: . score_filter = reviews[\"score\"] &gt; 7 score_filter . 0 True 1 True 2 True 3 True 4 True ... 18620 True 18621 True 18622 False 18623 True 18624 True Name: score, Length: 18625, dtype: bool . Once we have a Boolean Series, we can use it to select only rows in a DataFrame where the Series contains the value True. So, we could only select rows in reviews where score is greater than 7: . filtered_reviews = reviews[score_filter] filtered_reviews.head() . | | score_phrase | title | url | platform | score | genre | editors_choice | release_year | release_month | release_day | . | 0 | Amazing | LittleBigPlanet PS Vita | /games/littlebigplanet-vita/vita-98907 | PlayStation Vita | 9.0 | Platformer | Y | 2012 | 9 | 12 | . | 1 | Amazing | LittleBigPlanet PS Vita -- Marvel Super Hero E... | /games/littlebigplanet-ps-vita-marvel-super-he... | PlayStation Vita | 9.0 | Platformer | Y | 2012 | 9 | 12 | . | 2 | Great | Splice: Tree of Life | /games/splice/ipad-141070 | iPad | 8.5 | Puzzle | N | 2012 | 9 | 12 | . | 3 | Great | NHL 13 | /games/nhl-13/xbox-360-128182 | Xbox 360 | 8.5 | Sports | N | 2012 | 9 | 11 | . | 4 | Great | NHL 13 | /games/nhl-13/ps3-128181 | PlayStation 3 | 8.5 | Sports | N | 2012 | 9 | 11 | . It’s possible to use multiple conditions for filtering. Let’s say we want to find games released for the Xbox One that have a score of more than 7. In the below code, we: . | Setup a filter with two conditions: . | Check if score is greater than 7. | Check if platform equals Xbox One | . | Apply the filter to reviews to get only the rows we want. | Use the head method to print the first 5 rows of filtered_reviews. | . # setup the boolean filtr xbox_one_filter = (reviews[\"score\"] &gt; 7) &amp; (reviews[\"platform\"] == \"Xbox One\") # select the data based on the filter filtered_reviews = reviews[xbox_one_filter] # display the first 5 results filtered_reviews.head() . | | score_phrase | title | url | platform | score | genre | editors_choice | release_year | release_month | release_day | . | 17137 | Amazing | Gone Home | /games/gone-home/xbox-one-20014361 | Xbox One | 9.5 | Simulation | Y | 2013 | 8 | 15 | . | 17197 | Amazing | Rayman Legends | /games/rayman-legends/xbox-one-20008449 | Xbox One | 9.5 | Platformer | Y | 2013 | 8 | 26 | . | 17295 | Amazing | LEGO Marvel Super Heroes | /games/lego-marvel-super-heroes/xbox-one-20000826 | Xbox One | 9.0 | Action | Y | 2013 | 10 | 22 | . | 17313 | Great | Dead Rising 3 | /games/dead-rising-3/xbox-one-124306 | Xbox One | 8.3 | Action | N | 2013 | 11 | 18 | . | 17317 | Great | Killer Instinct | /games/killer-instinct-2013/xbox-one-20000538 | Xbox One | 8.4 | Fighting | N | 2013 | 11 | 18 | . When filtering with multiple conditions, it’s important to put each condition in parentheses (), and separate them with a single ampersand &amp;. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#boolean-indexing-in-pandas",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#boolean-indexing-in-pandas"
  },"144": {
    "doc": "02.a - Pandas Data analysis Part 1",
    "title": "Pandas Plotting",
    "content": "Now that we know how to filter, we can create plots to observe the review distribution for the Xbox One vs the review distribution for the PlayStation 4. This will help us figure out which console has better games. We can do this via a histogram, which will plot the frequencies for different score ranges. This will tell us which console has more highly reviewed games. We can make a histogram for each console using the pandas.DataFrame.plot method. This method utilizes matplotlib, the popular Python plotting library, under the hood to generate good-looking plots. The plot method defaults to drawing a line graph. We’ll need to pass in the keyword argument kind=\"hist\" to draw a histogram instead. In the below code, we: . | Call %matplotlib inline to set up plotting inside a Jupyter notebook. | Filter reviews to only have data about the Xbox One. | Plot the score column. | . reviews[\"platform\"].unique() . array(['PlayStation Vita', 'iPad', 'Xbox 360', 'PlayStation 3', 'Macintosh', 'PC', 'iPhone', 'Nintendo DS', 'Nintendo 3DS', 'Android', 'Wii', 'PlayStation 4', 'Wii U', 'Linux', 'PlayStation Portable', 'PlayStation', 'Nintendo 64', 'Saturn', 'Lynx', 'Game Boy', 'Game Boy Color', 'NeoGeo Pocket Color', 'Game.Com', 'Dreamcast', 'Dreamcast VMU', 'WonderSwan', 'Arcade', 'Nintendo 64DD', 'PlayStation 2', 'WonderSwan Color', 'Game Boy Advance', 'Xbox', 'GameCube', 'DVD / HD Video Game', 'Wireless', 'Pocket PC', 'N-Gage', 'NES', 'iPod', 'Genesis', 'TurboGrafx-16', 'Super NES', 'NeoGeo', 'Master System', 'Atari 5200', 'TurboGrafx-CD', 'Atari 2600', 'Sega 32X', 'Vectrex', 'Commodore 64/128', 'Sega CD', 'Nintendo DSi', 'Windows Phone', 'Web Games', 'Xbox One', 'Windows Surface', 'Ouya', 'New Nintendo 3DS', 'SteamOS'], dtype=object) . # enable display of chart inline %matplotlib inline # setup the filter platform_filter = reviews[\"platform\"] == \"Xbox One\" # select and plot reviews[platform_filter][\"score\"].plot(kind=\"hist\") . &lt;AxesSubplot:ylabel='Frequency'&gt; . We can also do the same for the PS4: . # setup the filter mask = reviews[\"platform\"].isin([\"Xbox One\", \"PlayStation 4\"]) # select and plot reviews[mask][\"score\"].plot(kind=\"hist\") . &lt;AxesSubplot:ylabel='Frequency'&gt; . It appears from our histogram that the PlayStation 4 has many more highly rated games than the Xbox One. # plot all filtered_reviews[\"score\"].hist() . &lt;AxesSubplot:&gt; . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#pandas-plotting",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html#pandas-plotting"
  },"145": {
    "doc": "02.a - Pandas Data analysis Part 1",
    "title": "02.a - Pandas Data analysis Part 1",
    "content": "Source . Python is a great language for doing data analysis, primarily because of the fantastic ecosystem of data-centric Python packages. Pandas is one of those packages, and makes importing and analyzing data much easier. Pandas builds on packages like NumPy and matplotlib to give you a single, convenient, place to do most of your data analysis and visualization work. In this introduction, we’ll use Pandas to analyze data on video game reviews from IGN, a popular video game review site. The data was scraped by Eric Grinstein, and can be found here. As we analyze the video game reviews, we’ll learn key Pandas concepts like indexing. Do games like the Witcher 3 tend to get better reviews on the PS4 than the Xbox One? This dataset can help us find out. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.a%20-%20Pandas%20Data%20analysis%20Part%201.html"
  },"146": {
    "doc": "02.b - Pandas Data analysis Part 2",
    "title": "Reading in and summarizing the data",
    "content": "Our first step is to read in the data and do some preliminary exploration. This will help us figure out how we want to approach creating groups and finding patterns. As you may recall from part one of this tutorial, we can read in the data using the pandas.read_csv function. The data is stored using Latin-1 encoding, so we additionally need to specify the encoding keyword argument. If we don’t, pandas won’t be able to load in the data, and we’ll get an error: . # disable warnings for lecture import warnings warnings.filterwarnings('ignore') . import pandas as pd data = pd.read_csv(\"thanksgiving-2015-poll-data.csv\", encoding=\"Latin-1\") data.head() . | | RespondentID | Do you celebrate Thanksgiving? | What is typically the main dish at your Thanksgiving dinner? | What is typically the main dish at your Thanksgiving dinner? - Other (please specify) | How is the main dish typically cooked? | How is the main dish typically cooked? - Other (please specify) | What kind of stuffing/dressing do you typically have? | What kind of stuffing/dressing do you typically have? - Other (please specify) | What type of cranberry saucedo you typically have? | What type of cranberry saucedo you typically have? - Other (please specify) | ... | Have you ever tried to meet up with hometown friends on Thanksgiving night? | Have you ever attended a \"Friendsgiving?\" | Will you shop any Black Friday sales on Thanksgiving Day? | Do you work in retail? | Will you employer make you work on Black Friday? | How would you describe where you live? | Age | What is your gender? | How much total combined money did all members of your HOUSEHOLD earn last year? | US Region | . | 0 | 4337954960 | Yes | Turkey | NaN | Baked | NaN | Bread-based | NaN | None | NaN | ... | Yes | No | No | No | NaN | Suburban | 18 - 29 | Male | $75,000 to $99,999 | Middle Atlantic | . | 1 | 4337951949 | Yes | Turkey | NaN | Baked | NaN | Bread-based | NaN | Other (please specify) | Homemade cranberry gelatin ring | ... | No | No | Yes | No | NaN | Rural | 18 - 29 | Female | $50,000 to $74,999 | East South Central | . | 2 | 4337935621 | Yes | Turkey | NaN | Roasted | NaN | Rice-based | NaN | Homemade | NaN | ... | Yes | Yes | Yes | No | NaN | Suburban | 18 - 29 | Male | $0 to $9,999 | Mountain | . | 3 | 4337933040 | Yes | Turkey | NaN | Baked | NaN | Bread-based | NaN | Homemade | NaN | ... | Yes | No | No | No | NaN | Urban | 30 - 44 | Male | $200,000 and up | Pacific | . | 4 | 4337931983 | Yes | Tofurkey | NaN | Baked | NaN | Bread-based | NaN | Canned | NaN | ... | Yes | No | No | No | NaN | Urban | 30 - 44 | Male | $100,000 to $124,999 | Pacific | . 5 rows × 65 columns . As you can see above, the data has 65 columns of mostly categorical data. For example, the first column appears to allow for Yes and No responses only. Let’s verify by using the pandas.Series.unique method to see what unique values are in the Do you celebrate Thanksgiving? column of data: . # investigate respondent values data[\"Do you celebrate Thanksgiving?\"].unique() . array(['Yes', 'No'], dtype=object) . # investigate respondent values data[\"What is typically the main dish at your Thanksgiving dinner?\"].unique() . array(['Turkey', 'Tofurkey', 'Other (please specify)', nan, 'Ham/Pork', 'Turducken', 'Roast beef', 'Chicken', \"I don't know\"], dtype=object) . We can also view all the column names to see all of the survey questions. We’ll truncate the output below to save you from having to scroll: . data.columns[50:] . Index(['Which of these desserts do you typically have at Thanksgiving dinner? Please select all that apply. - Other (please specify).1', 'Do you typically pray before or after the Thanksgiving meal?', 'How far will you travel for Thanksgiving?', 'Will you watch any of the following programs on Thanksgiving? Please select all that apply. - Macy's Parade', 'What's the age cutoff at your \"kids' table\" at Thanksgiving?', 'Have you ever tried to meet up with hometown friends on Thanksgiving night?', 'Have you ever attended a \"Friendsgiving?\"', 'Will you shop any Black Friday sales on Thanksgiving Day?', 'Do you work in retail?', 'Will you employer make you work on Black Friday?', 'How would you describe where you live?', 'Age', 'What is your gender?', 'How much total combined money did all members of your HOUSEHOLD earn last year?', 'US Region'], dtype='object') . Using this Thanksgiving survey data, we can answer quite a few interesting questions, like: . | Do people in Suburban areas eat more Tofurkey than people in Rural areas? | Where do people go to Black Friday sales most often? | Is there a correlation between praying on Thanksgiving and income? | What income groups are most likely to have homemade cranberry sauce? | . In order to answer these questions and others, we’ll first need to become familiar with applying, grouping and aggregation in Pandas. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#reading-in-and-summarizing-the-data",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#reading-in-and-summarizing-the-data"
  },"147": {
    "doc": "02.b - Pandas Data analysis Part 2",
    "title": "Applying functions to Series in pandas",
    "content": "There are times when we’re using pandas that we want to apply a function to every row or every column in the data. A good example is getting from the values in our What is your gender? column to numeric values. We’ll assign 0 to Male, and 1 to Female. Before we dive into transforming the values, let’s confirm that the values in the column are either Male or Female. We can use the pandas.Series.value_counts method to help us with this. We’ll pass the dropna=False keyword argument to also count missing values: . # investigate respondent values data[\"What is your gender?\"].unique() . array(['Male', 'Female', nan], dtype=object) . # get respondent counts per response data[\"What is your gender?\"].value_counts(dropna=False) . Female 544 Male 481 NaN 33 Name: What is your gender?, dtype: int64 . As you can see, not all of the values are Male or Female. We’ll preserve any missing values in the final output when we transform our column. Here’s a diagram of the input and outputs we need: . Source: https://www.dataquest.io/blog/content/images/2017/12/ditaa_diagram_1-2.png . We’ll need to apply a custom function to each value in the What is your gender? column to get the output we want. Here’s a function that will do the transformation we want: . # print the integer representation of True and False print(int(True), int(False)) . 1 0 . import math def gender_code(gender_val): # check to see if the gender iss a float or not specified if isinstance(gender_val, float) and math.isnan(gender_val): # if so return the float or nan value return gender_val # otherwise return 1 for female and 0 for male return int(gender_val.lower().strip() == \"female\") . In order to apply this function to each item in the What is your gender? column, we could either write a for loop, and loop across each element in the column, or we could use the pandas.Series.apply method. This method will take a function as input, then return a new pandas Series that contains the results of applying the function to each item in the Series. We can assign the result back to a column in the data DataFrame, then verify the results using value_counts: . # calculate a new gender code column where 1 represents female and 0 represents male gender_codes = data[\"What is your gender?\"].apply(gender_code) print(type(gender_codes), gender_codes.head()) . &lt;class 'pandas.core.series.Series'&gt; 0 0.0 1 1.0 2 0.0 3 0.0 4 0.0 Name: What is your gender?, dtype: float64 . # add a new column containing the series gender_codes data[\"gender\"] = gender_codes # display the value counts data[\"gender\"].value_counts(dropna=False) . 1.0 544 0.0 481 NaN 33 Name: gender, dtype: int64 . # check the data type of the new column data[\"gender\"].dtype . dtype('float64') . # change the datatype to categorical data[\"gender\"] = data[\"gender\"].astype('category') # check the datatype data[\"gender\"].dtype . CategoricalDtype(categories=[0.0, 1.0], ordered=False) . # print the top 5 values data[\"gender\"].head() . 0 0.0 1 1.0 2 0.0 3 0.0 4 0.0 Name: gender, dtype: category Categories (2, float64): [0.0, 1.0] . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#applying-functions-to-series-in-pandas",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#applying-functions-to-series-in-pandas"
  },"148": {
    "doc": "02.b - Pandas Data analysis Part 2",
    "title": "Applying functions to DataFrames in pandas",
    "content": "We can use the apply method on DataFrames as well as Series. When we use the pandas.DataFrame.apply method, an entire row or column will be passed into the function we specify. By default, apply will work across each column in the DataFrame. If we pass the axis=1 keyword argument, it will work across each row. In the below example, we check the data type of each column in data using a lambda function. We also call the head method on the result to avoid having too much output: . # create a method to return the datatype of a row def get_type(row): return row.dtype # apply the method to all rows in the dataframe data.apply(get_type).head() . RespondentID int64 Do you celebrate Thanksgiving? object What is typically the main dish at your Thanksgiving dinner? object What is typically the main dish at your Thanksgiving dinner? - Other (please specify) object How is the main dish typically cooked? object dtype: object . # return data types with a lambda instead of a function data.apply(lambda row: row.dtype).head() . RespondentID int64 Do you celebrate Thanksgiving? object What is typically the main dish at your Thanksgiving dinner? object What is typically the main dish at your Thanksgiving dinner? - Other (please specify) object How is the main dish typically cooked? object dtype: object . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#applying-functions-to-dataframes-in-pandas",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#applying-functions-to-dataframes-in-pandas"
  },"149": {
    "doc": "02.b - Pandas Data analysis Part 2",
    "title": "Using the apply method to clean up income",
    "content": "We can now use what we know about the apply method to clean up the How much total combined money did all members of your HOUSEHOLD earn last year? column. Cleaning up the income column will allow us to go from string values to numeric values. First, let’s see all the unique values that are in the How much total combined money did all members of your HOUSEHOLD earn last year? . # specify the column by name column_name = \"How much total combined money did all members of your HOUSEHOLD earn last year?\" # get value counts for the specified column data[column_name].value_counts(dropna=False) . $25,000 to $49,999 180 Prefer not to answer 136 $50,000 to $74,999 135 $75,000 to $99,999 133 $100,000 to $124,999 111 $200,000 and up 80 $10,000 to $24,999 68 $0 to $9,999 66 $125,000 to $149,999 49 $150,000 to $174,999 40 NaN 33 $175,000 to $199,999 27 Name: How much total combined money did all members of your HOUSEHOLD earn last year?, dtype: int64 . Looking at this, there are 4 different patterns for the values in the column: . | Pattern | Example | Notes | . | X to Y | $25,000 to $49,999 | We can convert this to a numeric value by extracting the numbers and averaging them. | . | NaN |   | We’ll preserve NaN values, and not convert them at all. | . | X and up | $200,000 and up | We can convert this to a numeric value by extracting the number. | . | Prefer not to answer |   | We’ll turn this into an NaN value. | . Here is how we want the transformations to work: . Source: https://www.dataquest.io/blog/content/images/2017/12/ditaa_diagram_2-1.png . We can write a function that covers all of these cases. In the below function, we: . Take a string called value as input. | Check to see if value is $200,000 and up, and return 200000 if so. | Check if value is Prefer not to answer, and return NaN if so. | Check if value is NaN, and return NaN if so. | Clean up value by removing any dollar signs or commas. | Split the string to extract the incomes, then average them. | . import numpy as np def clean_income(value): if value == \"$200,000 and up\": return 200000 elif value == \"Prefer not to answer\": return np.nan elif isinstance(value, float) and math.isnan(value): return np.nan # remove commas and $ with empty spaces value = value.replace(\",\", \"\").replace(\"$\", \"\") # split high and low income ranges income_low, income_high = value.split(\" to \") # return the average between the ranges return (int(income_high) + int(income_low)) / 2 . After creating the function, we can apply it to the How much total combined money did all members of your HOUSEHOLD earn last year? column: . column_name = \"How much total combined money did all members of your HOUSEHOLD earn last year?\" data[\"income\"] = data[column_name].apply(clean_income) data[\"income\"].head() . 0 87499.5 1 62499.5 2 4999.5 3 200000.0 4 112499.5 Name: income, dtype: float64 . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#using-the-apply-method-to-clean-up-income",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#using-the-apply-method-to-clean-up-income"
  },"150": {
    "doc": "02.b - Pandas Data analysis Part 2",
    "title": "Grouping data with pandas",
    "content": "Now that we’ve covered applying functions, we can move on to grouping data using Pandas. When performing data analysis, it’s often useful to explore only a subset of the data. For example, what if we want to compare income between people who tend to eat homemade cranberry sauce for Thanksgiving vs people who eat canned cranberry sauce? . First, let’s see what the unique values in the column are: . data[\"What type of cranberry saucedo you typically have?\"].value_counts() . Canned 502 Homemade 301 None 146 Other (please specify) 25 Name: What type of cranberry saucedo you typically have?, dtype: int64 . We can now filter data to get two DataFrames that only contain rows where the What type of cranberry saucedo you typically have? is Canned or Homemade, respectively: . column_name = \"What type of cranberry saucedo you typically have?\" homemade_mask = data[column_name] == \"Homemade\" print(homemade_mask.head()) homemade = data[homemade_mask] . 0 False 1 False 2 True 3 True 4 False Name: What type of cranberry saucedo you typically have?, dtype: bool . canned = data[data[column_name] == \"Canned\"] . Finally, we can use the pandas.Series.mean method to find the average income in homemade and canned: . print(homemade[\"income\"].mean()) print(canned[\"income\"].mean()) . 94878.1072874494 83823.40340909091 . We get our answer, but it took more lines of code than it should have. What if we now want to compute the average income for people who didn’t have cranberry sauce? . An easier way to find groupwise summary statistics with pandas is to use the pandas.DataFrame.groupby method. This method will split a DataFrame into groups based on a column or set of columns. We’ll then be able to perform computations on each group. Here’s how splitting data based on the What type of cranberry saucedo you typically have? column would look: . Source: https://www.dataquest.io/blog/content/images/2017/12/ditaa_diagram_3-1.png . Note how each resulting group only has a single unique value in the What type of cranberry saucedo you typically have? column. One group is created for each unique value in the column we choose to group by. Let’s create groups from the What type of cranberry saucedo you typically have? column: . column_name = \"What type of cranberry saucedo you typically have?\" grouped = data.groupby(column_name) grouped . &lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7faaf8627d90&gt; . As you can see above, the groupby method returns a DataFrameGroupBy object. We can call the pandas.GroupBy.groups method to see what value for the What type of cranberry saucedo you typically have? column is in each group: . grouped.groups . {'Canned': [4, 6, 8, 11, 12, 15, 18, 19, 26, 27, 38, 43, 48, 53, 58, 59, 60, 68, 69, 71, 74, 76, 79, 80, 86, 87, 89, 90, 91, 97, 103, 106, 107, 109, 115, 116, 118, 119, 123, 127, 129, 130, 132, 135, 136, 137, 140, 141, 143, 144, 145, 150, 153, 155, 156, 157, 158, 159, 161, 162, 163, 166, 167, 168, 169, 173, 179, 180, 181, 182, 184, 186, 190, 192, 193, 195, 198, 199, 200, 204, 205, 207, 209, 210, 211, 212, 213, 215, 217, 218, 220, 222, 224, 226, 229, 230, 231, 239, 243, 245, ...], 'Homemade': [2, 3, 5, 7, 13, 14, 16, 20, 21, 23, 25, 28, 30, 32, 33, 37, 39, 42, 44, 46, 52, 54, 56, 57, 62, 64, 66, 70, 82, 83, 85, 88, 93, 94, 96, 98, 101, 102, 108, 110, 111, 112, 114, 120, 122, 128, 134, 138, 139, 152, 165, 171, 172, 174, 175, 176, 177, 178, 183, 188, 189, 194, 201, 202, 203, 208, 219, 223, 225, 232, 234, 235, 236, 238, 241, 242, 244, 246, 248, 254, 255, 256, 259, 261, 262, 263, 264, 268, 281, 285, 286, 287, 290, 291, 292, 295, 298, 300, 302, 303, ...], 'None': [0, 17, 24, 29, 34, 36, 40, 47, 49, 51, 55, 61, 67, 72, 73, 77, 78, 81, 92, 99, 100, 104, 105, 117, 121, 124, 126, 131, 133, 142, 146, 148, 149, 160, 164, 185, 187, 191, 197, 227, 228, 237, 240, 274, 275, 319, 321, 329, 337, 362, 370, 377, 391, 395, 406, 409, 414, 417, 421, 437, 439, 466, 480, 491, 492, 495, 505, 514, 526, 529, 532, 537, 540, 553, 560, 564, 571, 573, 580, 584, 591, 594, 598, 602, 605, 606, 609, 610, 618, 626, 631, 639, 647, 658, 672, 673, 684, 700, 701, 716, ...], 'Other (please specify)': [1, 9, 154, 216, 221, 233, 249, 265, 301, 336, 380, 435, 444, 447, 513, 550, 749, 750, 784, 807, 860, 872, 905, 1000, 1007]} . We can call the pandas.GroupBy.size method to see how many rows are in each group. This is equivalent to the value_counts method on a Series: . grouped.size() . What type of cranberry saucedo you typically have? Canned 502 Homemade 301 None 146 Other (please specify) 25 dtype: int64 . We can also use a loop to manually iterate through the groups: . for name, group in grouped: print(name) print('\\t', group.shape) print('\\t', type(group)) . Canned (502, 67) &lt;class 'pandas.core.frame.DataFrame'&gt; Homemade (301, 67) &lt;class 'pandas.core.frame.DataFrame'&gt; None (146, 67) &lt;class 'pandas.core.frame.DataFrame'&gt; Other (please specify) (25, 67) &lt;class 'pandas.core.frame.DataFrame'&gt; . As you can see above, each group is a DataFrame, and you can use any normal DataFrame methods on it. We can also extract a single column from a group. This will allow us to perform further computations just on that specific column: . grouped[\"income\"] . &lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x7faaf864e490&gt; . As you can see above, this gives us a SeriesGroupBy object. We can then call the normal methods we can call on a DataFrameGroupBy object: . grouped[\"income\"].size() . What type of cranberry saucedo you typically have? Canned 502 Homemade 301 None 146 Other (please specify) 25 Name: income, dtype: int64 . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#grouping-data-with-pandas",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#grouping-data-with-pandas"
  },"151": {
    "doc": "02.b - Pandas Data analysis Part 2",
    "title": "Aggregating values in groups",
    "content": "If all we could do was split a DataFrame into groups, it wouldn’t be of much use. The real power of groups is in the computations we can do after creating groups. We do these computations through the pandas.GroupBy.aggregate method, which we can abbreviate as agg. This method allows us to perform the same computation on every group. For example, we could find the average income for people who served each type of cranberry sauce for Thanksgiving (Canned, Homemade, None, etc). In the below code, we: . | Extract just the income column from grouped, so we don’t find the average of every column. | Call the agg method with np.mean as input. | This will compute the mean for each group, then combine the results from each group. | . | . grouped[\"income\"].agg(np.mean) . What type of cranberry saucedo you typically have? Canned 83823.403409 Homemade 94878.107287 None 78886.084034 Other (please specify) 86629.978261 Name: income, dtype: float64 . grouped[\"income\"].agg(np.std) . What type of cranberry saucedo you typically have? Canned 55835.478014 Homemade 62251.937645 None 54562.750866 Other (please specify) 54175.781001 Name: income, dtype: float64 . grouped[\"income\"].agg(np.median) . What type of cranberry saucedo you typically have? Canned 62499.5 Homemade 87499.5 None 62499.5 Other (please specify) 87499.5 Name: income, dtype: float64 . grouped[\"income\"].agg([np.mean, np.std, np.median]) . | | mean | std | median | . | What type of cranberry saucedo you typically have? | | | | . | Canned | 83823.403409 | 55835.478014 | 62499.5 | . | Homemade | 94878.107287 | 62251.937645 | 87499.5 | . | None | 78886.084034 | 54562.750866 | 62499.5 | . | Other (please specify) | 86629.978261 | 54175.781001 | 87499.5 | . If we left out only selecting the income column, here’s what we’d get: . grouped.agg(np.mean) . | | RespondentID | income | . | What type of cranberry saucedo you typically have? | | | . | Canned | 4.336699e+09 | 83823.403409 | . | Homemade | 4.336792e+09 | 94878.107287 | . | None | 4.336765e+09 | 78886.084034 | . | Other (please specify) | 4.336763e+09 | 86629.978261 | . The above code will find the mean for each group for every column in data. However, most columns are string columns, not integer or float columns, so Pandas didn’t process them, since calling np.mean on them raised an error. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#aggregating-values-in-groups",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#aggregating-values-in-groups"
  },"152": {
    "doc": "02.b - Pandas Data analysis Part 2",
    "title": "Plotting the results of aggregation",
    "content": "We can make a plot using the results of our agg method. This will create a bar chart that shows the average income of each category. %matplotlib inline sauce = grouped.agg(np.mean) sauce[\"income\"].plot(kind=\"bar\") . &lt;AxesSubplot:xlabel='What type of cranberry saucedo you typically have?'&gt; . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#plotting-the-results-of-aggregation",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#plotting-the-results-of-aggregation"
  },"153": {
    "doc": "02.b - Pandas Data analysis Part 2",
    "title": "Aggregating with multiple columns",
    "content": "We can call groupby with multiple columns as input to get more granular groups. If we use the What type of cranberry saucedo you typically have? and What is typically the main dish at your Thanksgiving dinner? columns as input, we’ll be able to find the average income of people who eat Homemade cranberry sauce and Tofurkey, for example: . grouped = data.groupby( [\"What type of cranberry saucedo you typically have?\", \"What is typically the main dish at your Thanksgiving dinner?\"]) grouped.agg(np.mean) . | | | RespondentID | income | . | What type of cranberry saucedo you typically have? | What is typically the main dish at your Thanksgiving dinner? | | | . | Canned | Chicken | 4.336354e+09 | 80999.600000 | . | Ham/Pork | 4.336757e+09 | 77499.535714 | . | I don't know | 4.335987e+09 | 4999.500000 | . | Other (please specify) | 4.336682e+09 | 53213.785714 | . | Roast beef | 4.336254e+09 | 25499.500000 | . | Tofurkey | 4.337157e+09 | 100713.857143 | . | Turkey | 4.336705e+09 | 85242.682045 | . | Homemade | Chicken | 4.336540e+09 | 19999.500000 | . | Ham/Pork | 4.337253e+09 | 96874.625000 | . | I don't know | 4.336084e+09 | NaN | . | Other (please specify) | 4.336863e+09 | 55356.642857 | . | Roast beef | 4.336174e+09 | 33749.500000 | . | Tofurkey | 4.336790e+09 | 57916.166667 | . | Turducken | 4.337475e+09 | 200000.000000 | . | Turkey | 4.336791e+09 | 97690.147982 | . | None | Chicken | 4.336151e+09 | 11249.500000 | . | Ham/Pork | 4.336680e+09 | 61249.500000 | . | I don't know | 4.336412e+09 | 33749.500000 | . | Other (please specify) | 4.336688e+09 | 119106.678571 | . | Roast beef | 4.337424e+09 | 162499.500000 | . | Tofurkey | 4.336950e+09 | 112499.500000 | . | Turducken | 4.336739e+09 | NaN | . | Turkey | 4.336784e+09 | 74606.275281 | . | Other (please specify) | Ham/Pork | 4.336465e+09 | 87499.500000 | . | Other (please specify) | 4.337335e+09 | 124999.666667 | . | Tofurkey | 4.336122e+09 | 37499.500000 | . | Turkey | 4.336724e+09 | 82916.194444 | . As you can see above, we get a nice table that shows us the mean of each column for each group. This enables us to find some interesting patterns, such as: . | People who have Turducken and Homemade cranberry sauce seem to have high household incomes. | People who eat Canned cranberry sauce tend to have lower incomes, but those who also have Roast Beef have the lowest incomes. | It looks like there’s one person who has Canned cranberry sauce and doesn’t know what type of main dish he’s having. | . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#aggregating-with-multiple-columns",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#aggregating-with-multiple-columns"
  },"154": {
    "doc": "02.b - Pandas Data analysis Part 2",
    "title": "Aggregating with multiple functions",
    "content": "We can also perform aggregation with multiple functions. This enables us to calculate the mean and standard deviation of a group, for example. In the below code, we find the sum, standard deviation, and mean of each group in the income column: . grouped[\"income\"].agg([np.mean, np.sum, np.std]).head(10) . | | | mean | sum | std | . | What type of cranberry saucedo you typically have? | What is typically the main dish at your Thanksgiving dinner? | | | | . | Canned | Chicken | 80999.600000 | 404998.0 | 75779.481062 | . | Ham/Pork | 77499.535714 | 1084993.5 | 56645.063944 | . | I don't know | 4999.500000 | 4999.5 | NaN | . | Other (please specify) | 53213.785714 | 372496.5 | 29780.946290 | . | Roast beef | 25499.500000 | 127497.5 | 24584.039538 | . | Tofurkey | 100713.857143 | 704997.0 | 61351.484439 | . | Turkey | 85242.682045 | 34182315.5 | 55687.436102 | . | Homemade | Chicken | 19999.500000 | 59998.5 | 16393.596311 | . | Ham/Pork | 96874.625000 | 387498.5 | 77308.452805 | . | I don't know | NaN | 0.0 | NaN | . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#aggregating-with-multiple-functions",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#aggregating-with-multiple-functions"
  },"155": {
    "doc": "02.b - Pandas Data analysis Part 2",
    "title": "Using apply on groups",
    "content": "One of the limitations of aggregation is that each function has to return a single number. While we can perform computations like finding the mean, we can’t for example, call value_counts to get the exact count of a category. We can do this using the pandas.GroupBy.apply method. This method will apply a function to each group, then combine the results. In the below code, we’ll apply value_counts to find the number of people who live in each area type (Rural, Suburban, etc) who eat different kinds of main dishes for Thanksgiving: . grouped = data.groupby(\"How would you describe where you live?\")[\"What is typically the main dish at your Thanksgiving dinner?\"] grouped.apply(lambda x: x.value_counts()) . How would you describe where you live? Rural Turkey 189 Other (please specify) 9 Ham/Pork 7 Tofurkey 3 I don't know 3 Turducken 2 Chicken 2 Roast beef 1 Suburban Turkey 449 Ham/Pork 17 Other (please specify) 13 Tofurkey 9 Chicken 3 Roast beef 3 Turducken 1 I don't know 1 Urban Turkey 198 Other (please specify) 13 Tofurkey 8 Chicken 7 Roast beef 6 Ham/Pork 4 Name: What is typically the main dish at your Thanksgiving dinner?, dtype: int64 . The above table shows us that people who live in different types of areas eat different Thanksgiving main dishes at about the same rate. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#using-apply-on-groups",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html#using-apply-on-groups"
  },"156": {
    "doc": "02.b - Pandas Data analysis Part 2",
    "title": "02.b - Pandas Data analysis Part 2",
    "content": "Source . In this tutorial, we’ll dive into one of the most powerful aspects of pandas — its grouping and aggregation functionality. With this functionality, it’s dead simple to compute group summary statistics, discover patterns, and slice up your data in various ways. Since Thanksgiving was just last week, we’ll use a dataset on what Americans typically eat for Thanksgiving dinner as we explore the pandas library. You can download the dataset here. It contains 1058 online survey responses collected by FiveThirtyEight. Each survey respondent was asked questions about what they typically eat for Thanksgiving, along with some demographic questions, like their gender, income, and location. This dataset will allow us to discover regional and income-based patterns in what Americans eat for Thanksgiving dinner. As we explore the data and try to find patterns, we’ll be heavily using the grouping and aggregation functionality of pandas. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/02.b%20-%20Pandas%20Data%20analysis%20Part%202.html"
  },"157": {
    "doc": "03 - Generating Synthetic Healthcare Data",
    "title": "Generating Synthetic Healthcare Data",
    "content": "One of the hardest things to do in data science is get access to high quality datasets that relate to your specific questions. There’s any number of reasons why researchers and analysts get incorrect results and misread the answers they’re getting. Bad data practice may be leading to bad research. Some bad data practice themes might include: . | Honest Statistical/Computing Error | Honest Misunderstanding of Data | Honest Misapplication of Methods | Honest Failure to Normalize and Malicious Manipulation | (made worse through the) Poor citation practices of Copy-Paste Google Scholar-ship. | . In this class we’re going to learn how to process and analyze data using python. Since I work in healthcare we’ll be using a tool called Synthea which will help us create consistent meaningful datasets at scale in a vareity of formats (Text, CSV, C-CDA, and FHIR). We’ll use these datasets to: . | Learn about basic and advanced python concepts | Learn about handling data in python | Answer simple and complex questions about the data | Generate interesting visualizations | . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/03%20-%20Generating%20Synthetic%20Healthcare%20Data.html#generating-synthetic-healthcare-data",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/03%20-%20Generating%20Synthetic%20Healthcare%20Data.html#generating-synthetic-healthcare-data"
  },"158": {
    "doc": "03 - Generating Synthetic Healthcare Data",
    "title": "What is Synthea?",
    "content": "Synthea Getting Started . SyntheaTM is a synthetic patient generator that models the medical history of synthetic patients. Our mission is to output high-quality synthetic, realistic but not real, patient data and associated health records covering every aspect of healthcare. The resulting data is free from cost, privacy, and security restrictions. It can be used without restriction for a variety of secondary uses in academia, research, industry, and government (although a citation would be appreciated). SyntheaTM generates synthetic patient records using an agent-based approach. Each synthetic patient is generated independently, as they progress from birth to death through modular representations of various diseases and conditions. Each patient runs through every module in the system. Once a patient dies or the simulation reaches the current day, that patient record can be exported in a number of different formats. ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/03%20-%20Generating%20Synthetic%20Healthcare%20Data.html#what-is-synthea",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/03%20-%20Generating%20Synthetic%20Healthcare%20Data.html#what-is-synthea"
  },"159": {
    "doc": "03 - Generating Synthetic Healthcare Data",
    "title": "Synthea Data Formats",
    "content": "Synthea generates these datasets in a variety of commonly used healthcare data formats including (Text, CSV, C-CDA, and FHIR) . Text . Text format is a quick human readable format. This format doesn’t adhere to any particular standard. Text formats are most commonly consumed by humans who may be clinicians or other application end users. The other data formats (CSV, C-CDA, and FHIR) can be easily converted to a Text format. However, converting a Text format to any of the other data formats is extremely challenging. Often times we use Natuarl Language Processing (NLP) and Regular Expressions (RegEx) as we attempt the Text to (CSV, C-CDA, FHIR) conversion. Sample: . Mekhi724 Kemmer911 ================== Race: White Ethnicity: Non-Hispanic Gender: F Age: 33 Birth Date: 1983-11-04 Marital Status: M -------------------------------------------------------------------------------- ALLERGIES: N/A -------------------------------------------------------------------------------- MEDICATIONS: 2013-08-22 [CURRENT] : Acetaminophen 160 MG for Acute bronchitis (disorder) 1996-05-12 [CURRENT] : Acetaminophen 160 MG for Acute bronchitis (disorder) 1995-04-13 [CURRENT] : Acetaminophen 160 MG for Acute bronchitis (disorder) 1984-01-14 [CURRENT] : Penicillin V Potassium 250 MG for Streptococcal sore throat (disorder) -------------------------------------------------------------------------------- CONDITIONS: 2015-10-30 - 2015-11-07 : Fetus with chromosomal abnormality 2015-10-30 - 2015-11-07 : Miscarriage in first trimester 2015-10-30 - 2015-11-07 : Normal pregnancy 2013-08-22 - 2013-09-08 : Acute bronchitis (disorder) 1985-08-07 - : Food Allergy: Fish -------------------------------------------------------------------------------- CARE PLANS: 2013-08-22 [STOPPED] : Respiratory therapy Reason: Acute bronchitis (disorder) Activity: Recommendation to avoid exercise Activity: Deep breathing and coughing exercises -------------------------------------------------------------------------------- OBSERVATIONS: 2014-01-14 : Body Weight 73.9 kg 2014-01-14 : Body Height 163.7 cm 2014-01-14 : Body Mass Index 27.6 kg/m2 2014-01-14 : Systolic Blood Pressure 133.0 mmHg 2014-01-14 : Diastolic Blood Pressure 76.0 mmHg 2014-01-14 : Blood Pressure 2.0 -------------------------------------------------------------------------------- PROCEDURES: 2015-10-30 : Standard pregnancy test for Normal pregnancy 2014-01-14 : Documentation of current medications -------------------------------------------------------------------------------- ENCOUNTERS: 2015-11-07 : Encounter for Fetus with chromosomal abnormality 2015-10-30 : Encounter for Normal pregnancy 2014-01-14 : Outpatient Encounter 2013-08-22 : Encounter for Acute bronchitis (disorder) -------------------------------------------------------------------------------- . CSV . Comma Separated Value (CSV) files are common in healthcare and one could argue one of the 3 most common data formats with the others being HL7v2 and C-CDA. Unlike the Text format generated by Synthea, which only contains a single patient per file, the CSV format contains many patients per file. However, the files themselves are “resource” based. The resources generated include: . | Patients - patients.csv | Encounters - encounters.csv | Allergies - allergies.csv | Medications - medications.csv | Conditions - conditions.csv | Care Plans - careplans.csv | Observations - observations.csv | Procedures - Procedures.csv | Immunizations - immunizations.csv | . Sample: . patients.csv . ID,BIRTHDATE,DEATHDATE,SSN,DRIVERS,PASSPORT,PREFIX,FIRST,LAST,SUFFIX,MAIDEN,MARITAL,RACE,ETHNICITY,GENDER,BIRTHPLACE,ADDRESS 5e0d195e-1cd9-494d-8f9a-757c15da2aed,1946-12-14,2015-10-03,999-12-2377,S99962866,false,Mrs.,Miracle267,Ledner332,,Raynor597,M,white,irish,F,Millbury MA,2502 Fisher Manor Boston MA 02132 52082709-06ce-4fde-9c93-cfb4e6542ae1,1968-05-23,,999-17-1808,S99941406,X41451685X,Mrs.,Alda869,Gorczany848,,Funk527,M,white,italian,F,Gardner MA,46973 Velda Gateway Franklin Town MA 02038 8b4c62c8-b116-4b58-9259-466485b0345c,1967-06-22,1985-07-04,999-11-1173,S99955795,,Ms.,Moshe832,Zulauf396,,,,white,english,F,Boston MA,250 Reba Park Carver MA 02330 965c5539-598b-4a9b-a670-e0259667deb8,1934-11-04,2015-06-19,999-63-2195,S99931866,X71888970X,Mr.,Verla554,Roberts329,,,S,white,irish,M,Fall River MA,321 Abdullah Bridge Needham MA 02492 2b28d6c3-9e0c-48d4-99f9-292488133101,1964-08-13,,999-55-5054,S99990374,X68574707X,Ms.,Henderson277,Labadie810,,,S,black,dominican,F,North Attleborough MA,55825 Barrows Prairie Suite 144 Boston MA 02134 . conditions.csv . START,STOP,PATIENT,ENCOUNTER,CODE,DESCRIPTION 1965-10-10,,5e0d195e-1cd9-494d-8f9a-757c15da2aed,918b17f4-e815-44ef-9eeb-41953bbcf7e9,38341003,Hypertension 1966-09-09,,5e0d195e-1cd9-494d-8f9a-757c15da2aed,918b17f4-e815-44ef-9eeb-41953bbcf7e9,15777000,Prediabetes 1988-09-25,,5e0d195e-1cd9-494d-8f9a-757c15da2aed,918b17f4-e815-44ef-9eeb-41953bbcf7e9,239872002,Osteoarthritis of hip 1990-09-01,,5e0d195e-1cd9-494d-8f9a-757c15da2aed,918b17f4-e815-44ef-9eeb-41953bbcf7e9,410429000,Cardiac Arrest 1990-09-01,,5e0d195e-1cd9-494d-8f9a-757c15da2aed,918b17f4-e815-44ef-9eeb-41953bbcf7e9,429007001,History of cardiac arrest (situation) . We’ll talk more about each of these resources below when we discuss the FHIR data format. C-CDA . Consolidated Clinical Document Architecture (C-CDA) format is an XML-based standard defined by HL7, that uses templates from a standard library to represent clinical concepts. For more information on C-CDA, see http://www.hl7.org/implement/standards/product_brief.cfm?product_id=258. Sample: . &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;ClinicalDocument xmlns=\"urn:hl7-org:v3\" xmlns:sdtc=\"urn:hl7-org:sdtc\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"urn:hl7-org:v3 http://xreg2.nist.gov:8080/hitspValidation/schema/cdar2c32/infrastructure/cda/C32_CDA.xsd\"&gt; &lt;realmCode code=\"US\"/&gt; &lt;typeId root=\"2.16.840.1.113883.1.3\" extension=\"POCD_HD000040\"/&gt; &lt;templateId root=\"2.16.840.1.113883.10.20.22.1.1\" extension=\"2015-08-01\"/&gt; &lt;templateId root=\"2.16.840.1.113883.10.20.22.1.2\" extension=\"2015-08-01\"/&gt; &lt;id root=\"2.16.840.1.113883.19.5\" extension=\"47b10305-eb1b-4a47-a27c-24b7e96ee1da\" assigningAuthorityName=\"https://github.com/synthetichealth/synthea\"/&gt; &lt;code code=\"34133-9\" displayName=\"Summarization of episode note\" codeSystem=\"2.16.840.1.113883.6.1\" codeSystemName=\"LOINC\"/&gt; &lt;title&gt;C-CDA R2.1 Patient Record: Augustine565 Cummings51&lt;/title&gt; &lt;effectiveTime value=\"20190308211454\"/&gt; &lt;confidentialityCode code=\"N\"/&gt; &lt;languageCode code=\"en-US\"/&gt; &lt;recordTarget&gt; &lt;patientRole&gt; &lt;id root=\"2.16.840.1.113883.19.5\" extension=\"47b10305-eb1b-4a47-a27c-24b7e96ee1da\" assigningAuthorityName=\"https://github.com/synthetichealth/synthea\"/&gt; &lt;addr use=\"HP\"&gt; &lt;streetAddressLine&gt;931 Watsica Lock&lt;/streetAddressLine&gt; &lt;city&gt;Pittsburgh&lt;/city&gt; &lt;state&gt;Pennsylvania&lt;/state&gt; &lt;postalCode&gt;15106&lt;/postalCode&gt; &lt;/addr&gt; &lt;telecom nullFlavor=\"NI\"/&gt; &lt;patient&gt; &lt;name&gt; &lt;given&gt;Augustine565&lt;/given&gt; &lt;family&gt;Cummings51&lt;/family&gt; &lt;/name&gt; &lt;administrativeGenderCode code=\"F\" codeSystem=\"2.16.840.1.113883.5.1\" codeSystemName=\"HL7 AdministrativeGender\"/&gt; &lt;birthTime value=\"19690509221454\"/&gt; &lt;raceCode code=\"2028-9\" displayName=\"asian\" codeSystemName=\"CDC Race and Ethnicity\" codeSystem=\"2.16.840.1.113883.6.238\"/&gt; &lt;ethnicGroupCode code=\"2186-5\" displayName=\"non-hispanic\" codeSystemName=\"CDC Race and Ethnicity\" codeSystem=\"2.16.840.1.113883.6.238\"/&gt; &lt;languageCommunication&gt; &lt;languageCode code=\"en-US\"/&gt; &lt;/languageCommunication&gt; &lt;/patient&gt; &lt;/patientRole&gt; &lt;/recordTarget&gt; &lt;!-- ... --&gt; &lt;/ClinicalDocument&gt; . FHIR . HL7 FHIR is possibly the most exciting and interesting data format to deal with. Many of the formats discussed above were created in a time where the vast majority of Health IT systems ran on-premise. These data format and transport protocol standards are extremely reliable, but not so friendly to the web developer. HL7 FHIR started as a community response to the legacy and hard to deal with Health IT standards. In just a few years the community has grown dramatically and most (if not all) EMR vendors have some support for the standard which is unheard of in the Health IT space. The primary drive for this rapid pace has been the US governement who has released a number of regulations and mandates for open data access. FHIR® – Fast Healtcare Interoperability Resources (hl7.org/fhir) – is a next generation standards framework created by HL7. FHIR combines the best features of HL7’s Version 2, Version 3 and CDA® product lines while leveraging the latest web standards and applying a tight focus on implementability. SyntheaTM currently supports exporting patients as Fast Healthcare Interoperability Resources (FHIR), versions 3.5.0 (R4), 3.0.1 (STU3) and 1.0.2 (DSTU2). FHIR is a standard created by HL7 for exchanging healthcare information electronically. While FHIR supports both XML and JSON, Synthea exports FHIR as JSON only. Sample: . { \"resourceType\": \"Bundle\", \"type\": \"transaction\", \"entry\": [ { \"fullUrl\": \"urn:uuid:4bd23de9-7d28-48a5-8093-1ac7ff1c64b7\", \"resource\": { \"resourceType\": \"Patient\", \"id\": \"4bd23de9-7d28-48a5-8093-1ac7ff1c64b7\", \"text\": { \"status\": \"generated\", \"div\": \"&lt;div xmlns=\\\"http://www.w3.org/1999/xhtml\\\"&gt;Generated by &lt;a href=\\\"https://github.com/synthetichealth/synthea\\\"&gt;Synthea&lt;/a&gt;.Version identifier: v2.4.0-44-g6dbf88c6\\n . Person seed: -1236052134575208584 Population seed: 12345&lt;/div&gt;\" }, \"name\": [ { \"use\": \"official\", \"family\": \"Cummings51\", \"given\": [ \"Augustine565\" ], \"prefix\": [ \"Mrs.\" ] }, { \"use\": \"maiden\", \"family\": \"Cremin516\", \"given\": [ \"Augustine565\" ], \"prefix\": [ \"Mrs.\" ] } ] }] } . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/03%20-%20Generating%20Synthetic%20Healthcare%20Data.html#synthea-data-formats",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/03%20-%20Generating%20Synthetic%20Healthcare%20Data.html#synthea-data-formats"
  },"160": {
    "doc": "03 - Generating Synthetic Healthcare Data",
    "title": "03 - Generating Synthetic Healthcare Data",
    "content": " ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/03%20-%20Generating%20Synthetic%20Healthcare%20Data.html",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/03%20-%20Generating%20Synthetic%20Healthcare%20Data.html"
  },"161": {
    "doc": "03 - Loops",
    "title": "for loops",
    "content": "# for loops for i in range(1, 10, 2): print(i) . 1 3 5 7 9 . # for...else loop for i in range(1, 5): print(i) else: print('printed all items') . 1 2 3 4 printed all items . # for...else loop with a break for i in range(1, 5): print(i) if i &gt; 2: break else: print('printed all items') . 1 2 3 . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/03%20-%20Loops.html#for-loops",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/03%20-%20Loops.html#for-loops"
  },"162": {
    "doc": "03 - Loops",
    "title": "while loops",
    "content": "# while loop condition = True i = 0 while condition: print(i) i += 1 if i &gt; 3: condition = False . 0 1 2 3 . # while...else loop condition = True i = 0 while condition: print(i) i += 1 if i &gt; 3: condition = False else: print('done') . 0 1 2 3 done . # while...else loop with a break condition = True i = 0 while condition: print(i) i += 1 if i &gt; 3: break else: print('done') . 0 1 2 3 . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/03%20-%20Loops.html#while-loops",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/03%20-%20Loops.html#while-loops"
  },"163": {
    "doc": "03 - Loops",
    "title": "nested loops",
    "content": "width = 5 chars = \"*@\" for char in chars: for i in range(1, width): print(char * i) for j in range(width, 0, -1): print(char * j) . * ** *** **** ***** **** *** ** * @ @@ @@@ @@@@ @@@@@ @@@@ @@@ @@ @ . width = 5 chars = \"*@\" for char in chars: for i in range(1, width): for spaces in range(width - i, 0, -1): print(' ', end='') print(char * i) for j in range(width, 0, -1): for spaces in range(width - j, 0, -1): print(' ', end='') print(char * j) . * ** *** **** ***** **** *** ** * @ @@ @@@ @@@@ @@@@@ @@@@ @@@ @@ @ . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/03%20-%20Loops.html#nested-loops",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/03%20-%20Loops.html#nested-loops"
  },"164": {
    "doc": "03 - Loops",
    "title": "03 - Loops",
    "content": " ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/03%20-%20Loops.html",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/03%20-%20Loops.html"
  },"165": {
    "doc": "03 - Matplotlib Tutorial Python Plotting",
    "title": "Matplotlib Tutorial Python Plotting",
    "content": "Source . This Matplotlib tutorial takes you through the basics Python data visualization: the anatomy of a plot, pyplot and pylab, and much more. Humans are very visual creatures: we understand things better when we see things visualized. However, the step to presenting analyses, results or insights can be a bottleneck: you might not even know where to start or you might have already a right format in mind, but then questions like “Is this the right way to visualize the insights that I want to bring to my audience?” will have definitely come across your mind. When you’re working with the Python plotting library Matplotlib, the first step to answering the above questions is by building up knowledge on topics like: . | The anatomy of a Matplotlib plot: what is a subplot? What are the Axes? What exactly is a figure? | Plot creation, which could raise questions about what module you exactly need to import (pylab or pyplot?), how you exactly should go about initializing the figure and the Axes of your plot, how to use matplotlib in Jupyter notebooks, etc. | Plotting routines, from simple ways to plot your data to more advanced ways of visualizing your data. | Basic plot customizations, with a focus on plot legends and text, titles, axes labels and plot layout. | Saving, showing, clearing, … your plots: show the plot, save one or more figures to, for example, pdf files, clear the axes, clear the figure or close the plot, etc. | Lastly, you’ll briefly cover two ways in which you can customize Matplotlib: with style sheets and the rc settings. | . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#matplotlib-tutorial-python-plotting",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#matplotlib-tutorial-python-plotting"
  },"166": {
    "doc": "03 - Matplotlib Tutorial Python Plotting",
    "title": "What Does A Matplotlib Python Plot Look Like?",
    "content": "At first sight, it will seem that there are quite some components to consider when you start plotting with this Python data visualization library. You’ll probably agree with me that it’s confusing and sometimes even discouraging seeing the amount of code that is necessary for some plots, not knowing where to start yourself and which components you should use. Luckily, this library is very flexible and has a lot of handy, built-in defaults that will help you out tremendously. As such, you don’t need much to get started: you need to make the necessary imports, prepare some data, and you can start plotting with the help of the plot() function! When you’re ready, don’t forget to show your plot using the show() function. # Import the necessary packages and modules import matplotlib.pyplot as plt import numpy as np # Prepare the data x = np.linspace(0, 10, 100) # Plot the data plt.plot(x, x, label='linear') # Add a legend plt.legend() # Show the plot plt.show() . Note that you import the pyplot module of the matplotlib library under the alias plt. Congrats, you have now successfully created your first plot! Now let’s take a look at the resulting plot in a little bit more detail. What you can’t see on the surface is that you have maybe unconsciously made use of the built-in defaults that take care of the creation of the underlying components, such as the Figure and the Axes. You’ll read more about these defaults in the section that deals with the differences between pylab and pyplot. For now, you’ll understand that working with matplotlib will already become a lot easier when you understand how the underlying components are instantiated. Or, in other words, what the anatomy of a matplotlib plot looks like: . Source . In essence, there are two big components that you need to take into account: . | The Figure is the overall window or page that everything is drawn on. It’s the top-level component of all the ones that you will consider in the following points. You can create multiple independent Figures. A Figure can have several other things in it, such as a suptitle, which is a centered title to the figure. You’ll also find that you can add a legend and color bar, for example, to your Figure. | To the figure you add Axes. The Axes is the area on which the data is plotted with functions such as plot() and scatter() and that can have ticks, labels, etc. associated with it. This explains why Figures can contain multiple Axes. | . Tip: when you see, for example, plt.xlim, you’ll call ax.set_xlim() behind the covers. All methods of an Axes object exist as a function in the pyplot module and vice versa. Note that mostly, you’ll use the functions of the pyplot module because they’re much cleaner, at least for simple plots! . You’ll see what “clean” means when you take a look at the following pieces of code. Compare, for example, this piece of code: . fig = plt.figure() ax = fig.add_subplot(111) ax.plot([1, 2, 3, 4], [10, 20, 25, 30], color='lightblue', linewidth=3) ax.scatter([0.3, 3.8, 1.2, 2.5], [11, 25, 9, 26], color='darkgreen', marker='^') ax.set_xlim(0.5, 4.5) plt.show() . With the piece of code below: . plt.plot([1, 2, 3, 4], [10, 20, 25, 30], color='lightblue', linewidth=3) plt.scatter([0.3, 3.8, 1.2, 2.5], [11, 25, 9, 26], color='darkgreen', marker='^') plt.xlim(0.5, 4.5) plt.show() . The second code chunk is definitely cleaner, isn’it it? . Note: that the above code examples come from the Anatomy of Matplotlib Tutorial by Benjamin Root. However, if you have multiple axes, it’s still better to make use of the first code chunk because it’s always better to prefer explicit above implicit code! In such cases, you want to make use of the Axes object ax. Next to these two components, there are a couple more that you can keep in mind: . | Each Axes has an x-axis and a y-axis, which contain ticks, which have major and minor ticklines and ticklabels. There’s also the axis labels, title, and legend to consider when you want to customize your axes, but also taking into account the axis scales and gridlines might come in handy. | Spines are lines that connect the axis tick marks and that designate the boundaries of the data area. In other words, they are the simple black square that you get to see when you don’t plot any data at all but when you have initialized the Axes, like in the picture below: | . Source . You see that the right and top spines are set to invisible. Note that you’ll sometimes also read about Artist objects, which are virtually all objects that the package has to offers to users like yourself. Everything drawn using Matplotlib is part of the Artist module. The containers that you will use to plot your data, such as Axis, Axes and Figure, and other graphical objects such as text, patches, etc. are types of Artists. For those who have already got some coding experience, it might be good to check out and study the code examples that you find in the Matplotlib gallery. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#what-does-a-matplotlib-python-plot-look-like",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#what-does-a-matplotlib-python-plot-look-like"
  },"167": {
    "doc": "03 - Matplotlib Tutorial Python Plotting",
    "title": "Matplotlib, pyplot and pylab: how are they related?",
    "content": "First off, you’ll already know Matplotlib by now. When you talk about “Matplotlib”, you talk about the whole Python data visualization package. This should not come to you as a big surprise :) . Secondly, pyplot is a module in the matplotlib package. That’s why you often see matplotlib.pyplot in code. The module provides an interface that allows you to implicitly and automatically create figures and axes to achieve the desired plot. This is especially handy when you want to quickly plot something without instantiating any Figures or Axes, as you saw in the example in the first section of this tutorial. You see, you haven’t explicitly specified these components, yet you manage to output a plot that you have even customized! The defaults are initialized and any customizations that you do, will be done with the current Figure and Axes in mind. Lastly, pylab is another module, but it gets installed alongside the matplotlib package. It bulk imports pyplot and the numpy library and was generally recommended when you were working with arrays, doing mathematics interactively and wanted access to plotting features. You might still see this popping up in older tutorials and examples of matplotlib, but its use is no longer recommended, especially not when you’re using the IPython kernel in your Jupyter notebook. You can read more about this here. As a solution, you can best use %matplotlib magic in combination with the right backend, such as inline, qt, etc. Most of the times, you will want to use inline, as this will make sure that the plots are embedded inside the notebook. Read more about that in DataCamp’s Definitive Guide to Jupyter Notebook. Note that also when you’re not working in a Jupyter notebook, you’ll still need to choose a different backend, depending on your use case. In other words, if you don’t want to embed plots inside a notebook, but you rather want to embed them into graphical user interfaces, in batch scripts or web application servers, etc., you will need to specify the backend that you want to use. However, this topic is outside the scope of this tutorial; Instead, the tutorial assumes that you will be using Matplotlib to save your images to your local file system. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#matplotlib-pyplot-and-pylab-how-are-they-related",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#matplotlib-pyplot-and-pylab-how-are-they-related"
  },"168": {
    "doc": "03 - Matplotlib Tutorial Python Plotting",
    "title": "Data For Matplotlib Plots",
    "content": "As you have read in one of the previous sections, Matplotlib is often used to visualize analyses or calcuations. That’s why the first step that you have to take in order to start plotting in Python yourself is to consider revising NumPy, the Python library for scientific computing. Scientific computing might not really seem of much interest, but when you’re doing data science you’ll find yourself working a lot with data that is stored in arrays. You’ll need to perform operations on them, inspect your arrays and manipulate them so that you’re working with the (subset of the) data that is interesting for your analysis and that is in the right format, etc. In short, you’ll find NumPy extremely handy when you’re working with this data visualization library. If you’re interested in taking a NumPy tutorial to start well-prepared, go and take DataCamp’s tutorial and make sure to have your copy of their NumPy cheat sheet close! . Of course, arrays are not the only thing that you pass to your plotting functions; There’s also the possibility to, for example, pass Python lists. If you would like to know more about Python lists, consider checking out their Python list tutorial or the free Intro to Python for Data Science course. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#data-for-matplotlib-plots",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#data-for-matplotlib-plots"
  },"169": {
    "doc": "03 - Matplotlib Tutorial Python Plotting",
    "title": "Create Your Plot",
    "content": "Alright, you’re off to create your first plot yourself with Python! As you have read in one of the previous sections, the Figure is the first step and the key to unlocking the power of this package. Next, you see that you initialize the axes of the Figure in the code chunk above with fig.add_axes(): . # Initialize a Figure fig = plt.figure() # Add Axes to the Figure fig.add_axes([0,0,1,1]) plt.show() . What Is A Subplot? . You have seen all components of a plot and you have initialized your first figure and Axes, but to make things a bit more complicated, you’ll sometimes see subplots pop up in code. You use subplots to set up and place your Axes on a regular grid. So that means that in most cases, Axes and subplot are synonymous, they will designate the same thing. When you do call subplot to add Axes to your figure, do so with the add_subplots() function. There is, however, a difference between the add_axes() and the add_subplots() function, but you’ll learn more about this later on in the tutorial. Consider the following example: . import numpy as np # Create a Figure fig = plt.figure() # Set up Axes ax = fig.add_subplot(111) # Scatter the data ax.scatter(np.linspace(0, 1, 5), np.linspace(0, 5, 5)) # Show the plot plt.show() . You see that the add_subplot() function in itsef also poses you with a challenge, because you see add_subplots(111) in the above code chunk. What does 111 mean? . Well, 111 is equal to 1,1,1, which means that you actually give three arguments to add_subplot(). The three arguments designate the: . | the number of rows (1), | the number of columns (1) | the plot number (1). | . So you actually make one subplot. Note: that you can really go bananas with this function when you are using this function, especially when you’re just starting out with this library and you keep on forgetting for what the three numbers stand. Consider the following commands and try to envision what the plot will look like and how many Axes your Figure will have: ax = fig.add_subplot(2,2,1). Your Figure will have four axes in total, arranged in a structure that has two rows and two columns. With the line of code that you have considered, you say that the variable ax is the first of the four axes to which you want to start plotting. The “first” in this case means that it will be the first axes on the left of the 2x2 structure that you have initialized. What Is The Difference Between add_axes() and add_subplot()? . The difference between fig.add_axes() and fig.add_subplot() doesn’t lie in the result: they both return an Axes object. However, they do differ in the mechanism that is used to add the axes: you pass a list to add_axes() which is the lower left point, the width and the height. This means that the axes object is positioned in absolute coordinates. In contrast, the add_subplot() function doesn’t provide the option to put the axes at a certain position: it does, however, allow the axes to be situated according to a subplot grid, as you have seen in the section above. In most cases, you’ll use add_subplot() to create axes; Only in cases where the positioning matters, you’ll resort to add_axes(). Alternatively, you can also use subplots() if you want to get one or more subplots at the same time. You’ll see an example of how this works in the next section. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#create-your-plot",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#create-your-plot"
  },"170": {
    "doc": "03 - Matplotlib Tutorial Python Plotting",
    "title": "How To Change The Size of Figures",
    "content": "Now that you have seen how to initialize a Figure and Axes from scratch, you will also want to know how you can change certain small details that the package sets up for you, such as the figure size. Let’s say you don’t have the luxury to follow along with the defaults and you want to change this. How do you set the size of your figures manually? . Like everything with this package, it’s pretty easy, but you need to know first what to change. Add an argument figsize to your plt.figure() function of the pyplot module; You just have to specify a tuple with the width and hight of your figure in inches, just like this plt.figure(figsize=(3,4)), for it to work. Note that you can also pass figsize to the the plt.subplots() function of the same module; The inner workings are the same as the figure() function that you’ve just seen. See an example of how this would work here: . # Initialize the plot fig = plt.figure(figsize=(20,10)) ax1 = fig.add_subplot(121) ax2 = fig.add_subplot(122) # Plot the data ax1.bar([1,2,3],[3,4,5]) ax2.barh([0.5,1,2.5],[0,1,2]) # Show the plot plt.show() . # alternatively fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,10)) # or replace the three lines of code above by the following line: #fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,10)) # Plot the data ax1.bar([1,2,3],[3,4,5]) ax2.barh([0.5,1,2.5],[0,1,2]) # Show the plot plt.show() . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#how-to-change-the-size-of-figures",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#how-to-change-the-size-of-figures"
  },"171": {
    "doc": "03 - Matplotlib Tutorial Python Plotting",
    "title": "Working With Pyplot: Plotting Routines",
    "content": "Now that all is set for you to start plotting your data, it’s time to take a closer look at some plotting routines. You’ll often come across functions like plot() and scatter(), which either draw points with lines or markers connecting them, or draw unconnected points, which are scaled or colored. But, as you have already seen in the example of the first section, you shouldn’t forget to pass the data that you want these functions to use! . These functions are only the bare basics. You will need some other functions to make sure your plots look awesome: . | Method | Description | . | ax.bar() | Vertical rectangles | . | ax.barh() | Horizontal rectangles | . | ax.axhline() | Horizontal line across axes | . | ax.vline() | Vertical line across axes | . | ax.fill() | Filled polygons | . | ax.fill_between() | Fill between y-values and 0 | . | ax.stackplot() | Stack plot | . If you’re curious how you can use these functions to plot your data, consider the following example. data = [ {'x': [1,2,3], 'y':[3,4,5], 'horizontal': False}, {'x': [0.5,1,2.5], 'y':[0,1,2], 'horizontal': True} ] # alternatively fig, axes = plt.subplots(len(data), 1, figsize=(10,20)) # or replace the three lines of code above by the following line: #fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,10)) for index, ax in enumerate(axes): if data[index]['horizontal']: ax.barh(data[index]['x'], data[index]['y']) else: ax.bar(data[index]['x'], data[index]['y']) # Show the plot plt.show() . x = np.array([ 0., 0.25, 0.5, 0.75, 1. ], dtype=np.float64) y = np.array([ 0., 1.25, 2.5, 3.75, 5. ], dtype=np.float64) # Initialize the plot # fig = plt.figure(figsize=(15,10)) fig = plt.figure() ax1 = fig.add_subplot(131) ax2 = fig.add_subplot(132) ax3 = fig.add_subplot(133) # Plot the data ax1.bar([1,2,3],[3,4,5]) ax2.barh([0.5,1,2.5],[0,1,2]) ax2.axhline(0.45) ax1.axvline(0.65) ax3.scatter(x,y) # Show the plot plt.show() . Most functions speak for themselves because the names are quite clear. But that doesn’t mean that you need to limit yourself: for example, the fill_between() function is perfect for those who want to create area plots, but they can also be used to create a stacked line graph; Just use the plotting function a couple of times to make sure that the areas overlap and give the illusion of being stacked. Note that, of course, simply passing the data is not enough to create great plots. Make sure to manipulate your data in such a way that the visualization makes sense: don’t be afraid to change your array shape, combine arrays, etc. When you move on and you start to work with vector fields or data distributions, you might want to check out the following functions: . | Method | Description | . | ax.arrow() | Arrow | . | ax.quiver() | 2D field of arrows | . | ax.streamplot() | 2D vector fields | . | ax.hist() | Histogram | . | ax.boxplot() | Boxplot | . | ax.violinplot() | Violinplot | . Note of course that you probably won’t use all of the functions listed in these tables; It really depends on your data and your use case. If you’re totally new to data science, you might want to check out the statistical plotting routines first! . On the other hand, when you work with 2-D or n-D data, you might also find yourself in need of some more advanced plotting routines, like these ones: . | Method | Description | . | ax.pcolor() | Pseudocolor plot | . | ax.pcolormesh() | Pseudocolor plot | . | ax.contour() | Contour plot | . | ax.contourf() | Filled contour plot | . | ax.clabel() | Labeled contour plot | . Note that contour plots are used to explore the potential relationship between three variables. Just like contour plots, also pseudocolor plots can be used for this purpose, since they are surface plot seen from above. Of course, this are not nearly all the functions that you can use to plot your data. If you’re working with images or 2D data, for example, you might also want to check out imshow() to show images in your subplots. For a practical example of how to use the imshow() function, go to DataCamp’s scikit-learn tutorial. The examples in the tutorial also make clear that this data visualization library is really the cherry on the pie in the data science workflow: you have to be quite well-versed in general Python concepts, such as lists and control flow, which can come especially handy if you want to automate the plotting for a great number of subplots. If you feel like revising these concepts, consider taking the free introduction to Python for data science course. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#working-with-pyplot-plotting-routines",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#working-with-pyplot-plotting-routines"
  },"172": {
    "doc": "03 - Matplotlib Tutorial Python Plotting",
    "title": "Customizing Your PyPlot",
    "content": "A lot of questions about this package come from the fact that there are a lot of things that you can do to personalize your plots and make sure that they are unique: besides adjusting the colors, you also have the option to change markers, linestyles and linewidths, add text, legend and annotations, and change the limits and layout of your plots. It’s exactly the fact that there is an endless range of possibilities when it comes to these plots that makes it difficult to set out some things that you need to know when you start working on this topic. Great tips that you should keep in the back of your mind are not only the gallery, which contains many real-life examples that are already coded for you and which you can use, but also the documentation, which can tell you more about the arguments that you can pass to certain functions to adjust visual features. Also keep in mind that there are multiple solutions for one problem and that you learn most of this stuff when you’re getting your hands dirty with the package itself and when you run into troubles. You’ll see some of the most common questions and solutions in this section. Deleting an Axis . If you ever want to remove an axes form your plot, you can use delaxes() to remove and update the current axes. Note: you can restore a deleted axes by adding fig.add_axes(ax) right after fig.delaxes(ax3). # Initialize the plot fig = plt.figure() ax1 = fig.add_subplot(131) ax2 = fig.add_subplot(132) ax3 = fig.add_subplot(133) # Plot the data ax1.bar([1,2,3],[3,4,5]) ax2.barh([0.5,1,2.5],[0,1,2]) ax2.axhline(0.45) ax1.axvline(0.65) ax3.scatter(np.linspace(0, 1, 5), np.linspace(0, 5, 5)) # Delete `ax3` fig.delaxes(ax3) # Show the plot plt.show() . How To Put The Legend Out of the Plot . There are a number of ways to address this question, but mostly all come back to the arguments that you can provide to legend(): . | You can specify the loc or location argument to something like center left or upper right, which ensures that your legend does not fall in the Axes or subplot area. | Alternatively, you can also add the bbox_to_anchor argument to your function and pass a tuple with the coordinates in which you want to put the legend. In this case, the box is put in the upper right corner of the plotting area: ax.legend(bbox_to_anchor=(1.1, 1.05)). | . How To Set Plot Title And Axes Labels . To change your plot title and axes labels, you can follow one of the following approaches, depending of which container of which you want to make use: . | The easiest way to set these things right is by using ax.set(title=\"A title\", xlabel=\"x\", ylabel=\"y\") or ax.set_xlim(), ax.set_ylim() or ax.set_title(). | If you want to work with the figure, you might also resort to fig.suptitle() to add a title to your plot. | If you’re making use of the default settings that the package has to offer, you might want to use plt.title(), plt.xlabel(), plt.ylabel(). | Define your own style sheet or change the default matplotlibrc settings. Read more about this here. | . How To Fix The Plot Layout . A thing to consider when you’re using subplots to build up your plot is the tight_layout function, which will help you to make sure that the plots fit nicely in your figure. You ideally call it after you have plotted your data and customized your plot; So that’s right before you call plt.show() that you should use plt.tight_layout(). Additionally, you might also be interested to use subplots_adjust(), which allows you to manually set the width and height reserved for blank space between subplots, and also fix the left and right sides, and the top and bottom of the subplots. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#customizing-your-pyplot",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#customizing-your-pyplot"
  },"173": {
    "doc": "03 - Matplotlib Tutorial Python Plotting",
    "title": "Showing, Saving And Closing Your Plot",
    "content": "After you have done all the necessary customizations, you will want to show your plot because, as you will have noticed from working in the terminal, you just get to see that an object is made, but you never see the nice plot every time you make adjustments. In the first example of this tutorial, this was implicitly done; . Do you remember? It’s this piece of code: . # Prepare the data x = np.linspace(0, 10, 100) # Plot the data plt.plot(x, x, label='linear') # Add a legend plt.legend() # Show the plot plt.show() . The line plt.show() says indeed that you want to see the plot. If you execute this line, you’ll see a window popping up. And you’ll see if it looks like what you had in your mind! . But this is where your questions start. How can I save this image and if it’s not to your liking, can you clear the image so that you can start anew? The following short sections will cover these questions. How To Save A Plot To An Image File . You can easily save a figure to, for example, a png file by making use of plt.savefig(). The only argument you need to pass to this function is the file name, just like in this example: . # Prepare the data x = np.linspace(0, 10, 100) # Plot the data plt.plot(x, x, label='linear') # Add a legend plt.legend() # Save Figure plt.savefig(\"foo.png\") . # Prepare the data x = np.linspace(0, 10, 100) # Plot the data plt.plot(x, x, label='linear') # Add a legend plt.legend() # Save Transparent Figure plt.savefig(\"foo_transparent.png\", transparent=True) . How To Save A Plot To A Pdf File . If you want to save multiple plots to a pdf file, you want to make use of the pdf backend, which you can easily import: . # Import PdfPages from matplotlib.backends.backend_pdf import PdfPages # Prepare the data x = np.linspace(0, 10, 100) # Plot the data plt.plot(x, x, label='linear') # Add a legend plt.legend() # Initialize the pdf file pp = PdfPages('multipage.pdf') # Save the figure to the file pp.savefig() # Close the file pp.close() . When To Use cla(), clf() or close()? . When you’re finally ready with the inspection of your plot, it’s time to move on with something else (maybe with another plot!). When you’re working with this data visualization library for the first time, it might be weird at start because you can, of course, shut down the GUI window that appears, but that’s usually not the way you want to handle things, because it doesn’t always work as well when you’re working on several things at a time. You have to explicitly tell Matplotlib to close down the plot that you’ve been working on so that you can move on. There are three functions that will come in handy once you’re at this point: . Use: . | plt.cla() to clear an axis | plt.clf() to clear the entire figure | plt.close() to close a window that has popped up to show you your plot | . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#showing-saving-and-closing-your-plot",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#showing-saving-and-closing-your-plot"
  },"174": {
    "doc": "03 - Matplotlib Tutorial Python Plotting",
    "title": "Customizing Matplotlib",
    "content": "By now, you’re already familiar with some basic options to customize your plots. But what if the customizations that you want to make situate more on a library level instead of a plot level? In such cases, also, you don’t need to panic: Matplotlib offers you several options to adjust some of the internal workings. This section will just cover two options, namely style sheets and rc settings. If you want to know more, definitely check out this page. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#customizing-matplotlib",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#customizing-matplotlib"
  },"175": {
    "doc": "03 - Matplotlib Tutorial Python Plotting",
    "title": "How To Use A ggplot2 Style",
    "content": "For the R enthusiasts among you, Matplotlib also offers you the option to set the style of the plots to ggplot. You can easily do this by running the following piece of code: . # Set the style to `ggplot` plt.style.use(\"ggplot\") . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#how-to-use-a-ggplot2-style",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#how-to-use-a-ggplot2-style"
  },"176": {
    "doc": "03 - Matplotlib Tutorial Python Plotting",
    "title": "rc Settings",
    "content": "“rc” is common for configuration files: they usually end in rc. It comes from the practice of having configs as executables: they are automatically run and configure settings, for example. You can read more about it here. Matplotlib has such an rc file to which you can make adjustments dynamically and statically. To dynamically change default rc settings, you can use the rcParams variable: . # without rc Settings # Prepare the data x = np.linspace(0, 10, 100) # Plot the data plt.plot(x, x, label='linear') # Add a legend plt.legend() # Show the plot plt.show() . # with rc Settings import matplotlib as mpl mpl.rcParams['lines.linewidth'] = 5 # Prepare the data x = np.linspace(0, 10, 100) # Plot the data plt.plot(x, x, label='linear') # Add a legend plt.legend() # Show the plot plt.show() . You just adjusted the line width in the example above, but you can also change figure size and dpi, line width, color and style, axes, axis and grid properties, text and font properties, … . If you want to work more statically, you should probably also know that you have a matplotlibrc configuration file, which you can use to customize all kinds of properties (just like you did above with the line width parameter). If you want to find this specific file, you can just run the following: . import matplotlib matplotlib.matplotlib_fname() . '/Users/bk/opt/miniconda3/envs/cmu39/lib/python3.9/site-packages/matplotlib/mpl-data/matplotlibrc' . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#rc-settings",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#rc-settings"
  },"177": {
    "doc": "03 - Matplotlib Tutorial Python Plotting",
    "title": "Continue Learning",
    "content": "Congratulations! You have gone through today’s Matplotlib tutorial successfully! There is still much to learn, but you’re definitely ready to go out on your own and create your own amazing plots. Don’t miss out on DataCamp’s Matplotlib cheat sheet that can help you to make plots in no time, step by step. If you’re eager to discover more from Matplotlib, consider checking out DataCamp’s Viewing 3D Volumetric Data With Matplotlib tutorial to learn how to work with matplotlib’s event handler API or this tutorial, in which you’ll learn all about animating your plots. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#continue-learning",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html#continue-learning"
  },"178": {
    "doc": "03 - Matplotlib Tutorial Python Plotting",
    "title": "03 - Matplotlib Tutorial Python Plotting",
    "content": " ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/03%20-%20Matplotlib%20Tutorial%20Python%20Plotting.html"
  },"179": {
    "doc": "03 - Sets and Set Comprehensions",
    "title": "Sets and Set Comprehensions",
    "content": "Python Sets Set Comprehensions . Perhaps you recall learning about sets and set theory at some point in your mathematical education. Maybe you even remember Venn diagrams: . In mathematics, a rigorous definition of a set can be abstract and difficult to grasp. Practically though, a set can be thought of simply as a well-defined collection of distinct objects, typically called elements or members. Grouping objects into a set can be useful in programming as well, and Python provides a built-in set type to do so. Sets are distinguished from other object types by the unique operations that can be performed on them. Here’s what you’ll learn in this tutorial: . | You’ll see how to define set objects in Python and discover the operations that they support. | You’ll also learn about frozen sets, which are similar to sets except for one important detail. | . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/03%20-%20Sets%20and%20Set%20Comprehensions.html#sets-and-set-comprehensions",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/03%20-%20Sets%20and%20Set%20Comprehensions.html#sets-and-set-comprehensions"
  },"180": {
    "doc": "03 - Sets and Set Comprehensions",
    "title": "Defining a Set",
    "content": "Python’s built-in set type has the following characteristics: . | Sets are unordered. | Set elements are unique. Duplicate elements are not allowed. | A set itself may be modified, but the elements contained in the set must be of an immutable type. | . A set can be created in two ways. First, you can define a set with the built-in set() function: . x = set(&lt;iter&gt;) . x = set(['foo', 'bar', 'baz', 'foo', 'qux']) print(x) . {'bar', 'qux', 'baz', 'foo'} . y = set(('foo', 'bar', 'baz', 'foo', 'qux')) print(y) . {'bar', 'qux', 'baz', 'foo'} . z = set() print(type(z), z) . &lt;class 'set'&gt; set() . Strings are also iterable, so a string can be passed to set() as well. You have already seen that list(s) generates a list of the characters in the string s. Similarly, set(s) generates a set of the characters in s: . s = 'data focused python' print(s) . data focused python . for c in s: print(c) . d a t a f o c u s e d p y t h o n . print(list(s)) . ['d', 'a', 't', 'a', ' ', 'f', 'o', 'c', 'u', 's', 'e', 'd', ' ', 'p', 'y', 't', 'h', 'o', 'n'] . print(set(s)) . {'h', 'y', ' ', 'u', 'c', 'e', 'p', 'd', 'o', 't', 'a', 'n', 's', 'f'} . s = 'data focused python is cool because we learn python and work with data' words = s.split(' ') print(words) print(set(words)) . ['data', 'focused', 'python', 'is', 'cool', 'because', 'we', 'learn', 'python', 'and', 'work', 'with', 'data'] {'because', 'python', 'we', 'work', 'is', 'with', 'data', 'focused', 'learn', 'cool', 'and'} . You can see that the resulting sets are unordered: the original order, as specified in the definition, is not necessarily preserved. Additionally, duplicate values are only represented in the set once, as with the string ‘foo’ in the first two examples and the letter ‘u’ in the third. Alternately, a set can be defined with curly braces ({}): . x = {&lt;obj&gt;, &lt;obj&gt;, ..., &lt;obj&gt;} . When a set is defined this way, each becomes a distinct element of the set, even if it is an iterable. This behavior is similar to that of the ```.append()``` list method. Thus, the sets shown above can also be defined like this: . x = { 'foo', 'bar', 'baz', 'foo', 'qux' } print(type(x), x) . &lt;class 'set'&gt; {'bar', 'qux', 'baz', 'foo'} . y = {'q', 'u', 'u', 'x'} print(type(y), y) . &lt;class 'set'&gt; {'x', 'u', 'q'} . To recap: . | The argument to set() is an iterable. It generates a list of elements to be placed into the set. | The objects in curly braces are placed into the set intact, even if they are iterable. | . Observe the difference between these two set definitions: . x = {'foo'} print(type(x), x) . &lt;class 'set'&gt; {'foo'} . y = set('foo') print(type(y), y) . &lt;class 'set'&gt; {'f', 'o'} . A set can be empty. However, recall that Python interprets empty curly braces ({}) as an empty dictionary, so the only way to define an empty set is with the set() function: . x = {} print(type(x)) . &lt;class 'dict'&gt; . y = set() print(type(y)) . &lt;class 'set'&gt; . An empty set is falsy in Boolean context: . x = set() . bool(x) . False . x or 1 . 1 . x and 1 . set() . You might think the most intuitive sets would contain similar objects—for example, even numbers or surnames: . s1 = {2, 4, 6, 8, 10} s2 = {'Smith', 'McArthur', 'Wilson', 'Johansson'} . Python does not require this, though. The elements in a set can be objects of different types: . x = {42, 'foo', 3.14159, None} print(x) . {42, 3.14159, 'foo', None} . Don’t forget that set elements must be immutable. For example, a tuple may be included in a set: . x = {42, 'foo', (1, 2, 3), 3.14159} print(x) . {42, 3.14159, 'foo', (1, 2, 3)} . But lists and dictionaries are mutable, so they can’t be set elements: . a = [1, 2, 3] x = {a} . --------------------------------------------------------------------------- TypeError Traceback (most recent call last) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_93351/1659420868.py in &lt;module&gt; 1 a = [1, 2, 3] ----&gt; 2 x = {a} TypeError: unhashable type: 'list' . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/03%20-%20Sets%20and%20Set%20Comprehensions.html#defining-a-set",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/03%20-%20Sets%20and%20Set%20Comprehensions.html#defining-a-set"
  },"181": {
    "doc": "03 - Sets and Set Comprehensions",
    "title": "Set Size and Membership",
    "content": "x = {'foo', 'bar', 'baz'} . len(x) . 3 . 'bar' in x . True . 'qux' in x . False . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/03%20-%20Sets%20and%20Set%20Comprehensions.html#set-size-and-membership",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/03%20-%20Sets%20and%20Set%20Comprehensions.html#set-size-and-membership"
  },"182": {
    "doc": "03 - Sets and Set Comprehensions",
    "title": "Operating on a Set",
    "content": "Many of the operations that can be used for Python’s other composite data types don’t make sense for sets. For example, sets can’t be indexed or sliced. However, Python provides a whole host of operations on set objects that generally mimic the operations that are defined for mathematical sets. Operators vs. Methods . Most, though not quite all, set operations in Python can be performed in two different ways: by operator or by method. Let’s take a look at how these operators and methods work, using set union as an example. Given two sets, x1 and x2, the union of x1 and x2 is a set consisting of all elements in either set. Consider these two sets: . x1 = {'foo', 'bar', 'baz'} x2 = {'baz', 'qux', 'quux'} . The union of x1 and x2 is {'foo', 'bar', 'baz', 'qux', 'quux'}. In Python, set union can be performed with the | operator: . union_x = x1 | x2 print(union_x) . {'baz', 'foo', 'bar', 'qux', 'quux'} . # note : there aren't any duplicates print(len(union_x), len(x1), len(x2)) . 5 3 3 . Set union can also be obtained with the .union() method. The method is invoked on one of the sets, and the other is passed as an argument: . x1.union(x2) . {'bar', 'baz', 'foo', 'quux', 'qux'} . | The way they are used in the examples above, the operator and method behave identically. But there is a subtle difference between them. When you use the | operator, both operands must be sets. The .union() method, on the other hand, will take any iterable as an argument, convert it to a set, and then perform the union. | . Observe the difference between these two statements: . x1 | ('baz', 'qux', 'quux') . --------------------------------------------------------------------------- TypeError Traceback (most recent call last) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_93351/1141666621.py in &lt;module&gt; ----&gt; 1 x1 | ('baz', 'qux', 'quux') TypeError: unsupported operand type(s) for |: 'set' and 'tuple' . x1.union(('baz', 'qux', 'quux')) . {'bar', 'baz', 'foo', 'quux', 'qux'} . Available Operators and Methods . Below is a list of the set operations available in Python. Some are performed by operator, some by method, and some by both. The principle outlined above generally applies: where a set is expected, methods will typically accept any iterable as an argument, but operators require actual sets as operands. Union . x1 = {'foo', 'bar', 'baz'} x2 = {'baz', 'qux', 'quux'} print(x1.union(x2)) print(x1 | x2) . {'baz', 'foo', 'bar', 'qux', 'quux'} {'baz', 'foo', 'bar', 'qux', 'quux'} . More than two sets may be specified with either the operator or the method: . a = {1, 2, 3, 4} b = {2, 3, 4, 5} c = {3, 4, 5, 6} d = {4, 5, 6, 7} print(a.union(b, c, d)) print(a | b | c | d) . {1, 2, 3, 4, 5, 6, 7} {1, 2, 3, 4, 5, 6, 7} . Intersection . x1 = {'foo', 'bar', 'baz'} x2 = {'baz', 'qux', 'quux'} print(x1.intersection(x2)) print(x1 &amp; x2) . {'baz'} {'baz'} . You can specify multiple sets with the intersection method and operator, just like you can with set union: . a = {1, 2, 3, 4} b = {2, 3, 4, 5} c = {3, 4, 5, 6} d = {4, 5, 6, 7} print(a.intersection(b, c, d)) print(a &amp; b &amp; c &amp; d) . {4} {4} . Difference . x1 = {'foo', 'bar', 'baz'} x2 = {'baz', 'qux', 'quux'} print(x1.difference(x2)) print(x1 - x2) . {'bar', 'foo'} {'bar', 'foo'} . Once again, you can specify more than two sets: . a = {1, 2, 3, 30, 300} b = {10, 20, 30, 40} c = {100, 200, 300, 400} print(a.difference(b, c)) print(a - b - c) . {1, 2, 3} {1, 2, 3} . When multiple sets are specified, the operation is performed from left to right. In the example above, a - b is computed first, resulting in {1, 2, 3, 300}. Then c is subtracted from that set, leaving {1, 2, 3}: . Symmetric Difference . x1 = {'foo', 'bar', 'baz'} x2 = {'baz', 'qux', 'quux'} print(x1.symmetric_difference(x2)) print(x1 ^ x2) . {'foo', 'bar', 'qux', 'quux'} {'foo', 'bar', 'qux', 'quux'} . The ^ operator also allows more than two sets: . a = {1, 2, 3, 4, 5} b = {10, 2, 3, 4, 50} c = {1, 50, 100} print(a ^ b ^ c) . {100, 5, 10} . Disjoint . Determines whether or not two sets have any elements in common. x1.isdisjoint(x2) returns True if x1 and x2 have no elements in common: . x1 = {'foo', 'bar', 'baz'} x2 = {'baz', 'qux', 'quux'} print(x1.isdisjoint(x2)) . False . x3 = x2 - {'baz'} print(x1.isdisjoint(x3)) . True . If x1.isdisjoint(x2) is True, then x1 &amp; x2 is the empty set: . x1 = {1, 3, 5} x2 = {2, 4, 6} print(x1.isdisjoint(x2)) . True . print(x1 &amp; x2) . set() . Is Subset . Determine whether one set is a subset of the other. In set theory, a set x1 is considered a subset of another set x2 if every element of x1 is in x2. x1.issubset(x2) and x1 &lt;= x2 return True if x1 is a subset of x2: . x1 = {'foo', 'bar', 'baz'} print(x1.issubset({'foo', 'bar', 'baz', 'qux', 'quux'})) . True . x2 = {'baz', 'qux', 'quux'} print(x1 &lt;= x2) . False . Is Proper Subset . A proper subset is the same as a subset, except that the sets can’t be identical. A set x1 is considered a proper subset of another set x2 if every element of x1 is in x2, and x1 and x2 are not equal. x1 = {'foo', 'bar'} x2 = {'foo', 'bar', 'baz'} print(x1 &lt; x2) . True . x1 = {'foo', 'bar', 'baz'} x2 = {'foo', 'bar', 'baz'} print(x1 &lt; x2) . False . Is Superset . A superset is the reverse of a subset. A set x1 is considered a superset of another set x2 if x1 contains every element of x2. x1.issuperset(x2) and x1 &gt;= x2 return True if x1 is a superset of x2: . x1 = {'foo', 'bar', 'baz'} print(x1.issuperset({'foo', 'bar'})) . True . x2 = {'baz', 'qux', 'quux'} print(x1 &gt;= x2) . False . Is Proper Superset . A proper superset is the same as a superset, except that the sets can’t be identical. A set x1 is considered a proper superset of another set x2 if x1 contains every element of x2, and x1 and x2 are not equal. x1 = {'foo', 'bar', 'baz'} x2 = {'foo', 'bar'} print(x1 &gt; x2) . True . x1 = {'foo', 'bar', 'baz'} x2 = {'foo', 'bar', 'baz'} print(x1 &gt; x2) . False . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/03%20-%20Sets%20and%20Set%20Comprehensions.html#operating-on-a-set",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/03%20-%20Sets%20and%20Set%20Comprehensions.html#operating-on-a-set"
  },"183": {
    "doc": "03 - Sets and Set Comprehensions",
    "title": "Frozen Sets",
    "content": "Python provides another built-in type called a frozenset, which is in all respects exactly like a set, except that a frozenset is immutable. You can perform non-modifying operations on a frozenset: . x = frozenset(['foo', 'bar', 'baz']) print(x) . frozenset({'bar', 'baz', 'foo'}) . print(len(x)) . 3 . print(x &amp; {'baz', 'qux', 'quux'}) . frozenset({'baz'}) . But methods that attempt to modify a frozenset fail: . x.add('quux') . --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_93351/3158252394.py in &lt;module&gt; ----&gt; 1 x.add('quux') AttributeError: 'frozenset' object has no attribute 'add' . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/03%20-%20Sets%20and%20Set%20Comprehensions.html#frozen-sets",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/03%20-%20Sets%20and%20Set%20Comprehensions.html#frozen-sets"
  },"184": {
    "doc": "03 - Sets and Set Comprehensions",
    "title": "Set Comprehension",
    "content": "import random from random import randint . seed = 1234 random.seed(seed) x = 0 y = 5 a = [randint(x, y) for i in range(0, 10)] print(a) . [3, 0, 0, 0, 4, 0, 5, 5, 0, 0] . random.seed(seed) x = 0 y = 5 b = {randint(x, y) for i in range(0, 10)} print(b) . {0, 3, 4, 5} . random.seed(seed) a = ['Even' if i % 2 else 'Odd' for i in range(10)] print(a) . ['Odd', 'Even', 'Odd', 'Even', 'Odd', 'Even', 'Odd', 'Even', 'Odd', 'Even'] . random.seed(seed) b = {'Even' if i % 2 else 'Odd' for i in range(10)} print(b) . {'Even', 'Odd'} . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/03%20-%20Sets%20and%20Set%20Comprehensions.html#set-comprehension",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/03%20-%20Sets%20and%20Set%20Comprehensions.html#set-comprehension"
  },"185": {
    "doc": "03 - Sets and Set Comprehensions",
    "title": "03 - Sets and Set Comprehensions",
    "content": " ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/03%20-%20Sets%20and%20Set%20Comprehensions.html",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/03%20-%20Sets%20and%20Set%20Comprehensions.html"
  },"186": {
    "doc": "03 - Using fhirclient to parse Healthcare Data",
    "title": "fhirclient",
    "content": "Note: this example uses FHIR DSTU3 whereas synthea now supports FHIR R4 so you’ll need a different version of the fhirclient library to deal with each dataset . The fhirclient, a flexible Python client for FHIR servers supporting the SMART on FHIR protocol. fhirclient versioning is not identical to FHIR versioning, see the full table for reference. | Version | FHIR |   | . | 4.0.0 | 4.0.0 | (R4) | . | 3.0.0 | 3.0.0 | (STU-3) | . | 1.0.3 | 1.0.2 | (DSTU 2) | . | 1.0 | 1.0.1 | (DSTU 2) | . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/03%20-%20Using%20fhirclient%20to%20parse%20Healthcare%20Data.html#fhirclient",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/03%20-%20Using%20fhirclient%20to%20parse%20Healthcare%20Data.html#fhirclient"
  },"187": {
    "doc": "03 - Using fhirclient to parse Healthcare Data",
    "title": "Installation",
    "content": "For FHIR version 3 . pip install fhirclient . or for FHIR version 4 . pip install git+https://github.com/smart-on-fhir/client-py.git . NOTE: We’ll use FHIR R4 here so you need to install from GitHub . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/03%20-%20Using%20fhirclient%20to%20parse%20Healthcare%20Data.html#installation",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/03%20-%20Using%20fhirclient%20to%20parse%20Healthcare%20Data.html#installation"
  },"188": {
    "doc": "03 - Using fhirclient to parse Healthcare Data",
    "title": "Documentation",
    "content": "Technical documentation is available at docs. ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/03%20-%20Using%20fhirclient%20to%20parse%20Healthcare%20Data.html#documentation",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/03%20-%20Using%20fhirclient%20to%20parse%20Healthcare%20Data.html#documentation"
  },"189": {
    "doc": "03 - Using fhirclient to parse Healthcare Data",
    "title": "Client Use",
    "content": "To connect to a SMART on FHIR server (or any open FHIR server), you can use the FHIRClient class. It will initialize and handle a FHIRServer instance, your actual handle to the FHIR server you’d like to access. Read Data from Server . To read a given patient from an open FHIR server, you can use: . from pprint import pprint from fhirclient import client settings = { 'app_id': 'my_web_app', 'api_base': 'http://hapi.fhir.org/baseR4' } smart = client.FHIRClient(settings=settings) import fhirclient.models.patient as p patient = p.Patient.read('2543713', smart.server) print(patient.birthDate.isostring) print(smart.human_name(patient.name[0])) . 1974-12-25 Peter James Chalmers . If this is a protected server, you will first have to send your user to the authorize endpoint to log in. | Call smart.authorize_url to obtain the correct URL. | You can use smart.prepare(), which will return False if the server is protected and you need to authorize. | The smart.ready property has the same purpose, it will however not retrieve the server’s CapabilityStatement resource and hence is only useful as a quick check whether the server instance is ready. | . | . smart = client.FHIRClient(settings=settings) smart.ready # prints False print(smart.prepare()) # prints True after fetching CapabilityStatement print(smart.ready) # prints True print(smart.prepare()) # prints True immediately print(smart.authorize_url) . True True True None . You can work with the FHIRServer class directly, without using FHIRClient, but this is not recommended: . Search Records on Server . You can also search for resources matching a particular set of criteria: . smart = client.FHIRClient(settings=settings) import fhirclient.models.patient as p search = p.Patient.where(struct={'family': 'Cushing'}) patients = search.perform_resources(smart.server) # print(observations) pprint(patients[0].as_json()) . {'id': '2581341', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': 'dbee78a3-a72d-41a2-b705-0023498f5228'}], 'meta': {'lastUpdated': '2021-10-07T05:30:35.359+00:00', 'source': '#ghjs1LZGhTvv0mp2', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb &lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;dbee78a3-a72d-41a2-b705-0023498f5228&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}} . # to get the raw Bundle instead of resources only, you can use: bundle = search.perform(smart.server) pprint(bundle.as_json()) . {'entry': [{'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/2581341', 'resource': {'id': '2581341', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': 'dbee78a3-a72d-41a2-b705-0023498f5228'}], 'meta': {'lastUpdated': '2021-10-07T05:30:35.359+00:00', 'source': '#ghjs1LZGhTvv0mp2', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;dbee78a3-a72d-41a2-b705-0023498f5228&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/2581393', 'resource': {'id': '2581393', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': '2575e281-136c-489b-9b4e-4baba209258e'}], 'meta': {'lastUpdated': '2021-10-07T05:30:55.659+00:00', 'source': '#CHofcK6KSc6z9CD3', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;2575e281-136c-489b-9b4e-4baba209258e&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/2581353', 'resource': {'id': '2581353', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': '174ee1ec-c239-4b82-84a6-f0354aa6ee5f'}], 'meta': {'lastUpdated': '2021-10-07T05:30:39.301+00:00', 'source': '#vB6DyRxG5VKe1DAW', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;174ee1ec-c239-4b82-84a6-f0354aa6ee5f&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/2581361', 'resource': {'id': '2581361', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': '7c411bcd-c418-42d5-98d1-15aac35c7be0'}], 'meta': {'lastUpdated': '2021-10-07T05:30:41.751+00:00', 'source': '#bmgIW3uJ6beOv2VV', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;7c411bcd-c418-42d5-98d1-15aac35c7be0&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/2581369', 'resource': {'id': '2581369', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': '9972a064-bebe-424c-9093-3c9a159ef810'}], 'meta': {'lastUpdated': '2021-10-07T05:30:47.767+00:00', 'source': '#Gv3zhMPhdfY3kmZV', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;9972a064-bebe-424c-9093-3c9a159ef810&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/2581377', 'resource': {'id': '2581377', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': 'f5f32d1f-4cb7-46ca-aa9e-2cf0ceddcf2d'}], 'meta': {'lastUpdated': '2021-10-07T05:30:50.334+00:00', 'source': '#H1FwlSuIbTH8HoQO', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;f5f32d1f-4cb7-46ca-aa9e-2cf0ceddcf2d&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/2581385', 'resource': {'id': '2581385', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': '6354a4f3-d14c-46fd-935b-061f12ae61cf'}], 'meta': {'lastUpdated': '2021-10-07T05:30:52.897+00:00', 'source': '#PY6zZl3CLaJBv1iE', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;6354a4f3-d14c-46fd-935b-061f12ae61cf&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/1710186', 'resource': {'id': '1710186', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': '4221bcac-4dc9-4bc4-9998-c926228c82b6'}], 'meta': {'lastUpdated': '2020-12-12T05:30:20.585+00:00', 'source': '#MW9t13hB5Sot1wr8', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;4221bcac-4dc9-4bc4-9998-c926228c82b6&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/1710194', 'resource': {'id': '1710194', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': '8c01f1ab-b1bd-49ed-8d2b-d05604d3b647'}], 'meta': {'lastUpdated': '2020-12-12T05:30:25.227+00:00', 'source': '#oFl3LmIy0jJBy2uW', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;8c01f1ab-b1bd-49ed-8d2b-d05604d3b647&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/1710202', 'resource': {'id': '1710202', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': 'c7fdceca-5c46-43a3-8612-f05b653d8d50'}], 'meta': {'lastUpdated': '2020-12-12T05:30:26.105+00:00', 'source': '#PObX9KJkPSlqEaqq', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;c7fdceca-5c46-43a3-8612-f05b653d8d50&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/1710210', 'resource': {'id': '1710210', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': '524ba1f0-4d0b-4156-a027-090271ad0b9d'}], 'meta': {'lastUpdated': '2020-12-12T05:30:27.026+00:00', 'source': '#QJBnM09DMDikgg58', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;524ba1f0-4d0b-4156-a027-090271ad0b9d&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/1710218', 'resource': {'id': '1710218', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': '670ff5c0-1bb8-49b6-afa4-ba1ea116dfcd'}], 'meta': {'lastUpdated': '2020-12-12T05:30:27.852+00:00', 'source': '#fr8pH05YdV3GcXaX', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;670ff5c0-1bb8-49b6-afa4-ba1ea116dfcd&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/1710226', 'resource': {'id': '1710226', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': '71083472-9e8e-4850-9e71-3e0ea9eb72bf'}], 'meta': {'lastUpdated': '2020-12-12T05:30:28.868+00:00', 'source': '#n8oLKEzzwtyWtIq0', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;71083472-9e8e-4850-9e71-3e0ea9eb72bf&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/1710238', 'resource': {'id': '1710238', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': 'b0f4dc2f-62b2-480f-a9de-9330e20bcf73'}], 'meta': {'lastUpdated': '2020-12-12T05:30:30.304+00:00', 'source': '#qTLtPryVVRvxQWJx', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;b0f4dc2f-62b2-480f-a9de-9330e20bcf73&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/1567009', 'resource': {'id': '1567009', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': '835855bd-e369-4dad-b29f-88b3eac35886'}], 'meta': {'lastUpdated': '2020-10-13T05:30:33.394+00:00', 'source': '#v4eEjiC3mU1IFuh1', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;835855bd-e369-4dad-b29f-88b3eac35886&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/1567021', 'resource': {'id': '1567021', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': 'b769ce39-54b0-42cd-bfc1-b2bc00bc8503'}], 'meta': {'lastUpdated': '2020-10-13T05:30:35.498+00:00', 'source': '#PPFZF3sf3RtNCdg3', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;b769ce39-54b0-42cd-bfc1-b2bc00bc8503&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/1567029', 'resource': {'id': '1567029', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': '67687225-1f93-49fc-a269-f33b1dea0fc2'}], 'meta': {'lastUpdated': '2020-10-13T05:30:36.314+00:00', 'source': '#YKCsK4p3FGH5uptc', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;67687225-1f93-49fc-a269-f33b1dea0fc2&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/1567037', 'resource': {'id': '1567037', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': 'c345df12-ff3a-431a-b149-77c4e20d1e84'}], 'meta': {'lastUpdated': '2020-10-13T05:30:40.008+00:00', 'source': '#7xIEqCRuJyGrEyWX', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;c345df12-ff3a-431a-b149-77c4e20d1e84&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/1567045', 'resource': {'id': '1567045', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': 'e4d1beaa-5034-4f52-9489-69c7a4af355d'}], 'meta': {'lastUpdated': '2020-10-13T05:30:40.845+00:00', 'source': '#1ig80EBKlB6QubTN', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;e4d1beaa-5034-4f52-9489-69c7a4af355d&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}, {'fullUrl': 'http://hapi.fhir.org/baseR4/Patient/1567053', 'resource': {'id': '1567053', 'identifier': [{'type': {'coding': [{'code': 'MR', 'system': 'http://hl7.org/fhir/v2/0203'}]}, 'value': '02d67065-0107-4f99-b967-ee4396a037ba'}], 'meta': {'lastUpdated': '2020-10-13T05:30:41.748+00:00', 'source': '#Jrmnbq6HWRAMwvQ6', 'versionId': '1'}, 'name': [{'family': 'Cushing', 'given': ['Caleb']}], 'resourceType': 'Patient', 'text': {'div': '&lt;div ' 'xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;div ' 'class=\"hapiHeaderText\"&gt;Caleb ' '&lt;b&gt;CUSHING &lt;/b&gt;&lt;/div&gt;&lt;table ' 'class=\"hapiPropertyTable\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Identifier&lt;/td&gt;&lt;td&gt;02d67065-0107-4f99-b967-ee4396a037ba&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;', 'status': 'generated'}}, 'search': {'mode': 'match'}}], 'id': '80e8c67f-800d-4fce-8299-5b674b257a5b', 'link': [{'relation': 'self', 'url': 'http://hapi.fhir.org/baseR4/Patient?family=Cushing'}, {'relation': 'next', 'url': 'http://hapi.fhir.org/baseR4?_getpages=80e8c67f-800d-4fce-8299-5b674b257a5b&amp;_getpagesoffset=20&amp;_count=20&amp;_pretty=true&amp;_bundletype=searchset'}], 'meta': {'lastUpdated': '2021-10-21T14:34:50.512+00:00'}, 'resourceType': 'Bundle', 'type': 'searchset'} . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/03%20-%20Using%20fhirclient%20to%20parse%20Healthcare%20Data.html#client-use",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/03%20-%20Using%20fhirclient%20to%20parse%20Healthcare%20Data.html#client-use"
  },"190": {
    "doc": "03 - Using fhirclient to parse Healthcare Data",
    "title": "Data Model Use",
    "content": "The client contains data model classes, built using fhir-parser, that handle (de)serialization and allow to work with FHIR data in a Pythonic way. Initialize Data Model . import fhirclient.models.patient import fhirclient.models.humanname data = {'id': 'patient-1'} patient = fhirclient.models.patient.Patient(data) print(patient.id) . patient-1 . name = fhirclient.models.humanname.HumanName() name.given = ['Peter'] name.family = 'Parker' patient.name = [name] pprint(patient.as_json()) . {'id': 'patient-1', 'name': [{'family': 'Parker', 'given': ['Peter']}], 'resourceType': 'Patient'} . name.given = 'Peter' patient.as_json() # throws FHIRValidationError: because we incorrectly set the name to a string . --------------------------------------------------------------------------- FHIRValidationError Traceback (most recent call last) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_93131/1295677727.py in &lt;module&gt; 1 name.given = 'Peter' ----&gt; 2 patient.as_json() # throws FHIRValidationError: because we incorrectly set the name to a string ~/opt/miniconda3/envs/cmu39/lib/python3.9/site-packages/fhirclient/models/fhirabstractresource.py in as_json(self) 40 41 def as_json(self): ---&gt; 42 js = super(FHIRAbstractResource, self).as_json() 43 js['resourceType'] = self.resource_type 44 return js ~/opt/miniconda3/envs/cmu39/lib/python3.9/site-packages/fhirclient/models/fhirabstractbase.py in as_json(self) 295 296 if len(errs) &gt; 0: --&gt; 297 raise FHIRValidationError(errs) 298 return js 299 FHIRValidationError: {root}: name.0: given: Expecting property \"given\" on &lt;class 'fhirclient.models.humanname.HumanName'&gt; to be list, but is &lt;class 'str'&gt; . Initialize from JSON file . import json import fhirclient.models.patient with open('patient.json', 'r') as h: pjs = json.load(h) patient = fhirclient.models.patient.Patient(pjs) print(patient.name[0].family) print(patient.name[0].given[0]) print(patient.gender) print(patient.birthDate.isostring) . Sky Luc male 1980-01-01 . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/03%20-%20Using%20fhirclient%20to%20parse%20Healthcare%20Data.html#data-model-use",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/03%20-%20Using%20fhirclient%20to%20parse%20Healthcare%20Data.html#data-model-use"
  },"191": {
    "doc": "03 - Using fhirclient to parse Healthcare Data",
    "title": "03 - Using fhirclient to parse Healthcare Data",
    "content": " ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/03%20-%20Using%20fhirclient%20to%20parse%20Healthcare%20Data.html",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/03%20-%20Using%20fhirclient%20to%20parse%20Healthcare%20Data.html"
  },"192": {
    "doc": "03 - Using requests to fetch Healthcare Data",
    "title": "Using Requests",
    "content": "import requests import json . response = requests.get(\"http://hapi.fhir.org/baseR4/Patient/2543713\") . response.status_code . 200 . data = json.loads(response.content.decode('utf-8')) data['name'] . [{'use': 'official', 'family': 'Chalmers', 'given': ['Peter', 'James']}] . data['name'][0]['family'] . 'Chalmers' . data['name'][0]['given'] . ['Peter', 'James'] . data['name'][0]['given'][0] . 'Peter' . response.text . '{\\n \"resourceType\": \"Patient\",\\n \"id\": \"2543713\",\\n \"meta\": {\\n \"versionId\": \"1\",\\n \"lastUpdated\": \"2021-09-06T03:21:51.345+00:00\",\\n \"source\": \"#tE3DOhkavDLHnkhZ\"\\n },\\n \"text\": {\\n \"status\": \"generated\",\\n \"div\": \"&lt;div xmlns=\\\\\"http://www.w3.org/1999/xhtml\\\\\"&gt;Some narrative&lt;/div&gt;\"\\n },\\n \"active\": true,\\n \"name\": [ {\\n \"use\": \"official\",\\n \"family\": \"Chalmers\",\\n \"given\": [ \"Peter\", \"James\" ]\\n } ],\\n \"gender\": \"male\",\\n \"birthDate\": \"1974-12-25\"\\n}' . response.json() . {'resourceType': 'Patient', 'id': '2543713', 'meta': {'versionId': '1', 'lastUpdated': '2021-09-06T03:21:51.345+00:00', 'source': '#tE3DOhkavDLHnkhZ'}, 'text': {'status': 'generated', 'div': '&lt;div xmlns=\"http://www.w3.org/1999/xhtml\"&gt;Some narrative&lt;/div&gt;'}, 'active': True, 'name': [{'use': 'official', 'family': 'Chalmers', 'given': ['Peter', 'James']}], 'gender': 'male', 'birthDate': '1974-12-25'} . ",
    "url": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/03%20-%20Using%20requests%20to%20fetch%20Healthcare%20Data.html#using-requests",
    "relUrl": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/03%20-%20Using%20requests%20to%20fetch%20Healthcare%20Data.html#using-requests"
  },"193": {
    "doc": "03 - Using requests to fetch Healthcare Data",
    "title": "03 - Using requests to fetch Healthcare Data",
    "content": " ",
    "url": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/03%20-%20Using%20requests%20to%20fetch%20Healthcare%20Data.html",
    "relUrl": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/03%20-%20Using%20requests%20to%20fetch%20Healthcare%20Data.html"
  },"194": {
    "doc": "03 - Your First Python Program",
    "title": "Your first Python Program",
    "content": "Now that you have everything setup, you’ll want to dive in and create your first program. Jetbrains has a great tutorial that describes how to Create and Running Your First Python Project. Once you’re comfortable creating programs you might want to learn how to Debug Your First Python Application, Test Your First Python Application, or even Creating and Running Your First Django Project. Django is a great way to build web applications in python, but debugging, testing, and creating web applications are not required components of this course. However, these are useful skills to pick up as you become more familiar with building solutions in python. ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/03%20-%20Your%20First%20Python%20Program.html#your-first-python-program",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/03%20-%20Your%20First%20Python%20Program.html#your-first-python-program"
  },"195": {
    "doc": "03 - Your First Python Program",
    "title": "03 - Your First Python Program",
    "content": " ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/03%20-%20Your%20First%20Python%20Program.html",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/03%20-%20Your%20First%20Python%20Program.html"
  },"196": {
    "doc": "03.a - NumPy Introduction",
    "title": "Python Numpy Tutorial",
    "content": "Source . A NumPy tutorial for beginners in which you’ll learn how to create a NumPy array, use broadcasting, access values, manipulate arrays, and much more. NumPy is, just like SciPy, Scikit-Learn, Pandas, etc. one of the packages that you just can’t miss when you’re learning data science, mainly because this library provides you with an array data structure that holds some benefits over Python lists, such as: being more compact, faster access in reading and writing items, being more convenient and more efficient. Today we’ll focus precisely on this. This NumPy tutorial will not only show you what NumPy arrays actually are and how you can install Python, but you’ll also learn how to make arrays (even when your data comes from files!), how broadcasting works, how you can ask for help, how to manipulate your arrays and how to visualize them. ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#python-numpy-tutorial",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#python-numpy-tutorial"
  },"197": {
    "doc": "03.a - NumPy Introduction",
    "title": "Content",
    "content": ". | What Is A Python Numpy Array? | How To Make NumPy Arrays | How NumPy Broadcasting Works | How Do Array Mathematics Work? | How To Subset, Slice, And Index Arrays | How To Manipulate Arrays | How To Visualize NumPy Arrays | Beyond Data Analysis with NumPy | . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#content",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#content"
  },"198": {
    "doc": "03.a - NumPy Introduction",
    "title": "What Is A Python Numpy Array?",
    "content": "You already read in the introduction that NumPy arrays are a bit like Python lists, but still very much different at the same time. For those of you who are new to the topic, let’s clarify what it exactly is and what it’s good for. As the name gives away, a NumPy array is a central data structure of the numpy library. The library’s name is short for “Numeric Python” or “Numerical Python”. This already gives an idea of what you’re dealing with, right? . In other words, NumPy is a Python library that is the core library for scientific computing in Python. It contains a collection of tools and techniques that can be used to solve on a computer mathematical models of problems in Science and Engineering. One of these tools is a high-performance multidimensional array object that is a powerful data structure for efficient computation of arrays and matrices. To work with these arrays, there’s a vast amount of high-level mathematical functions operate on these matrices and arrays. Then, what is an array? . When you look at the print of a couple of arrays, you could see it as a grid that contains values of the same type: . # import the library import numpy as np . # create a 1-dimensional array my_array = np.array([1, 2, 3, 4, 5, 6, 7, 8 ,9, 10, 11, 12]) print(type(my_array), my_array) . &lt;class 'numpy.ndarray'&gt; [ 1 2 3 4 5 6 7 8 9 10 11 12] . # create a 2-dimensional array my_2d_array = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]) print(my_2d_array) . [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] . # create a 3-dimensional array my_3d_array = np.array([[[1, 2, 3, 4], [5, 6, 7, 8]], [[9, 10, 11, 12], [13, 14, 15, 16]]]) print(my_3d_array) . [[[ 1 2 3 4] [ 5 6 7 8]] [[ 9 10 11 12] [13 14 15 16]]] . You see that, in the example above, the data are integers. The array holds and represents any regular data in a structured way. However, you should know that, on a structural level, an array is basically nothing but pointers. It’s a combination of a memory address, a data type, a shape, and strides: . | The data pointer indicates the memory address of the first byte in the array, | The data type or dtype pointer describes the kind of elements that are contained within the array, | The shape indicates the shape of the array, and | The strides are the number of bytes that should be skipped in memory to go to the next element. If your strides are (10,1), you need to proceed one byte to get to the next column and 10 bytes to locate the next row. | . Or, in other words, an array contains information about the raw data, how to locate an element and how to interpret an element. You can easily test this by exploring the numpy array attributes: . # Print out memory address print('Memory Address', my_2d_array.data) # Print out the shape of `my_array` print('Shape', my_2d_array.shape) # Print out the data type of `my_array` print('Data Type', my_2d_array.dtype) # Print out the stride of `my_array` print('Strides', my_2d_array.strides) . Memory Address &lt;memory at 0x7faf50c48040&gt; Shape (3, 4) Data Type int64 Strides (32, 8) . You see that now, you get a lot more information: for example, the data type that is printed out is ‘int64’ or signed 32-bit integer type; This is a lot more detailed! That also means that the array is stored in memory as 64 bytes (as each integer takes up 8 bytes and you have an array of 8 integers). The strides of the array tell us that you have to skip 8 bytes (one value) to move to the next column, but 32 bytes (4 values) to get to the same position in the next row. As such, the strides for the array will be (32,8). Note that if you set the data type to int32, the strides tuple that you get back will be (16, 4), as you will still need to move one value to the next column and 4 values to get the same position. The only thing that will have changed is the fact that each integer will take up 4 bytes instead of 8. The array that you see above is, as its name already suggested, a 2-dimensional array: you have rows and columns. The rows are indicated as the “axis 0”, while the columns are the “axis 1”. The number of the axis goes up accordingly with the number of the dimensions: in 3-D arrays, of which you have also seen an example in the previous code chunk, you’ll have an additional “axis 2”. Note that these axes are only valid for arrays that have at least 2 dimensions, as there is no point in having this for 1-D arrays; . These axes will come in handy later when you’re manipulating the shape of your NumPy arrays. ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#what-is-a-python-numpy-array",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#what-is-a-python-numpy-array"
  },"199": {
    "doc": "03.a - NumPy Introduction",
    "title": "How To Make NumPy Arrays",
    "content": "To make a numpy array, you can just use the np.array() function. All you need to do is pass a list to it, and optionally, you can also specify the data type of the data. If you want to know more about the possible data types that you can pick, go here or consider taking a brief look at DataCamp’s NumPy cheat sheet. There’s no need to go and memorize these NumPy data types if you’re a new user; But you do have to know and care what data you’re dealing with. The data types are there when you need more control over how your data is stored in memory and on disk. Especially in cases where you’re working with extensive data, it’s good that you know to control the storage type. Don’t forget that, in order to work with the np.array() function, you need to make sure that the numpy library is present in your environment. The NumPy library follows an import convention: when you import this library, you have to make sure that you import it as np. By doing this, you’ll make sure that other Pythonistas understand your code more easily. In the following example you’ll create the my_array array that you have already played around with above: . # Import `numpy` as `np` import numpy as np # Make the array `my_array` my_array = np.array([[1,2,3,4], [5,6,7,8]], dtype=np.int32) # Print `my_array` print(my_array, my_array.dtype) . [[1 2 3 4] [5 6 7 8]] int32 . However, sometimes you don’t know what data you want to put in your array, or you want to import data into a numpy array from another source. In those cases, you’ll make use of initial placeholders or functions to load data from text into arrays, respectively. The following sections will show you how to do this. How To Make An “Empty” NumPy Array . What people often mean when they say that they are creating “empty” arrays is that they want to make use of initial placeholders, which you can fill up afterward. You can initialize arrays with ones or zeros, but you can also create arrays that get filled up with evenly spaced values, constant or random values. However, you can still make a totally empty array, too. Luckily for us, there are quite a lot of functions to make . Try it all out below! . # Create an array of ones np.ones((3, 4)) . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) . # Create an array of zeros np.zeros((2, 3, 4), dtype=np.int16) . array([[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]], dtype=int16) . # Create an array with random values np.random.random((2, 2)) . array([[0.30514961, 0.53408847], [0.5699706 , 0.14205311]]) . # Create an empty array np.empty((3, 2)) . array([[1.72723371e-077, 1.72723371e-077], [1.38338381e-322, 0.00000000e+000], [0.00000000e+000, 0.00000000e+000]]) . # Create a full array np.full((2, 2), 6) . array([[6, 6], [6, 6]]) . # Create an array of evenly-spaced values print(np.arange(10, 31, 5)) . [10 15 20 25 30] . # Create an array of evenly-spaced values np.linspace(0, 2, 9) . array([0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) . | For some, such as np.ones(), np.random.random(), np.empty(), np.full() or np.zeros() the only thing that you need to do in order to make arrays with ones or zeros is pass the shape of the array that you want to make. As an option to np.ones() and np.zeros(), you can also specify the data type. In the case of np.full(), you also have to specify the constant value that you want to insert into the array. | With np.linspace() and np.arange() you can make arrays of evenly spaced values. The difference between these two functions is that the last value of the three that are passed in the code chunk above designates either the step value for np.linspace() or a number of samples for np.arange(). What happens in the first is that you want, for example, an array of 9 values that lie between 0 and 2. For the latter, you specify that you want an array to start at 10 and per steps of 5, generate values for the array that you’re creating. | Remember that NumPy also allows you to create an identity array or matrix with np.eye() and np.identity(). An identity matrix is a square matrix of which all elements in the principal diagonal are ones, and all other elements are zeros. When you multiply a matrix with an identity matrix, the given matrix is left unchanged. | . How To Load NumPy Arrays From Text . Creating arrays with the help of initial placeholders or with some example data is an excellent way of getting started with numpy. But when you want to get started with data analysis, you’ll need to load data from text files. With that what you have seen up until now, you won’t really be able to do much. Make use of some specific functions to load data from your files, such as loadtxt() or genfromtxt(). Let’s say you have the following text files with data: . # Import your data x, y, z = np.loadtxt('data.txt', skiprows=1, unpack=True) print(x) print(y) print(z) . [0.2536 0.4839 0.1292 0.1781 0.6253 0.6253] [0.1008 0.4536 0.6875 0.3049 0.3486 0.3486] [0.3857 0.3561 0.5929 0.8928 0.8791 0.8791] . In the code above, you use loadtxt() to load the data in your environment. You see that the first argument that both functions take is the text file data.txt. Next, there are some specific arguments for each: in the first statement, you skip the first row, and you return the columns as separate arrays with unpack=True. This means that the values in column Value1 will be put in x, and so on. Note that, in case you have comma-delimited data or if you want to specify the data type, there are also the arguments delimiter and dtype that you can add to the loadtxt() arguments. my_array2 = np.genfromtxt('data2.txt', skip_header=1, filling_values=-999) print(my_array2) . [[ 4.839e-01 4.536e-01 3.561e-01] [ 1.292e-01 6.875e-01 -9.990e+02] [ 1.781e-01 3.049e-01 8.928e-01] [-9.990e+02 5.801e-01 2.038e-01] [ 5.993e-01 4.357e-01 7.410e-01]] . You see that here, you resort to genfromtxt() to load the data. In this case, you have to handle some missing values that are indicated by the 'MISSING' strings. Since the genfromtxt() function converts character strings in numeric columns to nan, you can convert these values to other ones by specifying the filling_values argument. In this case, you choose to set the value of these missing values to -999. If by any chance, you have values that don’t get converted to nan by genfromtxt(), there’s always the missing_values argument that allows you to specify what the missing values of your data exactly are. But this is not all. Tip: check out this page to see what other arguments you can add to import your data successfully. You now might wonder what the difference between these two functions really is. The examples indicated this maybe implicitly, but, in general, genfromtxt() gives you a little bit more flexibility; It’s more robust than loadtxt(). Let’s make this difference a little bit more practical: the latter, loadtxt(), only works when each row in the text file has the same number of values; So when you want to handle missing values easily, you’ll typically find it easier to use genfromtxt(). But this is definitely not the only reason. A brief look on the number of arguments that genfromtxt() has to offer will teach you that there is really a lot more things that you can specify in your import, such as the maximum number of rows to read or the option to automatically strip white spaces from variables. How To Save NumPy Arrays . Once you have done everything that you need to do with your arrays, you can also save them to a file. If you want to save the array to a text file, you can use the savetxt() function to do this: . x = np.arange(0.0, 5.0, 1.0) print(x) np.savetxt('test.out', x, delimiter=',') . [0. 1. 2. 3. 4.] . Remember that np.arange() creates a NumPy array of evenly-spaced values. The third value that you pass to this function is the step value. There are, of course, other ways to save your NumPy arrays to text files. Check out the functions in the table below if you want to get your data to binary files or archives: . | Method | Description | . | save() | Save an array to a binary file in NumPy .npy format | . | savez() | Save several arrays into an uncompressed .npz archive | . | savez_compressed() | Save several arrays into a compressed .npz archive | . How To Inspect Your NumPy Arrays . Besides the array attributes that have been mentioned above, namely, data, shape, dtype and strides, there are some more that you can use to easily get to know more about your arrays. The ones that you might find interesting to use when you’re just starting out are the following: . # Print the number of `my_array`'s dimensions print(my_array.ndim) . 2 . # Print the number of `my_array`'s elements print(my_array.size) . 8 . # Print information about `my_array`'s memory layout print(my_array.flags) . C_CONTIGUOUS : True F_CONTIGUOUS : False OWNDATA : True WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . # Print the length of one array element in bytes print(my_array.itemsize) . 4 . # Print the total consumed bytes by `my_array`'s elements print(my_array.nbytes) . 32 . Also note that, besides the attributes, you also have some other ways of gaining more information on and even tweaking your array slightly: . # Print the length of `my_array` print(len(my_array)) . 2 . # Change the data type of `my_array` my_float_array = my_array.astype(float) print(my_float_array) . [[1. 2. 3. 4.] [5. 6. 7. 8.]] . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#how-to-make-numpy-arrays",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#how-to-make-numpy-arrays"
  },"200": {
    "doc": "03.a - NumPy Introduction",
    "title": "How NumPy Broadcasting Works",
    "content": "Before you go deeper into scientific computing, it might be a good idea to first go over what broadcasting exactly is: it’s a mechanism that allows NumPy to work with arrays of different shapes when you’re performing arithmetic operations. To put it in a more practical context, you often have an array that’s somewhat larger and another one that’s slightly smaller. Ideally, you want to use the smaller array multiple times to perform an operation (such as a sum, multiplication, etc.) on the larger array. To do this, you use the broadcasting mechanism. However, there are some rules if you want to use it. And, before you already sigh, you’ll see that these “rules” are very simple and kind of straightforward! . | First off, to make sure that the broadcasting is successful, the dimensions of your arrays need to be compatible. Two dimensions are compatible when they are equal. Consider the following example: | . # Initialize `x` x = np.ones((3, 4)) # Check shape of `x` print(x.shape) print(x) # Initialize `y` y = np.random.random((3, 4)) # Check shape of `y` print(y.shape) print(y) # Add `x` and `y` x + y . (3, 4) [[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]] (3, 4) [[0.56519198 0.79797203 0.29302353 0.42378585] [0.20190445 0.37107571 0.50090031 0.36828103] [0.48683869 0.81527133 0.01208389 0.50830169]] array([[1.56519198, 1.79797203, 1.29302353, 1.42378585], [1.20190445, 1.37107571, 1.50090031, 1.36828103], [1.48683869, 1.81527133, 1.01208389, 1.50830169]]) . | Two dimensions are also compatible when one of them is 1: | . # Initialize `x` x = np.ones((3, 4)) # Check shape of `x` print(x.shape) print(x) # Initialize `y` y = np.arange(4) # Check shape of `y` print(y.shape) print(y) # Subtract `x` and `y` x - y . (3, 4) [[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]] (4,) [0 1 2 3] array([[ 1., 0., -1., -2.], [ 1., 0., -1., -2.], [ 1., 0., -1., -2.]]) . Note that if the dimensions are not compatible, you will get a ValueError. Tip: also test what the size of the resulting array is after you have done the computations! You’ll see that the size is actually the maximum size along each dimension of the input arrays. In other words, you see that the result of x-y gives an array with shape (3,4): y had a shape of (4,) and x had a shape of (3,4). The maximum size along each dimension of x and y is taken to make up the shape of the new, resulting array. Lastly, the arrays can only be broadcast together if they are compatible in all dimensions. Consider the following example: . # Initialize `x` and `y` x = np.ones((3, 4)) y = np.random.random((5,1,4)) # Add `x` and `y` x + y . array([[[1.83525793, 1.84535921, 1.23908934, 1.51636293], [1.83525793, 1.84535921, 1.23908934, 1.51636293], [1.83525793, 1.84535921, 1.23908934, 1.51636293]], [[1.96363415, 1.53040604, 1.28514896, 1.55804427], [1.96363415, 1.53040604, 1.28514896, 1.55804427], [1.96363415, 1.53040604, 1.28514896, 1.55804427]], [[1.40773584, 1.34067363, 1.26809999, 1.42783587], [1.40773584, 1.34067363, 1.26809999, 1.42783587], [1.40773584, 1.34067363, 1.26809999, 1.42783587]], [[1.24076123, 1.67608402, 1.70843067, 1.96111856], [1.24076123, 1.67608402, 1.70843067, 1.96111856], [1.24076123, 1.67608402, 1.70843067, 1.96111856]], [[1.46894321, 1.37966024, 1.56355702, 1.29747049], [1.46894321, 1.37966024, 1.56355702, 1.29747049], [1.46894321, 1.37966024, 1.56355702, 1.29747049]]]) . You see that, even though x and y seem to have somewhat different dimensions, the two can be added together. That is because they are compatible in all dimensions: . | Array x has dimensions 3 X 4, | Array y has dimensions 5 X 1 X 4 | . Since you have seen above that dimensions are also compatible if one of them is equal to 1, you see that these two arrays are indeed a good candidate for broadcasting! . What you will notice is that in the dimension where y has size 1, and the other array has a size greater than 1 (that is, 3), the first array behaves as if it were copied along that dimension. Note that the shape of the resulting array will again be the maximum size along each dimension of x and y: the dimension of the result will be (5,3,4) . In short, if you want to make use of broadcasting, you will rely a lot on the shape and dimensions of the arrays with which you’re working. But what if the dimensions are not compatible? . What if they are not equal or if one of them is not equal to 1? . You’ll have to fix this by manipulating your array! You’ll see how to do this in one of the next sections. ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#how-numpy-broadcasting-works",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#how-numpy-broadcasting-works"
  },"201": {
    "doc": "03.a - NumPy Introduction",
    "title": "How Do Array Mathematics Work?",
    "content": "You’ve seen that broadcasting is handy when you’re doing arithmetic operations. In this section, you’ll discover some of the functions that you can use to do mathematics with arrays. As such, it probably won’t surprise you that you can just use +, -, *, / or % to add, subtract, multiply, divide or calculate the remainder of two (or more) arrays. However, a big part of why NumPy is so handy, is because it also has functions to do this. The equivalent functions of the operations that you have seen just now are, respectively, np.add(), np.subtract(), np.multiply(), np.divide() and np.remainder(). You can also easily do exponentiation and taking the square root of your arrays with np.exp() and np.sqrt(), or calculate the sines or cosines of your array with np.sin() and np.cos(). Lastly, its’ also useful to mention that there’s also a way for you to calculate the natural logarithm with np.log() or calculate the dot product by applying the dot() to your array. # Add `x` and `y` np.add(x, y) . array([[[1.83525793, 1.84535921, 1.23908934, 1.51636293], [1.83525793, 1.84535921, 1.23908934, 1.51636293], [1.83525793, 1.84535921, 1.23908934, 1.51636293]], [[1.96363415, 1.53040604, 1.28514896, 1.55804427], [1.96363415, 1.53040604, 1.28514896, 1.55804427], [1.96363415, 1.53040604, 1.28514896, 1.55804427]], [[1.40773584, 1.34067363, 1.26809999, 1.42783587], [1.40773584, 1.34067363, 1.26809999, 1.42783587], [1.40773584, 1.34067363, 1.26809999, 1.42783587]], [[1.24076123, 1.67608402, 1.70843067, 1.96111856], [1.24076123, 1.67608402, 1.70843067, 1.96111856], [1.24076123, 1.67608402, 1.70843067, 1.96111856]], [[1.46894321, 1.37966024, 1.56355702, 1.29747049], [1.46894321, 1.37966024, 1.56355702, 1.29747049], [1.46894321, 1.37966024, 1.56355702, 1.29747049]]]) . # Subtract `x` and `y` np.subtract(x, y) . array([[[0.16474207, 0.15464079, 0.76091066, 0.48363707], [0.16474207, 0.15464079, 0.76091066, 0.48363707], [0.16474207, 0.15464079, 0.76091066, 0.48363707]], [[0.03636585, 0.46959396, 0.71485104, 0.44195573], [0.03636585, 0.46959396, 0.71485104, 0.44195573], [0.03636585, 0.46959396, 0.71485104, 0.44195573]], [[0.59226416, 0.65932637, 0.73190001, 0.57216413], [0.59226416, 0.65932637, 0.73190001, 0.57216413], [0.59226416, 0.65932637, 0.73190001, 0.57216413]], [[0.75923877, 0.32391598, 0.29156933, 0.03888144], [0.75923877, 0.32391598, 0.29156933, 0.03888144], [0.75923877, 0.32391598, 0.29156933, 0.03888144]], [[0.53105679, 0.62033976, 0.43644298, 0.70252951], [0.53105679, 0.62033976, 0.43644298, 0.70252951], [0.53105679, 0.62033976, 0.43644298, 0.70252951]]]) . # Multiply `x` and `y` np.multiply(x, y) . array([[[0.83525793, 0.84535921, 0.23908934, 0.51636293], [0.83525793, 0.84535921, 0.23908934, 0.51636293], [0.83525793, 0.84535921, 0.23908934, 0.51636293]], [[0.96363415, 0.53040604, 0.28514896, 0.55804427], [0.96363415, 0.53040604, 0.28514896, 0.55804427], [0.96363415, 0.53040604, 0.28514896, 0.55804427]], [[0.40773584, 0.34067363, 0.26809999, 0.42783587], [0.40773584, 0.34067363, 0.26809999, 0.42783587], [0.40773584, 0.34067363, 0.26809999, 0.42783587]], [[0.24076123, 0.67608402, 0.70843067, 0.96111856], [0.24076123, 0.67608402, 0.70843067, 0.96111856], [0.24076123, 0.67608402, 0.70843067, 0.96111856]], [[0.46894321, 0.37966024, 0.56355702, 0.29747049], [0.46894321, 0.37966024, 0.56355702, 0.29747049], [0.46894321, 0.37966024, 0.56355702, 0.29747049]]]) . # Divide `x` and `y` np.divide(x, y) . array([[[1.19723497, 1.18292908, 4.18253694, 1.93662236], [1.19723497, 1.18292908, 4.18253694, 1.93662236], [1.19723497, 1.18292908, 4.18253694, 1.93662236]], [[1.03773824, 1.88534807, 3.50693892, 1.79197254], [1.03773824, 1.88534807, 3.50693892, 1.79197254], [1.03773824, 1.88534807, 3.50693892, 1.79197254]], [[2.45256831, 2.93536074, 3.72995169, 2.33734491], [2.45256831, 2.93536074, 3.72995169, 2.33734491], [2.45256831, 2.93536074, 3.72995169, 2.33734491]], [[4.15349266, 1.47910611, 1.41157073, 1.04045436], [4.15349266, 1.47910611, 1.41157073, 1.04045436], [4.15349266, 1.47910611, 1.41157073, 1.04045436]], [[2.13245436, 2.63393394, 1.77444335, 3.36167802], [2.13245436, 2.63393394, 1.77444335, 3.36167802], [2.13245436, 2.63393394, 1.77444335, 3.36167802]]]) . # Calculate the remainder of `x` and `y` np.remainder(x, y) . array([[[0.16474207, 0.15464079, 0.04364264, 0.48363707], [0.16474207, 0.15464079, 0.04364264, 0.48363707], [0.16474207, 0.15464079, 0.04364264, 0.48363707]], [[0.03636585, 0.46959396, 0.14455311, 0.44195573], [0.03636585, 0.46959396, 0.14455311, 0.44195573], [0.03636585, 0.46959396, 0.14455311, 0.44195573]], [[0.18452832, 0.31865274, 0.19570004, 0.14432825], [0.18452832, 0.31865274, 0.19570004, 0.14432825], [0.18452832, 0.31865274, 0.19570004, 0.14432825]], [[0.03695508, 0.32391598, 0.29156933, 0.03888144], [0.03695508, 0.32391598, 0.29156933, 0.03888144], [0.03695508, 0.32391598, 0.29156933, 0.03888144]], [[0.06211358, 0.24067951, 0.43644298, 0.10758854], [0.06211358, 0.24067951, 0.43644298, 0.10758854], [0.06211358, 0.24067951, 0.43644298, 0.10758854]]]) . Remember how broadcasting works? Check out the dimensions and the shapes of both x and y in your IPython shell. Are the rules of broadcasting respected? . But there is more. Check out this small list of aggregate functions: . | Function | Description | . | a.sum() | Array-wise sum | . | a.min() | Array-wise minimum value | . | b.max(axis=0) | Maximum value of an array row | . | b.cumsum(axis=1) | Cumulative sum of the elements | . | a.mean() | Mean | . | b.median() | Median | . | a.corrcoef() | Correlation coefficient | . | np.std(b) | Standard deviation | . Besides all of these functions, you might also find it useful to know that there are mechanisms that allow you to compare array elements. For example, if you want to check whether the elements of two arrays are the same, you might use the == operator. To check whether the array elements are smaller or bigger, you use the &lt; or &gt; operators. This all seems quite straightforward, yes? . However, you can also compare entire arrays with each other! In this case, you use the np.array_equal() function. Just pass in the two arrays that you want to compare with each other, and you’re done. Note that, besides comparing, you can also perform logical operations on your arrays. You can start with np.logical_or(), np.logical_not() and np.logical_and(). This basically works like your typical OR, NOT and AND logical operations; . In the simplest example, you use OR to see whether your elements are the same (for example, 1), or if one of the two array elements is 1. If both of them are 0, you’ll return FALSE. You would use AND to see whether your second element is also 1 and NOT to see if the second element differs from 1. # `x` AND `y` np.logical_and(x, y) . array([[[ True, True, True, True], [ True, True, True, True], [ True, True, True, True]], [[ True, True, True, True], [ True, True, True, True], [ True, True, True, True]], [[ True, True, True, True], [ True, True, True, True], [ True, True, True, True]], [[ True, True, True, True], [ True, True, True, True], [ True, True, True, True]], [[ True, True, True, True], [ True, True, True, True], [ True, True, True, True]]]) . # `x` OR `y` np.logical_or(x, y) . array([[[ True, True, True, True], [ True, True, True, True], [ True, True, True, True]], [[ True, True, True, True], [ True, True, True, True], [ True, True, True, True]], [[ True, True, True, True], [ True, True, True, True], [ True, True, True, True]], [[ True, True, True, True], [ True, True, True, True], [ True, True, True, True]], [[ True, True, True, True], [ True, True, True, True], [ True, True, True, True]]]) . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#how-do-array-mathematics-work",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#how-do-array-mathematics-work"
  },"202": {
    "doc": "03.a - NumPy Introduction",
    "title": "How To Subset, Slice, And Index Arrays",
    "content": "Besides mathematical operations, you might also consider taking just a part of the original array (or the resulting array) or just some array elements to use in further analysis or other operations. In such case, you will need to subset, slice and/or index your arrays. These operations are very similar to when you perform them on Python lists. If you want to check out the similarities for yourself, or if you want a more elaborate explanation, you might consider checking out DataCamp’s Python list tutorial. If you have no clue at all on how these operations work, it suffices for now to know these two basic things: . | You use square brackets [] as the index operator, and | Generally, you pass integers to these square brackets, but you can also put a colon : or a combination of the colon with integers in it to designate the elements/rows/columns you want to select. | . Besides from these two points, the easiest way to see how this all fits together is by looking at some examples of subsetting: . # Select the element at the 1st index print(my_array[1]) . [5 6 7 8] . # Select the element at row 1 column 2 print(my_2d_array[1][2]) . 7 . # Select the element at row 1 column 2 print(my_2d_array[1,2]) . 7 . # Select the element at row 1, column 2 and print(my_3d_array[1,1,2]) . 15 . Something a little bit more advanced than subsetting, if you will, is slicing. Here, you consider not just particular values of your arrays, but you go to the level of rows and columns. You’re basically working with “regions” of data instead of pure “locations”. # Select items at index 0 and 1 print(my_array[0:2]) . [[1 2 3 4] [5 6 7 8]] . # Select items at row 0 and 1, column 1 print(my_2d_array[0:2,1]) . [2 6] . # Select items at row 1 # This is the same as saying `my_3d_array[1,:,:] print(my_3d_array[1,...]) . [[ 9 10 11 12] [13 14 15 16]] . Lastly, there’s also indexing. When it comes to NumPy, there are boolean indexing and advanced or “fancy” indexing. First up is boolean indexing. Here, instead of selecting elements, rows or columns based on index number, you select those values from your array that fulfill a certain condition. # Try out a simple example mask = my_array &lt; 2 print(mask) print() print(my_array[mask]) . [[ True False False False] [False False False False]] [1] . print(my_array[my_array &lt; 2]) . [1] . # Try out a simple example mask = my_array &gt; 3 print(mask) print(my_array[mask]) . [[False False False True] [ True True True True]] [4 5 6 7 8] . # Specify a condition bigger_than_3 = (my_3d_array &gt;= 3) # Use the condition to index our 3d array print(my_3d_array[bigger_than_3]) . [ 3 4 5 6 7 8 9 10 11 12 13 14 15 16] . # Specify a condition mask = (my_3d_array &gt;= 3) &amp; (my_3d_array &lt; 10) # Use the condition to index our 3d array print(my_3d_array[mask]) . [3 4 5 6 7 8 9] . Note that, to specify a condition, you can also make use of the logical operators | (OR) and &amp; (AND). If you would want to rewrite the condition above in such a way (which would be inefficient, but I demonstrate it here for educational purposes :)), you would get bigger_than_3 = (my_3d_array &gt; 3) | (my_3d_array == 3). With the arrays that have been loaded in, there aren’t too many possibilities, but with arrays that contain for example, names or capitals, the possibilities could be endless! . When it comes to fancy indexing, that what you basically do with it is the following: you pass a list or an array of integers to specify the order of the subset of rows you want to select out of the original array. # Select elements at (1,0), (0,1), (1,2) and (0,0) print(my_2d_array[[1, 0, 1, 0],[0, 1, 2, 0]]) . [5 2 7 1] . # Select a subset of the rows and columns print(my_2d_array[[1, 0, 1, 0]][:,[0,1,2,0]]) . [[5 6 7 5] [1 2 3 1] [5 6 7 5] [1 2 3 1]] . Now, the second statement might seem to make less sense to you at first sight. This is normal. It might make more sense if you break it down: . | If you just execute my_2d_array[[1,0,1,0]], the result is the following: | . my_2d_array[[1,0,1,0]] . array([[5, 6, 7, 8], [1, 2, 3, 4], [5, 6, 7, 8], [1, 2, 3, 4]]) . | What the second part, namely, [:,[0,1,2,0]], is tell you that you want to keep all the rows of this result, but that you want to change the order of the columns around a bit. You want to display the columns 0, 1, and 2 as they are right now, but you want to repeat column 0 as the last column instead of displaying column number 3. This will give you the following result: | . my_2d_array[:,[0,1,2,0]] . array([[ 1, 2, 3, 1], [ 5, 6, 7, 5], [ 9, 10, 11, 9]]) . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#how-to-subset-slice-and-index-arrays",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#how-to-subset-slice-and-index-arrays"
  },"203": {
    "doc": "03.a - NumPy Introduction",
    "title": "How To Manipulate Arrays",
    "content": "Performing mathematical operations on your arrays is one of the things that you’ll be doing, but probably most importantly to make this and the broadcasting work is to know how to manipulate your arrays. Below are some of the most common manipulations that you’ll be doing. How To Transpose Your Arrays . What transposing your arrays actually does is permuting the dimensions of it. Or, in other words, you switch around the shape of the array. Let’s take a small example to show you the effect of transposition: . # Print `my_2d_array` print(my_2d_array) . [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] . # Transpose `my_2d_array` print(np.transpose(my_2d_array)) . [[ 1 5 9] [ 2 6 10] [ 3 7 11] [ 4 8 12]] . # Or use `T` to transpose `my_2d_array` print(my_2d_array.T) . [[ 1 5 9] [ 2 6 10] [ 3 7 11] [ 4 8 12]] . Reshaping Versus Resizing Your Arrays . You might have read in the broadcasting section that the dimensions of your arrays need to be compatible if you want them to be good candidates for arithmetic operations. But the question of what you should do when that is not the case, was not answered yet. Well, this is where you get the answer! . What you can do if the arrays don’t have the same dimensions, is resize your array. You will then return a new array that has the shape that you passed to the np.resize() function. If you pass your original array together with the new dimensions, and if that new array is larger than the one that you originally had, the new array will be filled with copies of the original array that are repeated as many times as is needed. However, if you just apply np.resize() to the array and you pass the new shape to it, the new array will be filled with zeros. # Print the shape of `x` print(x.shape) print(x) . (3, 4) [[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]] . # Resize `x` to ((6,4)) np.resize(x, (4, 3)) . array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) . np.resize(x, (3, 4)) . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) . Besides resizing, you can also reshape your array. This means that you give a new shape to an array without changing its data. The key to reshaping is to make sure that the total size of the new array is unchanged. If you take the example of array x that was used above, which has a size of 3 X 4 or 12, you have to make sure that the new array also has a size of 12. If you want to calculate the size of an array with code, make sure to use the size attribute: x.size or x.reshape((2,6)).size: . # Print the size of `x` to see what's possible print(x.size) . 12 . # Flatten `x` z = x.ravel() # Print `z` print(z) . [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] . If all else fails, you can also append an array to your original one or insert or delete array elements to make sure that your dimensions fit with the other array that you want to use for your computations. Another operation that you might keep handy when you’re changing the shape of arrays is ravel(). This function allows you to flatten your arrays. This means that if you ever have 2D, 3D or n-D arrays, you can just use this function to flatten it all out to a 1-D array. How To Append Arrays . When you append arrays to your original array, they are “glued” to the end of that original array. If you want to make sure that what you append does not come at the end of the array, you might consider inserting it. Go to the next section if you want to know more. Appending is a pretty easy thing to do thanks to the NumPy library; You can just make use of the np.append(). # Append a 1D array to your `my_array` new_array = np.append(my_array, [7, 8, 9, 10, 11, 12]) # Print `new_array` print(my_array) print(new_array) . [[1 2 3 4] [5 6 7 8]] [ 1 2 3 4 5 6 7 8 7 8 9 10 11 12] . # Append an extra column to your `my_2d_array` new_2d_array = np.append(my_2d_array, [[7], [8], [9]], axis=1) # Print `new_2d_array` print(new_2d_array) . [[ 1 2 3 4 7] [ 5 6 7 8 8] [ 9 10 11 12 9]] . Note how, when you append an extra column to my_2d_array, the axis is specified. Remember that axis 1 indicates the columns, while axis 0 indicates the rows in 2-D arrays. How To Insert And Delete Array Elements . Next to appending, you can also insert and delete array elements. As you might have guessed by now, the functions that will allow you to do these operations are np.insert() and np.delete(): . # Insert `5` at index 1 np.insert(my_array, 1, 5) . array([1, 5, 2, 3, 4, 5, 6, 7, 8], dtype=int32) . # Delete the value at index 1 # np.delete(my_array,[1]) . How To Join And Split Arrays . You can also ‘merge’ or join your arrays. There are a bunch of functions that you can use for that purpose and most of them are listed below. Try them out, but also make sure to test out what the shape of the arrays is in the IPython shell. The arrays that have been loaded are x, my_array, my_resized_array and my_2d_array. # Concatentate `my_array` and `x` print(np.concatenate((my_array, x))) . [[1. 2. 3. 4.] [5. 6. 7. 8.] [1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]] . # Stack arrays row-wise print(np.vstack((my_array, my_2d_array))) . [[ 1 2 3 4] [ 5 6 7 8] [ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] . my_resized_array = np.array([[91, 92, 93, 94], [91, 92, 93, 94], [91, 92, 93, 94]]) print(my_resized_array) . [[91 92 93 94] [91 92 93 94] [91 92 93 94]] . # Stack arrays row-wise print(np.r_[my_resized_array, my_2d_array]) . [[91 92 93 94] [91 92 93 94] [91 92 93 94] [ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] . # Stack arrays horizontally print(np.hstack((my_resized_array, my_2d_array))) . [[91 92 93 94 1 2 3 4] [91 92 93 94 5 6 7 8] [91 92 93 94 9 10 11 12]] . # Stack arrays column-wise print(np.column_stack((my_resized_array, my_2d_array))) . [[91 92 93 94 1 2 3 4] [91 92 93 94 5 6 7 8] [91 92 93 94 9 10 11 12]] . # Stack arrays column-wise print(np.c_[my_resized_array, my_2d_array]) . [[91 92 93 94 1 2 3 4] [91 92 93 94 5 6 7 8] [91 92 93 94 9 10 11 12]] . You’ll note a few things as you go through the functions: . | The number of dimensions needs to be the same if you want to concatenate two arrays with np.concatenate(). As such, if you want to concatenate an array with my_array, which is 1-D, you’ll need to make sure that the second array that you have, is also 1-D. | With np.vstack(), you effortlessly combine my_array with my_2d_array. You just have to make sure that, as you’re stacking the arrays row-wise, that the number of columns in both arrays is the same. As such, you could also add an array with shape (2,4) or (3,4) to my_2d_array, as long as the number of columns matches. Stated differently, the arrays must have the same shape along all but the first axis. The same holds also for when you want to use np.r[]. | For np.hstack(), you have to make sure that the number of dimensions is the same and that the number of rows in both arrays is the same. That means that you could stack arrays such as (2,3) or (2,4) to my_2d_array, which itself as a shape of (2,4). Anything is possible as long as you make sure that the number of rows matches. This function is still supported by NumPy, but you should prefer np.concatenate() or np.stack(). | With np.column_stack(), you have to make sure that the arrays that you input have the same first dimension. In this case, both shapes are the same, but if my_resized_array were to be (2,1) or (2,), the arrays still would have been stacked. | np.c_[] is another way to concatenate. Here also, the first dimension of both arrays needs to match. | . When you have joined arrays, you might also want to split them at some point. Just like you can stack them horizontally, you can also do the same but then vertically. You use np.hsplit() and np.vsplit(), respectively: . my_stacked_array = np.r_[my_resized_array, my_2d_array] . # Split `my_stacked_array` horizontally at the 2nd index print(np.hsplit(my_stacked_array, 2)) . [array([[91, 92], [91, 92], [91, 92], [ 1, 2], [ 5, 6], [ 9, 10]]), array([[93, 94], [93, 94], [93, 94], [ 3, 4], [ 7, 8], [11, 12]])] . # Split `my_stacked_array` vertically at the 2nd index print(np.vsplit(my_stacked_array, 2)) . [array([[91, 92, 93, 94], [91, 92, 93, 94], [91, 92, 93, 94]]), array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]])] . What you need to keep in mind when you’re using both of these split functions is probably the shape of your array. Let’s take the above case as an example: my_stacked_array has a shape of (2,8). If you want to select the index at which you want the split to occur, you have to keep the shape in mind. ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#how-to-manipulate-arrays",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#how-to-manipulate-arrays"
  },"204": {
    "doc": "03.a - NumPy Introduction",
    "title": "How To Visualize NumPy Arrays",
    "content": "Lastly, something that will definitely come in handy is to know how you can plot your arrays. This can especially be handy in data exploration, but also in later stages of the data science workflow, when you want to visualize your arrays. With np.histogram() . Contrary to what the function might suggest, the np.histogram() function doesn’t draw the histogram but it does compute the occurrences of the array that fall within each bin; This will determine the area that each bar of your histogram takes up. What you pass to the np.histogram() function then is first the input data or the array that you’re working with. The array will be flattened when the histogram is computed. # Initialize your array my_3d_array = np.array([[[1,2,3,4], [5,6,7,8]], [[1,2,3,4], [9,10,11,12]]], dtype=np.int64) . # Pass the array to `np.histogram()` print(np.histogram(my_3d_array)) . (array([4, 2, 2, 1, 1, 1, 1, 1, 1, 2]), array([ 1. , 2.1, 3.2, 4.3, 5.4, 6.5, 7.6, 8.7, 9.8, 10.9, 12. ])) . # Specify the number of bins print(np.histogram(my_3d_array, bins=range(0,13))) . (array([0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2]), array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])) . You’ll see that as a result, the histogram will be computed: the first array lists the frequencies for all the elements of your array, while the second array lists the bins that would be used if you don’t specify any bins. If you do specify a number of bins, the result of the computation will be different: the floats will be gone and you’ll see all integers for the bins. There are still some other arguments that you can specify that can influence the histogram that is computed. You can find all of them here. But what is the point of computing such a histogram if you can’t visualize it? . Visualization is a piece of cake with the help of Matplotlib, but you don’t need np.histogram() to compute the histogram. plt.hist() does this for itself when you pass it the (flattened) data and the bins: . # Import numpy and matplotlib import numpy as np import matplotlib.pyplot as plt # Construct the histogram with a flattened 3d array and a range of bins plt.hist(my_3d_array.ravel(), bins=range(0,13)) # Add a title to the plot plt.title('Frequency of My 3D Array Elements') # Show the plot plt.show() . Using np.meshgrid() . Another way to (indirectly) visualize your array is by using np.meshgrid(). The problem that you face with arrays is that you need 2-D arrays of x and y coordinate values. With the above function, you can create a rectangular grid out of an array of x values and an array of y values: the np.meshgrid() function takes two 1D arrays and produces two 2D matrices corresponding to all pairs of (x, y) in the two arrays. Then, you can use these matrices to make all sorts of plots. np.meshgrid() is particularly useful if you want to evaluate functions on a grid, as the code below demonstrates: . # Import NumPy and Matplotlib import numpy as np import matplotlib.pyplot as plt # Create an array points = np.arange(-5, 5, 0.01) # Make a meshgrid xs, ys = np.meshgrid(points, points) z = np.sqrt(xs ** 2 + ys ** 2) # Display the image on the axes plt.imshow(z, cmap=plt.cm.gray) # Draw a color bar plt.colorbar() # Show the plot plt.show() . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#how-to-visualize-numpy-arrays",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html#how-to-visualize-numpy-arrays"
  },"205": {
    "doc": "03.a - NumPy Introduction",
    "title": "03.a - NumPy Introduction",
    "content": " ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.a%20-%20NumPy%20Introduction.html"
  },"206": {
    "doc": "03.a Web Scraping with Beautiful Soup",
    "title": "Web Scraping with Beautiful Soup",
    "content": "source . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#web-scraping-with-beautiful-soup",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#web-scraping-with-beautiful-soup"
  },"207": {
    "doc": "03.a Web Scraping with Beautiful Soup",
    "title": "Introduction",
    "content": "Many data analysis, big data, and machine learning projects require scraping websites to gather the data that you’ll be working with. The Python programming language is widely used in the data science community, and therefore has an ecosystem of modules and tools that you can use in your own projects. In this tutorial we will be focusing on the Beautiful Soup module. Beautiful Soup is a Python library that allows for quick turnaround on web scraping projects. Currently available as Beautiful Soup 4 and compatible with both Python 2.7 and Python 3, Beautiful Soup creates a parse tree from parsed HTML and XML documents (including documents with non-closed tags or tag soup and other malformed markup). In this tutorial, we will collect and parse a web page in order to grab textual data and write the information we have gathered to a CSV file. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#introduction",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#introduction"
  },"208": {
    "doc": "03.a Web Scraping with Beautiful Soup",
    "title": "Prerequisites",
    "content": "Before working on this tutorial, you should have a local or server-based Python programming environment set up on your machine. You should have the Requests and Beautiful Soup modules installed, which you can achieve by following the tutorial “How To Work with Web Data Using Requests and Beautiful Soup with Python 3”. It would also be useful to have a working familiarity with these modules. Additionally, since we will be working with data scraped from the web, you should be comfortable with HTML structure and tagging. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#prerequisites",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#prerequisites"
  },"209": {
    "doc": "03.a Web Scraping with Beautiful Soup",
    "title": "Understanding the Data",
    "content": "In this tutorial, we’ll be working with data from the official website of the National Gallery of Art in the United States. The National Gallery is an art museum located on the National Mall in Washington, D.C. It holds over 120,000 pieces dated from the Renaissance to the present day done by more than 13,000 artists. We would like to search the Index of Artists, which, at the time of updating this tutorial, is available via the Internet Archive’s Wayback Machine at the following URL: . https://web.archive.org/web/20170131230332/https://www.nga.gov/collection/an.shtm . Beneath the Internet Archive’s header, you’ll see a page that looks like this: . Since we’ll be doing this project in order to learn about web scraping with Beautiful Soup, we don’t need to pull too much data from the site, so let’s limit the scope of the artist data we are looking to scrape. Let’s therefore choose one letter — in our example we’ll choose the letter Z — and we’ll see a page that looks like this: . In the page above, we see that the first artist listed at the time of writing is Zabaglia, Niccola, which is a good thing to note for when we start pulling data. We’ll start by working with this first page, with the following URL for the letter Z: . https://web.archive.org/web/20121007172955/http://www.nga.gov/collection/anZ1.htm . It is important to note for later how many pages total there are for the letter you are choosing to list, which you can discover by clicking through to the last page of artists. In this case, there are 4 pages total, and the last artist listed at the time of writing is Zykmund, Václav. The last page of Z artists has the following URL: . https://web.archive.org/web/20121010201041/http://www.nga.gov/collection/anZ4.htm . However, you can also access the above page by using the same Internet Archive numeric string of the first page: . https://web.archive.org/web/20121007172955/http://www.nga.gov/collection/anZ4.htm . This is important to note because we’ll be iterating through these pages later in this tutorial. To begin to familiarize yourself with how this web page is set up, you can take a look at its DOM, which will help you understand how the HTML is structured. In order to inspect the DOM, you can open your browser’s Developer Tools. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#understanding-the-data",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#understanding-the-data"
  },"210": {
    "doc": "03.a Web Scraping with Beautiful Soup",
    "title": "Importing the Libraries",
    "content": "We’ll create a new file called nga_z_artists.py in this tutorial. Within this file, we can begin to import the libraries we’ll be using — Requests and Beautiful Soup. The Requests library allows you to make use of HTTP within your Python programs in a human readable way, and the Beautiful Soup module is designed to get web scraping done quickly. We will import both Requests and Beautiful Soup with the import statement. For Beautiful Soup, we’ll be importing it from bs4, the package in which Beautiful Soup 4 is found. # Import libraries import requests from bs4 import BeautifulSoup . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#importing-the-libraries",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#importing-the-libraries"
  },"211": {
    "doc": "03.a Web Scraping with Beautiful Soup",
    "title": "Collecting and Parsing a Web Page",
    "content": "The next step we will need to do is collect the URL of the first web page with Requests. We’ll assign the URL for the first page to the variable page by using the method requests.get(). # Collect first page of artists’ list page = requests.get('https://web.archive.org/web/20121007172955/https://www.nga.gov/collection/anZ1.htm') . We’ll now create a BeautifulSoup object, or a parse tree. This object takes as its arguments the page.text document from Requests (the content of the server’s response) and then parses it from Python’s built-in html.parser. # Create a BeautifulSoup object soup = BeautifulSoup(page.text, 'html.parser') . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#collecting-and-parsing-a-web-page",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#collecting-and-parsing-a-web-page"
  },"212": {
    "doc": "03.a Web Scraping with Beautiful Soup",
    "title": "Pulling Text From a Web Page",
    "content": "For this project, we’ll collect artists’ names and the relevant links available on the website. You may want to collect different data, such as the artists’ nationality and dates. Whatever data you would like to collect, you need to find out how it is described by the DOM of the web page. To do this, in your web browser, right-click — or CTRL + click on macOS — on the first artist’s name, Zabaglia, Niccola. Within the context menu that pops up, you should see a menu item similar to Inspect Element (Firefox) or Inspect (Chrome). Once you click on the relevant Inspect menu item, the tools for web developers should appear within your browser. We want to look for the class and tags associated with the artists’ names in this list. We’ll see first that the table of names is within &lt;div&gt; tags where class=\"BodyText\". This is important to note so that we only search for text within this section of the web page. We also notice that the name Zabaglia, Niccola is in a link tag, since the name references a web page that describes the artist. So we will want to reference the &lt;a&gt; tag for links. Each artist’s name is a reference to a link. To do this, we’ll use Beautiful Soup’s find() and find_all() methods in order to pull the text of the artists’ names from the BodyText &lt;div&gt;. # Pull all text from the BodyText div artist_name_list = soup.find(class_='BodyText') . # Pull text from all instances of &lt;a&gt; tag within BodyText div artist_name_list_items = artist_name_list.find_all('a') . Next, at the bottom of our program file, we will want to create a for loop in order to iterate over all the artist names that we just put into the artist_name_list_items variable. We’ll print these names out with the prettify() method in order to turn the Beautiful Soup parse tree into a nicely formatted Unicode string. # Create for loop to print out the artists' names for i, artist_name in enumerate(artist_name_list_items): print(artist_name.prettify()) if i &gt; 4: break . &lt;a href=\"/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=11630\"&gt; Zabaglia, Niccola &lt;/a&gt; &lt;a href=\"/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=34202\"&gt; Zaccone, Fabian &lt;/a&gt; &lt;a href=\"/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=3475\"&gt; Zadkine, Ossip &lt;/a&gt; &lt;a href=\"/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=25135\"&gt; Zaech, Bernhard &lt;/a&gt; &lt;a href=\"/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=2298\"&gt; Zagar, Jacob &lt;/a&gt; &lt;a href=\"/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=23988\"&gt; Zagroba, Idalia &lt;/a&gt; . What we see in the output at this point is the full text and tags related to all of the artists’ names within the &lt;a&gt; tags found in the &lt;div class=\"BodyText\"&gt; tag on the first page, as well as some additional link text at the bottom. Since we don’t want this extra information, let’s work on removing this in the next section. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#pulling-text-from-a-web-page",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#pulling-text-from-a-web-page"
  },"213": {
    "doc": "03.a Web Scraping with Beautiful Soup",
    "title": "Removing Superfluous Data",
    "content": "So far, we have been able to collect all the link text data within one &lt;div&gt; section of our web page. However, we don’t want to have the bottom links that don’t reference artists’ names, so let’s work to remove that part. In order to remove the bottom links of the page, let’s again right-click and Inspect the DOM. We’ll see that the links on the bottom of the &lt;div class=\"BodyText\"&gt; section are contained in an HTML table: &lt;table class=\"AlphaNav\"&gt;: . We can therefore use Beautiful Soup to find the AlphaNav class and use the decompose() method to remove a tag from the parse tree and then destroy it along with its contents. We’ll use the variable last_links to reference these bottom links and add them to the program file: . page = requests.get('https://web.archive.org/web/20121007172955/https://www.nga.gov/collection/anZ1.htm') soup = BeautifulSoup(page.text, 'html.parser') # Remove bottom links last_links = soup.find(class_='AlphaNav') last_links.decompose() artist_name_list = soup.find(class_='BodyText') artist_name_list_items = artist_name_list.find_all('a') for i, artist_name in enumerate(artist_name_list_items): print(artist_name.prettify()) if i &gt; 4: break . &lt;a href=\"/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=11630\"&gt; Zabaglia, Niccola &lt;/a&gt; &lt;a href=\"/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=34202\"&gt; Zaccone, Fabian &lt;/a&gt; &lt;a href=\"/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=3475\"&gt; Zadkine, Ossip &lt;/a&gt; &lt;a href=\"/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=25135\"&gt; Zaech, Bernhard &lt;/a&gt; &lt;a href=\"/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=2298\"&gt; Zagar, Jacob &lt;/a&gt; &lt;a href=\"/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=23988\"&gt; Zagroba, Idalia &lt;/a&gt; . At this point, we see that the output no longer includes the links at the bottom of the web page, and now only displays the links associated with artists’ names. Until now, we have targeted the links with the artists’ names specifically, but we have the extra tag data that we don’t really want. Let’s remove that in the next section. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#removing-superfluous-data",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#removing-superfluous-data"
  },"214": {
    "doc": "03.a Web Scraping with Beautiful Soup",
    "title": "Pulling the Contents from a Tag",
    "content": "In order to access only the actual artists’ names, we’ll want to target the contents of the &lt;a&gt; tags rather than print out the entire link tag. We can do this with Beautiful Soup’s .contents, which will return the tag’s children as a Python list data type. Let’s revise the for loop so that instead of printing the entire link and its tag, we’ll print the list of children (i.e. the artists’ full names): . # Use .contents to pull out the &lt;a&gt; tag’s children for i, artist_name in enumerate(artist_name_list_items): names = artist_name.contents[0] print(names) if i &gt; 4: break . Zabaglia, Niccola Zaccone, Fabian Zadkine, Ossip Zaech, Bernhard Zagar, Jacob Zagroba, Idalia . Note that we are iterating over the list above by calling on the index number of each item. We have received back a list of all the artists’ names available on the first page of the letter Z. However, what if we want to also capture the URLs associated with those artists? We can extract URLs found within a page’s &lt;a&gt; tags by using Beautiful Soup’s get('href') method. From the output of the links above, we know that the entire URL is not being captured, so we will concatenate the link string with the front of the URL string (in this case https://web.archive.org/). These lines we’ll also add to the for loop: . for i, artist_name in enumerate(artist_name_list_items): names = artist_name.contents[0] links = 'https://web.archive.org' + artist_name.get('href') print(names) print(links) if i &gt; 4: break . Zabaglia, Niccola https://web.archive.org/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=11630 Zaccone, Fabian https://web.archive.org/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=34202 Zadkine, Ossip https://web.archive.org/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=3475 Zaech, Bernhard https://web.archive.org/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=25135 Zagar, Jacob https://web.archive.org/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=2298 Zagroba, Idalia https://web.archive.org/web/20121007172955/https://www.nga.gov/cgi-bin/tsearch?artistid=23988 . Although we are now getting information from the website, it is currently just printing to our terminal window. Let’s instead capture this data so that we can use it elsewhere by writing it to a file. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#pulling-the-contents-from-a-tag",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#pulling-the-contents-from-a-tag"
  },"215": {
    "doc": "03.a Web Scraping with Beautiful Soup",
    "title": "Writing the Data to a CSV File",
    "content": "Collecting data that only lives in a terminal window is not very useful. Comma-separated values (CSV) files allow us to store tabular data in plain text, and is a common format for spreadsheets and databases. Before beginning with this section, you should familiarize yourself with how to handle plain text files in Python. First, we need to import Python’s built-in csv module along with the other modules at the top of the Python programming file: . import csv . Next, we’ll create and open a file called z-artist-names.csv for us to write to (we’ll use the variable f for file here) by using the 'w' mode. We’ll also write the top row headings: Name and Link which we’ll pass to the writerow() method as a list: . f = csv.writer(open('z-artist-names.csv', 'w')) f.writerow(['Name', 'Link']) . 11 . Finally, within our for loop, we’ll write each row with the artists’ names and their associated links: . page = requests.get('https://web.archive.org/web/20121007172955/http://www.nga.gov/collection/anZ1.htm') soup = BeautifulSoup(page.text, 'html.parser') last_links = soup.find(class_='AlphaNav') last_links.decompose() # Create a file to write to, add headers row f = csv.writer(open('z-artist-names.csv', 'w')) f.writerow(['Name', 'Link']) artist_name_list = soup.find(class_='BodyText') artist_name_list_items = artist_name_list.find_all('a') for artist_name in artist_name_list_items: names = artist_name.contents[0] links = 'https://web.archive.org' + artist_name.get('href') # Add each artist’s name and associated link to a row f.writerow([names, links]) . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#writing-the-data-to-a-csv-file",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#writing-the-data-to-a-csv-file"
  },"216": {
    "doc": "03.a Web Scraping with Beautiful Soup",
    "title": "Retrieving Related Pages",
    "content": "We have created a program that will pull data from the first page of the list of artists whose last names start with the letter Z. However, there are 4 pages in total of these artists available on the website. In order to collect all of these pages, we can perform more iterations with for loops. This will revise most of the code we have written so far, but will employ similar concepts. To start, we’ll want to initialize a list to hold the pages: . pages = [] . We will populate this initialized list with the following for loop: . for i in range(1, 5): url = 'https://web.archive.org/web/20121007172955/https://www.nga.gov/collection/anZ' + str(i) + '.htm' pages.append(url) . Earlier in this tutorial, we noted that we should pay attention to the total number of pages there are that contain artists’ names starting with the letter Z (or whatever letter we’re using). Since there are 4 pages for the letter Z, we constructed the for loop above with a range of 1 to 5 so that it will iterate through each of the 4 pages. For this specific web site, the URLs begin with the string https://web.archive.org/web/20121007172955/https://www.nga.gov/collection/anZ and then are followed with a number for the page (which will be the integer i from the for loop that we convert to a string) and end with .htm. We will concatenate these strings together and then append the result to the pages list. In addition to this loop, we’ll have a second loop that will go through each of the pages above. The code in this for loop will look similar to the code we have created so far, as it is doing the task we completed for the first page of the letter Z artists for each of the 4 pages total. Note that because we have put the original program into the second for loop, we now have the original loop as a nested for loop contained in it. The two for loops will look like this: . pages = [] for i in range(1, 5): url = 'https://web.archive.org/web/20121007172955/https://www.nga.gov/collection/anZ' + str(i) + '.htm' pages.append(url) for item in pages: page = requests.get(item) soup = BeautifulSoup(page.text, 'html.parser') last_links = soup.find(class_='AlphaNav') last_links.decompose() artist_name_list = soup.find(class_='BodyText') artist_name_list_items = artist_name_list.find_all('a') for artist_name in artist_name_list_items: names = artist_name.contents[0] links = 'https://web.archive.org' + artist_name.get('href') f.writerow([names, links]) . In the code above, you should see that the first for loop is iterating over the pages and the second for loop is scraping data from each of those pages and then is adding the artists’ names and links line by line through each row of each page. These two for loops come below the import statements, the CSV file creation and writer (with the line for writing the headers of the file), and the initialization of the pages variable (assigned to a list). Within the greater context of the programming file, the complete code looks like this: . import requests import csv from bs4 import BeautifulSoup f = csv.writer(open('z-artist-names.csv', 'w')) f.writerow(['Name', 'Link']) pages = [] for i in range(1, 5): url = 'https://web.archive.org/web/20121007172955/https://www.nga.gov/collection/anZ' + str(i) + '.htm' pages.append(url) for item in pages: page = requests.get(item) soup = BeautifulSoup(page.text, 'html.parser') last_links = soup.find(class_='AlphaNav') last_links.decompose() artist_name_list = soup.find(class_='BodyText') artist_name_list_items = artist_name_list.find_all('a') for artist_name in artist_name_list_items: names = artist_name.contents[0] links = 'https://web.archive.org' + artist_name.get('href') f.writerow([names, links]) . Since this program is doing a bit of work, it will take a little while to create the CSV file. Once it is done, the output will be complete, showing the artists’ names and their associated links from Zabaglia, Niccola to Zykmund, Václav. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#retrieving-related-pages",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#retrieving-related-pages"
  },"217": {
    "doc": "03.a Web Scraping with Beautiful Soup",
    "title": "Being Considerate",
    "content": "When scraping web pages, it is important to remain considerate of the servers you are grabbing information from. Check to see if a site has terms of service or terms of use that pertains to web scraping. Also, check to see if a site has an API that allows you to grab data before scraping it yourself. Be sure to not continuously hit servers to gather data. Once you have collected what you need from a site, run scripts that will go over the data locally rather than burden someone else’s servers. Additionally, it is a good idea to scrape with a header that has your name and email so that a website can identify you and follow up if they have any questions. An example of a header you can use with the Python Requests library is as follows: . import requests headers = { 'User-Agent': 'Your Name, example.com', 'From': 'email@example.com' } url = 'https://example.com' page = requests.get(url, headers = headers) . Using headers with identifiable information ensures that the people who go over a server’s logs can reach out to you. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#being-considerate",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#being-considerate"
  },"218": {
    "doc": "03.a Web Scraping with Beautiful Soup",
    "title": "Conclusion",
    "content": "This tutorial went through using Python and Beautiful Soup to scrape data from a website. We stored the text that we gathered within a CSV file. You can continue working on this project by collecting more data and making your CSV file more robust. For example, you may want to include the nationalities and years of each artist. You can also use what you have learned to scrape data from other websites. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#conclusion",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html#conclusion"
  },"219": {
    "doc": "03.a Web Scraping with Beautiful Soup",
    "title": "03.a Web Scraping with Beautiful Soup",
    "content": " ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.a%20Web%20Scraping%20with%20Beautiful%20Soup.html"
  },"220": {
    "doc": "03.b - NumPy Data analysis",
    "title": "NumPy Tutorial: Data analysis with Python",
    "content": "Source . NumPy is a commonly used Python data analysis package. By using NumPy, you can speed up your workflow, and interface with other packages in the Python ecosystem, like scikit-learn, that use NumPy under the hood. NumPy was originally developed in the mid 2000s, and arose from an even older package called Numeric. This longevity means that almost every data analysis or machine learning package for Python leverages NumPy in some way. In this tutorial, we’ll walk through using NumPy to analyze data on wine quality. The data contains information on various attributes of wines, such as pH and fixed acidity, along with a quality score between 0 and 10 for each wine. The quality score is the average of at least 3 human taste testers. As we learn how to work with NumPy, we’ll try to figure out more about the perceived quality of wine. The wines we’ll be analyzing are from the Minho region of Portugal. The data was downloaded from the UCI Machine Learning Repository, and is available here. Here are the first few rows of the winequality-red.csv file, which we’ll be using throughout this tutorial: . \"fixed acidity\";\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlorides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"alcohol\";\"quality\" 7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5 7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5 . The data is in what I’m going to call ssv (semicolon separated values) format – each record is separated by a semicolon (;), and rows are separated by a new line. There are 1600 rows in the file, including a header row, and 12 columns. Before we get started, a quick version note – we’ll be using Python 3.5. Our code examples will be done using Jupyter notebook. If you want to jump right into a specific area, here are the topics: . | Creating an Array | Reading Text Files | Array Indexing | N-Dimensional Arrays | Data Types | Array Math | Array Methods | Array Comparison and Filtering | Reshaping and Combining Arrays | . Lists Of Lists for CSV Data Before using NumPy, we’ll first try to work with the data using Python and the csv package. We can read in the file using the csv.reader object, which will allow us to read in and split up all the content from the ssv file. In the below code, we: . | Import the csv library. | Open the winequality-red.csv file. | With the file open, create a new csv.reader object. | Pass in the keyword argument delimiter=”;” to make sure that the records are split up on the semicolon character instead of the default comma character. | . | Call the list type to get all the rows from the file. | Assign the result to wines. | . | . import csv with open(\"winequality-red.csv\", 'r') as f: wines = list(csv.reader(f, delimiter=\";\")) # print(wines[:3]) headers = wines[0] wines_only = wines[1:] . # print the headers print(headers) . ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality'] . # print the 1st row of data print(wines_only[0]) . ['7.4', '0.7', '0', '1.9', '0.076', '11', '34', '0.9978', '3.51', '0.56', '9.4', '5'] . # print the 1st three rows of data print(wines_only[:3]) . [['7.4', '0.7', '0', '1.9', '0.076', '11', '34', '0.9978', '3.51', '0.56', '9.4', '5'], ['7.8', '0.88', '0', '2.6', '0.098', '25', '67', '0.9968', '3.2', '0.68', '9.8', '5'], ['11.2', '0.28', '0.56', '1.9', '0.075', '17', '60', '0.998', '3.16', '0.58', '9.8', '6']] . The data has been read into a list of lists. Each inner list is a row from the ssv file. As you may have noticed, each item in the entire list of lists is represented as a string, which will make it harder to do computations. As you can see from the table above, we’ve read in three rows, the first of which contains column headers. Each row after the header row represents a wine. The first element of each row is the fixed acidity, the second is the volatile acidity, and so on. ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#numpy-tutorial-data-analysis-with-python",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#numpy-tutorial-data-analysis-with-python"
  },"221": {
    "doc": "03.b - NumPy Data analysis",
    "title": "Calculate Average Wine Quality",
    "content": "We can find the average quality of the wines. The below code will: . | Extract the last element from each row after the header row. | Convert each extracted element to a float. | Assign all the extracted elements to the list qualities. | Divide the sum of all the elements in qualities by the total number of elements in qualities to the get the mean. | . # calculate average wine quality with a loop qualities = [] for row in wines[1:]: qualities.append(float(row[-1])) sum(qualities) / len(wines[1:]) . 5.636420525657071 . # calculate average wine quality with a list comprehension qualities = [float(row[-1]) for row in wines[1:]] sum(qualities) / len(wines[1:]) . 5.636420525657071 . Although we were able to do the calculation we wanted, the code is fairly complex, and it won’t be fun to have to do something similar every time we want to compute a quantity. Luckily, we can use NumPy to make it easier to work with our data. ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#calculate-average-wine-quality",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#calculate-average-wine-quality"
  },"222": {
    "doc": "03.b - NumPy Data analysis",
    "title": "Numpy 2-Dimensional Arrays",
    "content": "With NumPy, we work with multidimensional arrays. We’ll dive into all of the possible types of multidimensional arrays later on, but for now, we’ll focus on 2-dimensional arrays. A 2-dimensional array is also known as a matrix, and is something you should be familiar with. In fact, it’s just a different way of thinking about a list of lists. A matrix has rows and columns. By specifying a row number and a column number, we’re able to extract an element from a matrix. If we picked the element at the first row and the second column, we’d get volatile acidity. If we picked the element in the third row and the second column, we’d get 0.88. In a NumPy array, the number of dimensions is called the rank, and each dimension is called an axis. So . | the rows are the first axis | the columns are the second axis | . Now that you understand the basics of matrices, let’s see how we can get from our list of lists to a NumPy array. ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#numpy-2-dimensional-arrays",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#numpy-2-dimensional-arrays"
  },"223": {
    "doc": "03.b - NumPy Data analysis",
    "title": "Creating A NumPy Array",
    "content": "We can create a NumPy array using the numpy.array function. If we pass in a list of lists, it will automatically create a NumPy array with the same number of rows and columns. Because we want all of the elements in the array to be float elements for easy computation, we’ll leave off the header row, which contains strings. One of the limitations of NumPy is that all the elements in an array have to be of the same type, so if we include the header row, all the elements in the array will be read in as strings. Because we want to be able to do computations like find the average quality of the wines, we need the elements to all be floats. In the below code, we: . | Import the numpy package. | Pass the list of lists wines into the array function, which converts it into a NumPy array. | Exclude the header row with list slicing. | Specify the keyword argument dtype to make sure each element is converted to a float. We’ll dive more into what the dtype is later on. | . | . import numpy as np np.set_printoptions(precision=2) # set the output print precision for readability # create the numpy array skipping the headers wines = np.array(wines[1:], dtype=np.float) . /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_13638/4037387242.py:5: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations wines = np.array(wines[1:], dtype=np.float) . # If we display wines, we'll now get a NumPy array: print(type(wines), wines) . &lt;class 'numpy.ndarray'&gt; [[ 7.4 0.7 0... 0.56 9.4 5. ] [ 7.8 0.88 0... 0.68 9.8 5. ] [11.2 0.28 0.56 ... 0.58 9.8 6. ] ... [ 6.3 0.51 0.13 ... 0.75 11. 6. ] [ 5.9 0.65 0.12 ... 0.71 10.2 5. ] [ 6. 0.31 0.47 ... 0.66 11. 6. ]] . # We can check the number of rows and columns in our data using the shape property of NumPy arrays: wines.shape . (1598, 12) . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#creating-a-numpy-array",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#creating-a-numpy-array"
  },"224": {
    "doc": "03.b - NumPy Data analysis",
    "title": "Alternative NumPy Array Creation Methods",
    "content": "There are a variety of methods that you can use to create NumPy arrays. It’s useful to create an array with all zero elements in cases when you need an array of fixed size, but don’t have any values for it yet. To start with, you can create an array where every element is zero. The below code will create an array with 3 rows and 4 columns, where every element is 0, using numpy.zeros: . empty_array = np.zeros((3, 4)) empty_array . array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) . Creating arrays full of random numbers can be useful when you want to quickly test your code with sample arrays. You can also create an array where each element is a random number using numpy.random.rand. np.random.rand(2, 3) . array([[0.63, 0.05, 0.44], [0.67, 0.3 , 0.26]]) . Using NumPy To Read In Files . It’s possible to use NumPy to directly read csv or other files into arrays. We can do this using the numpy.genfromtxt function. We can use it to read in our initial data on red wines. In the below code, we: . | Use the genfromtxt function to read in the winequality-red.csv file. | Specify the keyword argument delimiter=\";\" so that the fields are parsed properly. | Specify the keyword argument skip_header=1 so that the header row is skipped. | . wines = np.genfromtxt(\"winequality-red.csv\", delimiter=\";\", skip_header=1) wines . array([[ 7.4 , 0.7 , 0. , ..., 0.56, 9.4 , 5. ], [ 7.8 , 0.88, 0. , ..., 0.68, 9.8 , 5. ], [11.2 , 0.28, 0.56, ..., 0.58, 9.8 , 6. ], ..., [ 6.3 , 0.51, 0.13, ..., 0.75, 11. , 6. ], [ 5.9 , 0.65, 0.12, ..., 0.71, 10.2 , 5. ], [ 6. , 0.31, 0.47, ..., 0.66, 11. , 6. ]]) . Wines will end up looking the same as if we read it into a list then converted it to an array of floats. NumPy will automatically pick a data type for the elements in an array based on their format. ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#alternative-numpy-array-creation-methods",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#alternative-numpy-array-creation-methods"
  },"225": {
    "doc": "03.b - NumPy Data analysis",
    "title": "Indexing NumPy Arrays",
    "content": "We now know how to create arrays, but unless we can retrieve results from them, there isn’t a lot we can do with NumPy. We can use array indexing to select individual elements, groups of elements, or entire rows and columns. One important thing to keep in mind is that just like Python lists, NumPy is zero-indexed, meaning that: . | The index of the first row is 0 | The index of the first column is 0 | If we want to work with the fourth row, we’d use index 3 | If we want to work with the second row, we’d use index 1, and so on. | . We’ll again work with the wines array: . |   |   |   |   |   |   |   |   |   |   |   |   | . | 7.4 | 0.70 | 0.00 | 1.9 | 0.076 | 11 | 34 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 | . | 7.8 | 0.88 | 0.00 | 2.6 | 0.098 | 25 | 67 | 0.9968 | 3.20 | 0.68 | 9.8 | 5 | . | 7.8 | 0.76 | 0.04 | 2.3 | 0.092 | 15 | 54 | 0.9970 | 3.26 | 0.65 | 9.8 | 5 | . | 11.2 | 0.28 | 0.56 | 1.9 | 0.075 | 17 | 60 | 0.9980 | 3.16 | 0.58 | 9.8 | 6 | . | 7.4 | 0.70 | 0.00 | 1.9 | 0.076 | 11 | 34 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 | . Let’s select the element at row 3 and column 4. We pass: . | 2 as the row index | 3 as the column index. | . This retrieves the value from the third row and fourth column . wines[2, 3] . 1.9 . wines[2][3] . 1.9 . Since we’re working with a 2-dimensional array in NumPy we specify 2 indexes to retrieve an element. | The first index is the row, or axis 1, index | The second index is the column, or axis 2, index | . Any element in wines can be retrieved using 2 indexes. # rows 1, 2, 3 and column 4 wines[0:3, 3] . array([1.9, 2.6, 1.9]) . # all rows and column 3 wines[:, 2] . array([0. , 0. , 0.56, ..., 0.13, 0.12, 0.47]) . Just like with list slicing, it’s possible to omit the 0 to just retrieve all the elements from the beginning up to element 3: . # rows 1, 2, 3 and column 4 wines[:3, 3] . array([1.9, 2.6, 1.9]) . We can select an entire column by specifying that we want all the elements, from the first to the last. We specify this by just using the colon :, with no starting or ending indices. The below code will select the entire fourth column: . # all rows and column 4 wines[:, 3] . array([1.9, 2.6, 1.9, ..., 2.3, 2. , 3.6]) . We selected an entire column above, but we can also extract an entire row: . # row 4 and all columns wines[3, :] . array([ 7.4 , 0.7 , 0. , 1.9 , 0.08, 11. , 34. , 1. , 3.51, 0.56, 9.4 , 5. ]) . If we take our indexing to the extreme, we can select the entire array using two colons to select all the rows and columns in wines. This is a great party trick, but doesn’t have a lot of good applications: . wines[:, :] . array([[ 7.4 , 0.7 , 0. , ..., 0.56, 9.4 , 5. ], [ 7.8 , 0.88, 0. , ..., 0.68, 9.8 , 5. ], [11.2 , 0.28, 0.56, ..., 0.58, 9.8 , 6. ], ..., [ 6.3 , 0.51, 0.13, ..., 0.75, 11. , 6. ], [ 5.9 , 0.65, 0.12, ..., 0.71, 10.2 , 5. ], [ 6. , 0.31, 0.47, ..., 0.66, 11. , 6. ]]) . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#indexing-numpy-arrays",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#indexing-numpy-arrays"
  },"226": {
    "doc": "03.b - NumPy Data analysis",
    "title": "Assigning Values To NumPy Arrays",
    "content": "We can also use indexing to assign values to certain elements in arrays. We can do this by assigning directly to the indexed value: . # assign the value of 10 to the 2nd row and 6th column print('Before', wines[1, 4:7]) wines[1, 5] = 10 print('After', wines[1, 4:7]) . Before [ 0.1 25. 67. ] After [ 0.1 10. 67. ] . We can do the same for slices. To overwrite an entire column, we can do this: . # Overwrites all the values in the eleventh column with 50. print('Before', wines[:, 9:12]) wines[:, 10] = 50 print('After', wines[:, 9:12]) . Before [[ 0.56 9.4 5. ] [ 0.68 9.8 5. ] [ 0.58 9.8 6. ] ... [ 0.75 11. 6. ] [ 0.71 10.2 5. ] [ 0.66 11. 6. ]] After [[ 0.56 50. 5. ] [ 0.68 50. 5. ] [ 0.58 50. 6. ] ... [ 0.75 50. 6. ] [ 0.71 50. 5. ] [ 0.66 50. 6. ]] . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#assigning-values-to-numpy-arrays",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#assigning-values-to-numpy-arrays"
  },"227": {
    "doc": "03.b - NumPy Data analysis",
    "title": "1-Dimensional NumPy Arrays",
    "content": "So far, we’ve worked with 2-dimensional arrays, such as wines. However, NumPy is a package for working with multidimensional arrays. One of the most common types of multidimensional arrays is the 1-dimensional array, or vector. As you may have noticed above, when we sliced wines, we retrieved a 1-dimensional array. | A 1-dimensional array only needs a single index to retrieve an element. | Each row and column in a 2-dimensional array is a 1-dimensional array. | . Just like a list of lists is analogous to a 2-dimensional array, a single list is analogous to a 1-dimensional array. If we slice wines and only retrieve the third row, we get a 1-dimensional array: . third_wine = wines[3,:] third_wine . array([ 7.4 , 0.7 , 0. , 1.9 , 0.08, 11. , 34. , 1. , 3.51, 0.56, 50. , 5. ]) . We can retrieve individual elements from third_wine using a single index. # display the second item in third_wine third_wine[1] . 0.7 . Most NumPy functions that we’ve worked with, such as numpy.random.rand, can be used with multidimensional arrays. Here’s how we’d use numpy.random.rand to generate a random vector: . np.random.rand(3) . array([0.04, 0.38, 0.08]) . Previously, when we called np.random.rand, we passed in a shape for a 2-dimensional array, so the result was a 2-dimensional array. This time, we passed in a shape for a single dimensional array. The shape specifies the number of dimensions, and the size of the array in each dimension. A shape of (10,10) will be a 2-dimensional array with 10 rows and 10 columns. A shape of (10,) will be a 1-dimensional array with 10 elements. Where NumPy gets more complex is when we start to deal with arrays that have more than 2 dimensions. ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#1-dimensional-numpy-arrays",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#1-dimensional-numpy-arrays"
  },"228": {
    "doc": "03.b - NumPy Data analysis",
    "title": "N-Dimensional NumPy Arrays",
    "content": "This doesn’t happen extremely often, but there are cases when you’ll want to deal with arrays that have greater than 3 dimensions. One way to think of this is as a list of lists of lists. Let’s say we want to store the monthly earnings of a store, but we want to be able to quickly lookup the results for a quarter, and for a year. The earnings for one year might look like this: . [500, 505, 490, 810, 450, 678, 234, 897, 430, 560, 1023, 640] . The store earned $500 in January, $505 in February, and so on. We can split up these earnings by quarter into a list of lists: . year_one = [ [500,505,490], # 1st quarter [810,450,678], # 2nd quarter [234,897,430], # 3rd quarter [560,1023,640] # 4th quarter ] . We can retrieve the earnings from January by calling year_one[0][0]. If we want the results for a whole quarter, we can call year_one[0] or year_one[1]. We now have a 2-dimensional array, or matrix. But what if we now want to add the results from another year? We have to add a third dimension: . earnings = [ [ # year 1 [500,505,490], # year 1, 1st quarter [810,450,678], # year 1, 2nd quarter [234,897,430], # year 1, 3rd quarter [560,1023,640] # year 1, 4th quarter ], [ # year =2 [600,605,490], # year 2, 1st quarter [345,900,1000],# year 2, 2nd quarter [780,730,710], # year 2, 3rd quarter [670,540,324] # year 2, 4th quarter ] ] . We can retrieve the earnings from January of the first year by calling earnings[0][0][0]. We now need three indexes to retrieve a single element. A three-dimensional array in NumPy is much the same. In fact, we can convert earnings to an array and then get the earnings for January of the first year: . earnings = np.array(earnings) . # year 1, 1st quarter, 1st month (January) earnings[0,0,0] . 500 . # year 2, 3rd quarter, 1st month (July) earnings[1,2,0] . 780 . # we can also find the shape of the array earnings.shape . (2, 4, 3) . Indexing and slicing work the exact same way with a 3-dimensional array, but now we have an extra axis to pass in. If we wanted to get the earnings for January of all years, we could do this: . # all years, 1st quarter, 1st month (January) earnings[:,0,0] . array([500, 600]) . If we wanted to get first quarter earnings from both years, we could do this: . # all years, 1st quarter, all months (January, February, March) earnings[:,0,:] . array([[500, 505, 490], [600, 605, 490]]) . Adding more dimensions can make it much easier to query your data if it’s organized in a certain way. As we go from 3-dimensional arrays to 4-dimensional and larger arrays, the same properties apply, and they can be indexed and sliced in the same ways. ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#n-dimensional-numpy-arrays",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#n-dimensional-numpy-arrays"
  },"229": {
    "doc": "03.b - NumPy Data analysis",
    "title": "NumPy Data Types",
    "content": "As we mentioned earlier, each NumPy array can store elements of a single data type. For example, wines contains only float values. NumPy stores values using its own data types, which are distinct from Python types like float and str. This is because the core of NumPy is written in a programming language called C, which stores data differently than the Python data types. NumPy data types map between Python and C, allowing us to use NumPy arrays without any conversion hitches. You can find the data type of a NumPy array by accessing the dtype property: . wines.dtype . dtype('float64') . NumPy has several different data types, which mostly map to Python data types, like float, and str. You can find a full listing of NumPy data types here, but here are a few important ones: . | float – numeric floating point data. | int – integer data. | string – character data. | object – Python objects. | . Data types additionally end with a suffix that indicates how many bits of memory they take up. So int32 is a 32 bit integer data type, and float64 is a 64 bit float data type. Converting Data Types . You can use the numpy.ndarray.astype method to convert an array to a different type. The method will actually copy the array, and return a new array with the specified data type. For instance, we can convert wines to the int data type: . # convert wines to the int data type wines.astype(int) . array([[ 7, 0, 0, ..., 0, 50, 5], [ 7, 0, 0, ..., 0, 50, 5], [11, 0, 0, ..., 0, 50, 6], ..., [ 6, 0, 0, ..., 0, 50, 6], [ 5, 0, 0, ..., 0, 50, 5], [ 6, 0, 0, ..., 0, 50, 6]]) . As you can see above, all of the items in the resulting array are integers. Note that we used the Python int type instead of a NumPy data type when converting wines. This is because several Python data types, including float, int, and string, can be used with NumPy, and are automatically converted to NumPy data types. We can check the name property of the dtype of the resulting array to see what data type NumPy mapped the resulting array to: . # convert to int int_wines = wines.astype(int) # check the data type int_wines.dtype.name . 'int64' . The array has been converted to a 64-bit integer data type. This allows for very long integer values, but takes up more space in memory than storing the values as 32-bit integers. If you want more control over how the array is stored in memory, you can directly create NumPy dtype objects like numpy.int32 . np.int32 . numpy.int32 . You can use these directly to convert between types: . # convert to a 64-bit integer wines.astype(np.int64) . array([[ 7, 0, 0, ..., 0, 50, 5], [ 7, 0, 0, ..., 0, 50, 5], [11, 0, 0, ..., 0, 50, 6], ..., [ 6, 0, 0, ..., 0, 50, 6], [ 5, 0, 0, ..., 0, 50, 5], [ 6, 0, 0, ..., 0, 50, 6]]) . # convert to a 32-bit integer wines.astype(np.int32) . array([[ 7, 0, 0, ..., 0, 50, 5], [ 7, 0, 0, ..., 0, 50, 5], [11, 0, 0, ..., 0, 50, 6], ..., [ 6, 0, 0, ..., 0, 50, 6], [ 5, 0, 0, ..., 0, 50, 5], [ 6, 0, 0, ..., 0, 50, 6]], dtype=int32) . # convert to a 16-bit integer wines.astype(np.int16) . array([[ 7, 0, 0, ..., 0, 50, 5], [ 7, 0, 0, ..., 0, 50, 5], [11, 0, 0, ..., 0, 50, 6], ..., [ 6, 0, 0, ..., 0, 50, 6], [ 5, 0, 0, ..., 0, 50, 5], [ 6, 0, 0, ..., 0, 50, 6]], dtype=int16) . # convert to a 8-bit integer wines.astype(np.int8) . array([[ 7, 0, 0, ..., 0, 50, 5], [ 7, 0, 0, ..., 0, 50, 5], [11, 0, 0, ..., 0, 50, 6], ..., [ 6, 0, 0, ..., 0, 50, 6], [ 5, 0, 0, ..., 0, 50, 5], [ 6, 0, 0, ..., 0, 50, 6]], dtype=int8) . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#numpy-data-types",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#numpy-data-types"
  },"230": {
    "doc": "03.b - NumPy Data analysis",
    "title": "NumPy Array Operations",
    "content": "NumPy makes it simple to perform mathematical operations on arrays. This is one of the primary advantages of NumPy, and makes it quite easy to do computations. Single Array Math . If you do any of the basic mathematical operations /, *, -, +, ^ with an array and a value, it will apply the operation to each of the elements in the array. Let’s say we want to add 10 points to each quality score because we’re feeling generous. Here’s how we’d do that: . # add 10 points to the quality score wines[:,-1] + 10 . array([15., 15., 16., ..., 16., 15., 16.]) . Note: that the above operation won’t change the wines array – it will return a new 1-dimensional array where 10 has been added to each element in the quality column of wines. If we instead did +=, we’d modify the array in place: . print('Before', wines[:,11]) # modify the data in place wines[:,11] += 10 print('After', wines[:,11]) . Before [5. 5. 6... 6. 5. 6.] After [15. 15. 16... 16. 15. 16.] . All the other operations work the same way. For example, if we want to multiply each of the quality score by 2, we could do it like this: . # multiply the quality score by 2 wines[:,11] * 2 . array([30., 30., 32., ..., 32., 30., 32.]) . Multiple Array Math . It’s also possible to do mathematical operations between arrays. This will apply the operation to pairs of elements. For example, if we add the quality column to itself, here’s what we get: . # add the quality column to itself wines[:,11] + wines[:,11] . array([30., 30., 32., ..., 32., 30., 32.]) . Note that this is equivalent to wines[:,11] * 2 – this is because NumPy adds each pair of elements. The first element in the first array is added to the first element in the second array, the second to the second, and so on. # add the quality column to itself wines[:,11] * 2 . array([30., 30., 32., ..., 32., 30., 32.]) . We can also use this to multiply arrays. Let’s say we want to pick a wine that maximizes alcohol content and quality. We’d multiply alcohol by quality, and select the wine with the highest score: . # multiply alcohol content by quality alcohol_by_quality = wines[:,10] * wines[:,11] print(alcohol_by_quality) . [750. 750. 800... 800. 750. 800.] . alcohol_by_quality.sort() print(alcohol_by_quality, alcohol_by_quality[-1]) . [650. 650. 650... 900. 900. 900.] 900.0 . All of the common operations /, *, -, +, ^ will work between arrays. ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#numpy-array-operations",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#numpy-array-operations"
  },"231": {
    "doc": "03.b - NumPy Data analysis",
    "title": "NumPy Array Methods",
    "content": "In addition to the common mathematical operations, NumPy also has several methods that you can use for more complex calculations on arrays. An example of this is the numpy.ndarray.sum method. This finds the sum of all the elements in an array by default: . # find the sum of all rows and the quality column total = 0 for row in wines: total += row[11] print(total) . 24987.0 . # find the sum of all rows and the quality column wines[:,11].sum(axis=0) . 24987.0 . # find the sum of the rows 1, 2, and 3 across all columns totals = [] for i in range(3): total = 0 for col in wines[i,:]: total += col totals.append(total) print(totals) . [125.1438, 158.2548, 161.753] . # find the sum of the rows 1, 2, and 3 across all columns wines[0:3,:].sum(axis=1) . array([125.14, 158.25, 161.75]) . We can pass the axis keyword argument into the sum method to find sums over an axis. If we call sum across the wines matrix, and pass in axis=0, we’ll find the sums over the first axis of the array. This will give us the sum of all the values in every column. This may seem backwards that the sums over the first axis would give us the sum of each column, but one way to think about this is that the specified axis is the one “going away”. So if we specify axis=0, we want the rows to go away, and we want to find the sums for each of the remaining axes across each row: . # sum each column for all rows totals = [0] * len(wines[0]) for i, total in enumerate(totals): for row_val in wines[:,i]: total += row_val totals[i] = total print(totals) . [13295.300000000045, 843.2250000000005, 433.2499999999982, 4057.2500000000027, 139.76699999999963, 25354.0, 74248.0, 1592.8009399999985, 5291.210000000001, 1051.7300000000007, 79900.0, 24987.0] . # sum each column for all rows wines.sum(axis=0) . array([13295.3 , 843.23, 433.25, 4057.25, 139.77, 25354. , 74248. , 1592.8 , 5291.21, 1051.73, 79900. , 24987. ]) . We can verify that we did the sum correctly by checking the shape. The shape should be 12, corresponding to the number of columns: . wines.sum(axis=0).shape . (12,) . If we pass in axis=1, we’ll find the sums over the second axis of the array. This will give us the sum of each row: . # sum each row for all columns totals = [0] * len(wines) for i, total in enumerate(totals): for col_val in wines[i,:]: total += col_val totals[i] = total print(totals[0:3], '...', totals[-3:]) . [125.1438, 158.2548, 161.753] ... [149.48174, 155.01547, 141.49249] . # sum each row for all columns wines.sum(axis=1) . array([125.14, 158.25, 161.75, ..., 149.48, 155.02, 141.49]) . wines.sum(axis=1).shape . (1598,) . There are several other methods that behave like the sum method, including: . | numpy.ndarray.mean — finds the mean of an array. | numpy.ndarray.std — finds the standard deviation of an array. | numpy.ndarray.min — finds the minimum value in an array. | numpy.ndarray.max — finds the maximum value in an array. | . You can find a full list of array methods here. ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#numpy-array-methods",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#numpy-array-methods"
  },"232": {
    "doc": "03.b - NumPy Data analysis",
    "title": "NumPy Array Comparisons",
    "content": "NumPy makes it possible to test to see if rows match certain values using mathematical comparison operations like &lt;, &gt;, &gt;=, &lt;=, and ==. For example, if we want to see which wines have a quality rating higher than 5, we can do this: . # return True for all rows in the Quality column that are greater than 5 wines[:,11] &gt; 5 . array([ True, True, True, ..., True, True, True]) . We get a Boolean array that tells us which of the wines have a quality rating greater than 5. We can do something similar with the other operators. For instance, we can see if any wines have a quality rating equal to 10: . # return True for all rows that have a Quality rating of 10 wines[:,11] == 10 . array([False, False, False, ..., False, False, False]) . Subsetting . One of the powerful things we can do with a Boolean array and a NumPy array is select only certain rows or columns in the NumPy array. For example, the below code will only select rows in wines where the quality is over 7: . # create a boolean array for wines with quality greater than 15 high_quality = wines[:,11] &gt; 15 print(len(high_quality), high_quality) . 1598 [False False True ... True False True] . # use boolean indexing to find high quality wines high_quality_wines = wines[high_quality,:] print(len(high_quality_wines), high_quality_wines) . 855 [[1.12e+01 2.80e-01 5.60e-01 ... 5.80e-01 5.00e+01 1.60e+01] [7.30e+00 6.50e-01 0.00e+00 ... 4.70e-01 5.00e+01 1.70e+01] [7.80e+00 5.80e-01 2.00e-02 ... 5.70e-01 5.00e+01 1.70e+01] ... [5.90e+00 5.50e-01 1.00e-01 ... 7.60e-01 5.00e+01 1.60e+01] [6.30e+00 5.10e-01 1.30e-01 ... 7.50e-01 5.00e+01 1.60e+01] [6.00e+00 3.10e-01 4.70e-01 ... 6.60e-01 5.00e+01 1.60e+01]] . We select only the rows where high_quality contains a True value, and all of the columns. This subsetting makes it simple to filter arrays for certain criteria. For example, we can look for wines with a lot of alcohol and high quality. In order to specify multiple conditions, we have to place each condition in parentheses (...), and separate conditions with an ampersand &amp;: . # create a boolean array for high alcohol content and high quality high_alcohol_and_quality = (wines[:,11] &gt; 7) &amp; (wines[:,10] &gt; 10) print(high_alcohol_and_quality) # use boolean indexing to select out the wines wines[high_alcohol_and_quality,:] . [ True True True ... True True True] array([[ 7.4 , 0.7 , 0. , ..., 0.56, 50. , 15. ], [ 7.8 , 0.88, 0. , ..., 0.68, 50. , 15. ], [11.2 , 0.28, 0.56, ..., 0.58, 50. , 16. ], ..., [ 6.3 , 0.51, 0.13, ..., 0.75, 50. , 16. ], [ 5.9 , 0.65, 0.12, ..., 0.71, 50. , 15. ], [ 6. , 0.31, 0.47, ..., 0.66, 50. , 16. ]]) . We can combine subsetting and assignment to overwrite certain values in an array: . high_alcohol_and_quality = (wines[:,10] &gt; 10) &amp; (wines[:,11] &gt; 7) wines[high_alcohol_and_quality,10:] = 20 . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#numpy-array-comparisons",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#numpy-array-comparisons"
  },"233": {
    "doc": "03.b - NumPy Data analysis",
    "title": "Reshaping NumPy Arrays",
    "content": "We can change the shape of arrays while still preserving all of their elements. This often can make it easier to access array elements. The simplest reshaping is to flip the axes, so rows become columns, and vice versa. We can accomplish this with the numpy.transpose function: . np.transpose(wines).shape . (12, 1598) . We can use the numpy.ravel function to turn an array into a one-dimensional representation. It will essentially flatten an array into a long sequence of values: . wines.ravel() . array([ 7.4 , 0.7 , 0. , ..., 0.66, 20. , 20. ]) . Here’s an example where we can see the ordering of numpy.ravel: . array_one = np.array( [ [1, 2, 3, 4], [5, 6, 7, 8] ] ) array_one.ravel() . array([1, 2, 3, 4, 5, 6, 7, 8]) . Finally, we can use the numpy.reshape function to reshape an array to a certain shape we specify. The below code will turn the second row of wines into a 2-dimensional array with 2 rows and 6 columns: . # print the current shape of the 2nd row and all columns wines[1,:].shape . (12,) . # reshape the 2nd row to a 2 by 6 matrix wines[1,:].reshape((2,6)) . array([[ 7.8 , 0.88, 0. , 2.6 , 0.1 , 10. ], [67. , 1. , 3.2 , 0.68, 20. , 20. ]]) . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#reshaping-numpy-arrays",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#reshaping-numpy-arrays"
  },"234": {
    "doc": "03.b - NumPy Data analysis",
    "title": "Combining NumPy Arrays",
    "content": "With NumPy, it’s very common to combine multiple arrays into a single unified array. We can use numpy.vstack to vertically stack multiple arrays. Think of it like the second arrays’s items being added as new rows to the first array. We can read in the winequality-white.csv dataset that contains information on the quality of white wines, then combine it with our existing dataset, wines, which contains information on red wines. In the below code, we: . | Read in winequality-white.csv. | Display the shape of white_wines. | . white_wines = np.genfromtxt(\"winequality-white.csv\", delimiter=\";\", skip_header=1) white_wines.shape . (4898, 12) . As you can see, we have attributes for 4898 wines. Now that we have the white wines data, we can combine all the wine data. In the below code, we: . | Use the vstack function to combine wines and white_wines. | Display the shape of the result. | . all_wines = np.vstack((wines, white_wines)) all_wines.shape . (6496, 12) . As you can see, the result has 6497 rows, which is the sum of the number of rows in wines and the number of rows in red_wines. If we want to combine arrays horizontally, where the number of rows stay constant, but the columns are joined, then we can use the numpy.hstack function. The arrays we combine need to have the same number of rows for this to work. Finally, we can use numpy.concatenate as a general purpose version of hstack and vstack. If we want to concatenate two arrays, we pass them into concatenate, then specify the axis keyword argument that we want to concatenate along. | Concatenating along the first axis is similar to vstack | Concatenating along the second axis is similar to hstack: | . x = np.concatenate((wines, white_wines), axis=0) print(x.shape, x) . (6496, 12) [[ 7.4 0.7 0... 0.56 20. 20. ] [ 7.8 0.88 0... 0.68 20. 20. ] [11.2 0.28 0.56 ... 0.58 20. 20. ] ... [ 6.5 0.24 0.19 ... 0.46 9.4 6. ] [ 5.5 0.29 0.3 ... 0.38 12.8 7. ] [ 6. 0.21 0.38 ... 0.32 11.8 6. ]] . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#combining-numpy-arrays",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#combining-numpy-arrays"
  },"235": {
    "doc": "03.b - NumPy Data analysis",
    "title": "Broadcasting",
    "content": "Unless the arrays that you’re operating on are the exact same size, it’s not possible to do elementwise operations. In cases like this, NumPy performs broadcasting to try to match up elements. Essentially, broadcasting involves a few steps: . | The last dimension of each array is compared. | If the dimension lengths are equal, or one of the dimensions is of length 1, then we keep going. | If the dimension lengths aren’t equal, and none of the dimensions have length 1, then there’s an error. | . | Continue checking dimensions until the shortest array is out of dimensions. | . For example, the following two shapes are compatible: . A: (50,3) B (3,) . This is because the length of the trailing dimension of array A is 3, and the length of the trailing dimension of array B is 3. They’re equal, so that dimension is okay. Array B is then out of elements, so we’re okay, and the arrays are compatible for mathematical operations. The following two shapes are also compatible: . A: (1,2) B (50,2) . The last dimension matches, and A is of length 1 in the first dimension. These two arrays don’t match: . A: (50,50) B: (49,49) . The lengths of the dimensions aren’t equal, and neither array has either dimension length equal to 1. There’s a detailed explanation of broadcasting here, but we’ll go through a few examples to illustrate the principle: . wines * np.array([1,2]) . --------------------------------------------------------------------------- ValueError Traceback (most recent call last) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_13638/2228080465.py in &lt;module&gt; ----&gt; 1 wines * np.array([1,2]) ValueError: operands could not be broadcast together with shapes (1598,12) (2,) . The above example didn’t work because the two arrays don’t have a matching trailing dimension. Here’s an example where the last dimension does match: . array_one = np.array( [ [1,2], [3,4] ] ) array_two = np.array([4,5]) array_one + array_two . array([[5, 7], [7, 9]]) . As you can see, array_two has been broadcasted across each row of array_one. Here’s an example with our wines data: . rand_array = np.random.rand(12) wines + rand_array . array([[ 7.57, 1.1 , 0.66, ..., 0.59, 20.33, 20.78], [ 7.97, 1.28, 0.66, ..., 0.71, 20.33, 20.78], [11.37, 0.68, 1.22, ..., 0.61, 20.33, 20.78], ..., [ 6.47, 0.91, 0.79, ..., 0.78, 20.33, 20.78], [ 6.07, 1.04, 0.78, ..., 0.74, 20.33, 20.78], [ 6.17, 0.71, 1.13, ..., 0.69, 20.33, 20.78]]) . ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#broadcasting",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html#broadcasting"
  },"236": {
    "doc": "03.b - NumPy Data analysis",
    "title": "03.b - NumPy Data analysis",
    "content": " ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201/03.b%20-%20NumPy%20Data%20analysis.html"
  },"237": {
    "doc": "03.b Web Scraping Using Selenium",
    "title": "Web Scraping Using Selenium",
    "content": "Source . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.b%20Web%20Scraping%20Using%20Selenium.html#web-scraping-using-selenium",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.b%20Web%20Scraping%20Using%20Selenium.html#web-scraping-using-selenium"
  },"238": {
    "doc": "03.b Web Scraping Using Selenium",
    "title": "What is web scraping?",
    "content": "Web scraping is a technique for extracting information from the internet automatically using a software that simulates human web surfing. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.b%20Web%20Scraping%20Using%20Selenium.html#what-is-web-scraping",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.b%20Web%20Scraping%20Using%20Selenium.html#what-is-web-scraping"
  },"239": {
    "doc": "03.b Web Scraping Using Selenium",
    "title": "How is web-scraping useful?",
    "content": "Web scraping helps us extract large volumes of data about customers, products, people, stock markets, etc. It is usually difficult to get this kind of information on a large scale using traditional data collection methods. We can utilize the data collected from a website such as e-commerce portal, social media channels to understand customer behaviors and sentiments, buying patterns, and brand attribute associations which are critical insights for any business. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.b%20Web%20Scraping%20Using%20Selenium.html#how-is-web-scraping-useful",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.b%20Web%20Scraping%20Using%20Selenium.html#how-is-web-scraping-useful"
  },"240": {
    "doc": "03.b Web Scraping Using Selenium",
    "title": "Getting Started",
    "content": "Since we have defined our purpose of scraping, let us delve into the nitty-gritty of how to actually do all the fun stuff! Before that below are some of the housekeeping instructions regarding installations of packages. | Python version: We will be using Python 3.0 | Selenium package: You can install selenium package using the following command | . !pip install selenium . | Chrome driver: Please install the latest version of chromedriver from here. | . Please note you need Google Chromeinstalled on your machines to work through this illustration. The first and foremost thing while scraping a website is to understand the structure of the website. We will be scraping Edmunds.com, a car forum. This website aids people in their car buying decisions. People can post their reviews about different cars in the discussion forums (very similar to how one posts reviews on Amazon). We will be scraping the discussion about entry level luxury car brands. We will scrape ~5000 comments from different users across multiple pages. We will scrape user id, date of comment and comments and export it into a csv file for any further analysis. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.b%20Web%20Scraping%20Using%20Selenium.html#getting-started",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.b%20Web%20Scraping%20Using%20Selenium.html#getting-started"
  },"241": {
    "doc": "03.b Web Scraping Using Selenium",
    "title": "Let’s begin",
    "content": "We will first import important packages in our Notebook . #Importing packages import pandas as pd from selenium import webdriver . Let’s now create a new instance of google chrome. This will help our program open an url in google chrome. from webdriver_manager.microsoft import EdgeChromiumDriverManager driver = webdriver.Edge(EdgeChromiumDriverManager().install()) . [WDM] - ====== WebDriver manager ====== [WDM] - Driver [/Users/bk/.wdm/drivers/edgedriver/mac64/94.0.992.50/msedgedriver] found in cache /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_23762/1678680.py:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object driver = webdriver.Edge(EdgeChromiumDriverManager().install()) . Let’s now access google chrome and open our website. By the way, chrome knows that you are accessing it through an automated software! . driver.get('https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p1') . Woha! We just opened an url from python notebook. So, how does our web page look like? . We will inspect 3 items (user id, date and comment) on our web page and understand how we can extract them. | User id: Inspecting the userid, we can see the highlighted text represents the XML code for user id. | . The XML path (XPath)for the userid is shown below. There is an interesting thing to note here that the XML path contains a comment id, which uniquely denotes each comment on the website. This will be very helpful as we try to recursively scrape multiple comments . //*[@id=”Comment_5561090\"]/div/div[2]/div[1]/span[1]/a[2] . If we see the XPath in the picture, we will observe that it contains the user id ‘dino001’. How do we extract the values inside a XPath? . Selenium has a function called “find_elements_by_xpath”. We will pass our XPath into this function and get a selenium element. Once we have the element, we can extract the text inside our XPath using the ‘text’ function. In our case the text is basically the user id (‘dino001’). userid_element = driver.find_elements_by_xpath('//*[@id=\"Comment_5561090\"]/div/div[2]/div[1]/span[1]/a[2]')[0] userid = userid_element.text . /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_23762/2211819048.py:1: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead userid_element = driver.find_elements_by_xpath('//*[@id=\"Comment_5561090\"]/div/div[2]/div[1]/span[1]/a[2]')[0] --------------------------------------------------------------------------- IndexError Traceback (most recent call last) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_23762/2211819048.py in &lt;module&gt; ----&gt; 1 userid_element = driver.find_elements_by_xpath('//*[@id=\"Comment_5561090\"]/div/div[2]/div[1]/span[1]/a[2]')[0] 2 userid = userid_element.text IndexError: list index out of range . | Comment Date: Similar to the user id, we will now inspect the date when the comment was posted. | . Let’s also see the XPath for the comment date. Again note the unique comment id in the XPath. //*[@id=\"Comment_5561090\"]/div/div[2]/div[2]/span[1]/a/time . So, how do we extract date from the above XPath? . We will again use the function “find_elements_by_xpath” to get the selenium element. Now, if we carefully observe the highlighted text in the picture, we will see that the date is stored inside the ‘title’ attribute. We can access the values inside attributes using the function ‘get_attribute’. We will pass the tag name in this function to get the value inside the same. user_date = driver.find_elements_by_xpath('//*[@id=\"Comment_5561090\"]/div/div[2]/div[2]/span[1]/a/time')[0] date = user_date.get_attribute('title') . | Comments: Lastly, let’s explore how to extract the comments of each user. | . Below is the XPath for the user comment. //*[@id=\"Comment_5561090\"]/div/div[3]/div/div[1] . Once again, we have the comment id in our XPath. Similar to the userid we will extract the comment from the above XPath . user_message = driver.find_elements_by_xpath('//*[@id=\"Comment_5561090\"]/div/div[3]/div/div[1]')[0] comment = user_message.text . We just learnt how to scrape different elements from a web page. Now how to recursively extract these items for 5000 users? . As discussed above, we will use the comment ids, which are unique for a comment to extract different users data. If we see the XPath for the entire comment block, we will see that it has a comment id associated with it. //*[@id=\"Comment_5561090\"] . The following code snippet will help us extract all the comment ids on a particular web page. We will again use the function ‘find_elements_by_xpath’ on the above XPath and extract the ids from the ‘id’ attribute. This code gives us a list of all the comment ids from a particular web page. ids = driver.find_elements_by_xpath(\"//*[contains(@id,'Comment_')]\") comment_ids = [] for i in ids: comment_ids.append(i.get_attribute('id')) . /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_23762/2346684339.py:1: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead ids = driver.find_elements_by_xpath(\"//*[contains(@id,'Comment_')]\") . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.b%20Web%20Scraping%20Using%20Selenium.html#lets-begin",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.b%20Web%20Scraping%20Using%20Selenium.html#lets-begin"
  },"242": {
    "doc": "03.b Web Scraping Using Selenium",
    "title": "How to bring all this together?",
    "content": "Now we will bring all the things we have seen so far into one big code, which will recursively help us extract 5000 comments. We can extract user ids, date and comments for each user on a particular web page by looping through all the comment ids we found in the previous code. Below is the code snippet to extract all comments from a particular web page. driver = webdriver.Edge(EdgeChromiumDriverManager().install()) comments = pd.DataFrame(columns = ['Date','user_id','comments']) comment_ids = [] for page in range(1, 11, 1): driver.get(f'https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p{page}') ids = driver.find_elements_by_xpath(\"//*[contains(@id,'Comment_')]\") for i in ids: comment_ids.append(i.get_attribute('id')) for x in comment_ids: #Extract dates from for each user on a page user_date = driver.find_elements_by_xpath('//*[@id=\"' + x +'\"]/div/div[2]/div[2]/span[1]/a/time')[0] date = user_date.get_attribute('title') #Extract user ids from each user on a page userid_element = driver.find_elements_by_xpath('//*[@id=\"' + x +'\"]/div/div[2]/div[1]/span[1]/a[2]')[0] userid = userid_element.text #Extract Message for each user on a page user_message = driver.find_elements_by_xpath('//*[@id=\"' + x +'\"]/div/div[3]/div/div[1]')[0] comment = user_message.text #Adding date, userid and comment for each user in a dataframe comments.loc[len(comments)] = [date,userid,comment] . [WDM] - ====== WebDriver manager ====== [WDM] - Driver [/Users/bk/.wdm/drivers/edgedriver/mac64/94.0.992.50/msedgedriver] found in cache /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_23762/3028486432.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object driver = webdriver.Edge(EdgeChromiumDriverManager().install()) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_23762/3028486432.py:8: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead ids = driver.find_elements_by_xpath(\"//*[contains(@id,'Comment_')]\") /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_23762/3028486432.py:14: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead user_date = driver.find_elements_by_xpath('//*[@id=\"' + x +'\"]/div/div[2]/div[2]/span[1]/a/time')[0] /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_23762/3028486432.py:18: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead userid_element = driver.find_elements_by_xpath('//*[@id=\"' + x +'\"]/div/div[2]/div[1]/span[1]/a[2]')[0] /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_23762/3028486432.py:22: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead user_message = driver.find_elements_by_xpath('//*[@id=\"' + x +'\"]/div/div[3]/div/div[1]')[0] --------------------------------------------------------------------------- IndexError Traceback (most recent call last) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_23762/3028486432.py in &lt;module&gt; 12 for x in comment_ids: 13 #Extract dates from for each user on a page ---&gt; 14 user_date = driver.find_elements_by_xpath('//*[@id=\"' + x +'\"]/div/div[2]/div[2]/span[1]/a/time')[0] 15 date = user_date.get_attribute('title') 16 IndexError: list index out of range . comments.head() . | | Date | user_id | comments | . | 0 | March 25, 2002 5:54AM | merc1 | I personally think that with a few tweaks the ... | . | 1 | March 25, 2002 7:06AM | fredvh | I am debating a new purchase and these two are... | . | 2 | March 25, 2002 5:02PM | blueguydotcom | Great handling, RWD, excellent engine and the ... | . | 3 | March 25, 2002 11:04PM | hungrywhale | And no manual tranny. That may not matter to y... | . | 4 | March 26, 2002 12:44AM | riez | One beauty of BMW 3 Series is that there are s... | . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.b%20Web%20Scraping%20Using%20Selenium.html#how-to-bring-all-this-together",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.b%20Web%20Scraping%20Using%20Selenium.html#how-to-bring-all-this-together"
  },"243": {
    "doc": "03.b Web Scraping Using Selenium",
    "title": "03.b Web Scraping Using Selenium",
    "content": " ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.b%20Web%20Scraping%20Using%20Selenium.html",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.b%20Web%20Scraping%20Using%20Selenium.html"
  },"244": {
    "doc": "03.c Web scraping with Scrapy",
    "title": "Web scraping with Scrapy",
    "content": "source . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html#web-scraping-with-scrapy",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html#web-scraping-with-scrapy"
  },"245": {
    "doc": "03.c Web scraping with Scrapy",
    "title": "Introduction",
    "content": "Web scraping, often called web crawling or web spidering, or “programmatically going over a collection of web pages and extracting data,” is a powerful tool for working with data on the web. With a web scraper, you can mine data about a set of products, get a large corpus of text or quantitative data to play around with, get data from a site without an official API, or just satisfy your own personal curiosity. In this tutorial, you’ll learn about the fundamentals of the scraping and spidering process as you explore a playful data set. We’ll use BrickSet, a community-run site that contains information about LEGO sets. By the end of this tutorial, you’ll have a fully functional Python web scraper that walks through a series of pages on Brickset and extracts data about LEGO sets from each page, displaying the data to your screen. The scraper will be easily expandable so you can tinker around with it and use it as a foundation for your own projects scraping data from the web. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html#introduction",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html#introduction"
  },"246": {
    "doc": "03.c Web scraping with Scrapy",
    "title": "Prerequisites",
    "content": "To complete this tutorial, you’ll need a local development environment for Python 3. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html#prerequisites",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html#prerequisites"
  },"247": {
    "doc": "03.c Web scraping with Scrapy",
    "title": "Step 1 — Creating a Basic Scraper",
    "content": "Scraping is a two step process: . | You systematically find and download web pages. | You take those web pages and extract information from them. | . Both of those steps can be implemented in a number of ways in many languages. You can build a scraper from scratch using modules or libraries provided by your programming language, but then you have to deal with some potential headaches as your scraper grows more complex. For example, you’ll need to handle concurrency so you can crawl more than one page at a time. You’ll probably want to figure out how to transform your scraped data into different formats like CSV, XML, or JSON. And you’ll sometimes have to deal with sites that require specific settings and access patterns. You’ll have better luck if you build your scraper on top of an existing library that handles those issues for you. For this tutorial, we’re going to use Python and Scrapy to build our scraper. Scrapy . Scrapy is one of the most popular and powerful Python scraping libraries; it takes a “batteries included” approach to scraping, meaning that it handles a lot of the common functionality that all scrapers need so developers don’t have to reinvent the wheel each time. It makes scraping a quick and fun process! . Scrapy, like most Python packages, is on PyPI (also known as pip). PyPI, the Python Package Index, is a community-owned repository of all published Python software. If you have a Python installation like the one outlined in the prerequisite for this tutorial, you already have pip installed on your machine, so you can install Scrapy with the following command: . !pip install scrapy . If you run into any issues with the installation, or you want to install Scrapy without using pip, check out the official installation docs. With Scrapy installed, let’s create a new folder for our project. You can do this in the terminal by running: . !mkdir scrapers . mkdir: scrapers: File exists . Then create a new Python file for our scraper called scraper.py. We’ll place all of our code in this file for this tutorial. You can create this file in the terminal with the touch command, like this: . !touch scrapers/scraper.py . We’ll start by making a very basic scraper that uses Scrapy as its foundation. To do that, we’ll create a Python class that subclasses scrapy.Spider, a basic spider class provided by Scrapy. This class will have two required attributes: . | name — just a name for the spider. | start_urls — a list of URLs that you start to crawl from. We’ll start with one URL. | . Open the scrapy.py file in your text editor and add this code to create the basic spider: . import scrapy class BrickSetSpider(scrapy.Spider): name = \"brickset_spider\" start_urls = ['http://brickset.com/sets/year-2016'] . or you can copy scrapers/scraper1.py where this has already been done for you. Let’s break this down line by line: . | First, we import scrapy so that we can use the classes that the package provides. | Next, we take the Spider class provided by Scrapy and make a subclass out of it called BrickSetSpider. Think of a subclass as a more specialized form of its parent class. The Spider subclass has methods and behaviors that define how to follow URLs and extract data from the pages it finds, but it doesn’t know where to look or what data to look for. By subclassing it, we can give it that information. | Then we give the spider the name brickset_spider. | Finally, we give our scraper a single URL to start from: http://brickset.com/sets/year-2016. If you open that URL in your browser, it will take you to a search results page, showing the first of many pages containing LEGO sets. | . Now let’s test out the scraper. You typically run Python files by running a command like python path/to/file.py. However, Scrapy comes with its own command line interface to streamline the process of starting a scraper. Start your scraper with the following command: . !scrapy runspider scrapers/scraper1.py . 2021-10-21 11:30:37 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot) 2021-10-21 11:30:37 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.9.7 (default, Sep 16 2021, 08:50:36) - [Clang 10.0.0 ], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l 24 Aug 2021), cryptography 35.0.0, Platform macOS-10.16-x86_64-i386-64bit 2021-10-21 11:30:37 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor 2021-10-21 11:30:37 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True} 2021-10-21 11:30:37 [scrapy.extensions.telnet] INFO: Telnet Password: 69d098b24fbe31f9 2021-10-21 11:30:37 [scrapy.middleware] INFO: Enabled extensions: ['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.telnet.TelnetConsole', 'scrapy.extensions.memusage.MemoryUsage', 'scrapy.extensions.logstats.LogStats'] 2021-10-21 11:30:37 [scrapy.middleware] INFO: Enabled downloader middlewares: ['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware', 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware', 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware', 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware', 'scrapy.downloadermiddlewares.retry.RetryMiddleware', 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware', 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware', 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware', 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware', 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware', 'scrapy.downloadermiddlewares.stats.DownloaderStats'] 2021-10-21 11:30:37 [scrapy.middleware] INFO: Enabled spider middlewares: ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware', 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware', 'scrapy.spidermiddlewares.referer.RefererMiddleware', 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware', 'scrapy.spidermiddlewares.depth.DepthMiddleware'] 2021-10-21 11:30:37 [scrapy.middleware] INFO: Enabled item pipelines: [] 2021-10-21 11:30:37 [scrapy.core.engine] INFO: Spider opened 2021-10-21 11:30:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2021-10-21 11:30:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023 2021-10-21 11:30:37 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://brickset.com/sets/year-2016&gt; (referer: None) 2021-10-21 11:30:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://brickset.com/sets/year-2016&gt;: HTTP status code is not handled or not allowed 2021-10-21 11:30:37 [scrapy.core.engine] INFO: Closing spider (finished) 2021-10-21 11:30:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats: {'downloader/request_bytes': 230, 'downloader/request_count': 1, 'downloader/request_method_count/GET': 1, 'downloader/response_bytes': 2140, 'downloader/response_count': 1, 'downloader/response_status_count/403': 1, 'elapsed_time_seconds': 0.198192, 'finish_reason': 'finished', 'finish_time': datetime.datetime(2021, 10, 21, 15, 30, 37, 904414), 'httpcompression/response_bytes': 3136, 'httpcompression/response_count': 1, 'httperror/response_ignored_count': 1, 'httperror/response_ignored_status_count/403': 1, 'log_count/DEBUG': 1, 'log_count/INFO': 11, 'memusage/max': 228245504, 'memusage/startup': 228245504, 'response_received_count': 1, 'scheduler/dequeued': 1, 'scheduler/dequeued/memory': 1, 'scheduler/enqueued': 1, 'scheduler/enqueued/memory': 1, 'start_time': datetime.datetime(2021, 10, 21, 15, 30, 37, 706222)} 2021-10-21 11:30:37 [scrapy.core.engine] INFO: Spider closed (finished) . That’s a lot of output, so let’s break it down. | The scraper initialized and loaded additional components and extensions it needed to handle reading data from URLs. | It used the URL we provided in the start_urls list and grabbed the HTML, just like your web browser would do. | It passed that HTML to the parse method, which doesn’t do anything by default. Since we never wrote our own parse method, the spider just finishes without doing any work. | . Now let’s pull some data from the page. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html#step-1--creating-a-basic-scraper",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html#step-1--creating-a-basic-scraper"
  },"248": {
    "doc": "03.c Web scraping with Scrapy",
    "title": "Step 2 — Extracting Data from a Page",
    "content": "We’ve created a very basic program that pulls down a page, but it doesn’t do any scraping or spidering yet. Let’s give it some data to extract. If you look at the page we want to scrape, you’ll see it has the following structure: . | There’s a header that’s present on every page. | There’s some top-level search data, including the number of matches, what we’re searching for, and the breadcrumbs for the site. | Then there are the sets themselves, displayed in what looks like a table or ordered list. Each set has a similar format. | . When writing a scraper, it’s a good idea to look at the source of the HTML file and familiarize yourself with the structure. So here it is, with some things removed for readability: . &lt;!-- brickset.com/sets/year-2016 --&gt; &lt;body&gt; &lt;section class=\"setlist\"&gt; &lt;article class='set'&gt; &lt;a href=\"https://images.brickset.com/sets/large/10251-1.jpg?201510121127\" class=\"highslide plain mainimg\" onclick=\"return hs.expand(this)\"&gt;&lt;img src=\"https://images.brickset.com/sets/small/10251-1.jpg?201510121127\" title=\"10251-1: Brick Bank\" onError=\"this.src='/assets/images/spacer.png'\" /&gt;&lt;/a&gt; &lt;div class=\"highslide-caption\"&gt; &lt;h1&gt;Brick Bank&lt;/h1&gt;&lt;div class='tags floatleft'&gt;&lt;a href='/sets/10251-1/Brick- Bank'&gt;10251-1&lt;/a&gt; &lt;a href='/sets/theme-Creator-Expert'&gt;Creator Expert&lt;/a&gt; &lt;a class='subtheme' href='/sets/theme-Creator-Expert/subtheme-Modular- Buildings'&gt;Modular Buildings&lt;/a&gt; &lt;a class='year' href='/sets/theme-Creator- Expert/year-2016'&gt;2016&lt;/a&gt; &lt;/div&gt;&lt;div class='floatright'&gt;&amp;copy;2016 LEGO Group&lt;/div&gt; &lt;div class=\"pn\"&gt; &lt;a href=\"#\" onclick=\"return hs.previous(this)\" title=\"Previous (left arrow key)\"&gt;&amp;#171; Previous&lt;/a&gt; &lt;a href=\"#\" onclick=\"return hs.next(this)\" title=\"Next (right arrow key)\"&gt;Next &amp;#187;&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; ... &lt;/article&gt; &lt;/section&gt; &lt;/body&gt; . Scraping this page is a two step process: . | First, grab each LEGO set by looking for the parts of the page that have the data we want. | Then, for each set, grab the data we want from it by pulling the data out of the HTML tags. | . scrapy grabs data based on selectors that you provide. Selectors are patterns we can use to find one or more elements on a page so we can then work with the data within the element. scrapy supports either CSS selectors or XPath selectors. We’ll use CSS selectors for now since CSS is the easier option and a perfect fit for finding all the sets on the page. If you look at the HTML for the page, you’ll see that each set is specified with the class set. Since we’re looking for a class, we’d use .set for our CSS selector. All we have to do is pass that selector into the response object, like this: . import scrapy class BrickSetSpider(scrapy.Spider): name = \"brickset_spider\" start_urls = ['http://brickset.com/sets/year-2016'] def parse(self, response): SET_SELECTOR = '.set' for brickset in response.css(SET_SELECTOR): pass . This code grabs all the sets on the page and loops over them to extract the data. Now let’s extract the data from those sets so we can display it. Another look at the source of the page we’re parsing tells us that the name of each set is stored within an h1 tag for each set: . brickset.com/sets/year-2016 &lt;h1&gt;Brick Bank&lt;/h1&gt;&lt;div class='tags floatleft'&gt;&lt;a href='/sets/10251-1/Brick-Bank'&gt;10251-1&lt;/a&gt; . The brickset object we’re looping over has its own css method, so we can pass in a selector to locate child elements. Modify your code as follows to locate the name of the set and display it: . import scrapy class BrickSetSpider(scrapy.Spider): name = \"brickset_spider\" start_urls = ['http://brickset.com/sets/year-2016'] def parse(self, response): SET_SELECTOR = '.set' for brickset in response.css(SET_SELECTOR): NAME_SELECTOR = 'h1 ::text' yield { 'name': brickset.css(NAME_SELECTOR).extract_first(), } . Note: The trailing comma after extract_first() isn’t a typo. We’re going to add more to this section soon, so we’ve left the comma there to make adding to this section easier later. You’ll notice two things going on in this code: . | We append ::text to our selector for the name. That’s a CSS pseudo-selector that fetches the text inside of the a tag rather than the tag itself. | We call extract_first() on the object returned by brickset.css(NAME_SELECTOR) because we just want the first element that matches the selector. This gives us a string, rather than a list of elements. | . Save the file as scrapers/scraper2.py and run the scraper again: . !scrapy runspider scrapers/scraper2.py . 2021-10-21 11:30:48 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot) 2021-10-21 11:30:48 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.9.7 (default, Sep 16 2021, 08:50:36) - [Clang 10.0.0 ], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l 24 Aug 2021), cryptography 35.0.0, Platform macOS-10.16-x86_64-i386-64bit 2021-10-21 11:30:48 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor 2021-10-21 11:30:48 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True} 2021-10-21 11:30:48 [scrapy.extensions.telnet] INFO: Telnet Password: 9f07aa540cf71937 2021-10-21 11:30:48 [scrapy.middleware] INFO: Enabled extensions: ['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.telnet.TelnetConsole', 'scrapy.extensions.memusage.MemoryUsage', 'scrapy.extensions.logstats.LogStats'] 2021-10-21 11:30:48 [scrapy.middleware] INFO: Enabled downloader middlewares: ['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware', 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware', 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware', 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware', 'scrapy.downloadermiddlewares.retry.RetryMiddleware', 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware', 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware', 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware', 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware', 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware', 'scrapy.downloadermiddlewares.stats.DownloaderStats'] 2021-10-21 11:30:48 [scrapy.middleware] INFO: Enabled spider middlewares: ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware', 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware', 'scrapy.spidermiddlewares.referer.RefererMiddleware', 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware', 'scrapy.spidermiddlewares.depth.DepthMiddleware'] 2021-10-21 11:30:48 [scrapy.middleware] INFO: Enabled item pipelines: [] 2021-10-21 11:30:48 [scrapy.core.engine] INFO: Spider opened 2021-10-21 11:30:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2021-10-21 11:30:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023 2021-10-21 11:30:48 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://brickset.com/sets/year-2016&gt; (referer: None) 2021-10-21 11:30:48 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://brickset.com/sets/year-2016&gt;: HTTP status code is not handled or not allowed 2021-10-21 11:30:48 [scrapy.core.engine] INFO: Closing spider (finished) 2021-10-21 11:30:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats: {'downloader/request_bytes': 230, 'downloader/request_count': 1, 'downloader/request_method_count/GET': 1, 'downloader/response_bytes': 2139, 'downloader/response_count': 1, 'downloader/response_status_count/403': 1, 'elapsed_time_seconds': 0.173081, 'finish_reason': 'finished', 'finish_time': datetime.datetime(2021, 10, 21, 15, 30, 48, 436068), 'httpcompression/response_bytes': 3136, 'httpcompression/response_count': 1, 'httperror/response_ignored_count': 1, 'httperror/response_ignored_status_count/403': 1, 'log_count/DEBUG': 1, 'log_count/INFO': 11, 'memusage/max': 226410496, 'memusage/startup': 226410496, 'response_received_count': 1, 'scheduler/dequeued': 1, 'scheduler/dequeued/memory': 1, 'scheduler/enqueued': 1, 'scheduler/enqueued/memory': 1, 'start_time': datetime.datetime(2021, 10, 21, 15, 30, 48, 262987)} 2021-10-21 11:30:48 [scrapy.core.engine] INFO: Spider closed (finished) . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html#step-2--extracting-data-from-a-page",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html#step-2--extracting-data-from-a-page"
  },"249": {
    "doc": "03.c Web scraping with Scrapy",
    "title": "Step 3 — Crawling Multiple Pages",
    "content": "We’ve successfully extracted data from that initial page, but we’re not progressing past it to see the rest of the results. The whole point of a spider is to detect and traverse links to other pages and grab data from those pages too. You’ll notice that the top and bottom of each page has a little right carat (&gt;) that links to the next page of results. Here’s the HTML for that: . &lt;!-- brickset.com/sets/year-2016 --&gt; &lt;ul class=\"pagelength\"&gt; ... &lt;li class=\"next\"&gt; &lt;a href=\"http://brickset.com/sets/year-2017/page-2\"&gt;&amp;#8250;&lt;/a&gt; &lt;/li&gt; &lt;li class=\"last\"&gt; &lt;a href=\"http://brickset.com/sets/year-2016/page-32\"&gt;&amp;#187;&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; . As you can see, there’s a li tag with the class of next, and inside that tag, there’s an a tag with a link to the next page. All we have to do is tell the scraper to follow that link if it exists. Modify your code as follows: . import scrapy class BrickSetSpider(scrapy.Spider): name = 'brick_spider' start_urls = ['http://brickset.com/sets/year-2016'] def parse(self, response): SET_SELECTOR = '.set' for brickset in response.css(SET_SELECTOR): NAME_SELECTOR = 'h1 ::text' PIECES_SELECTOR = './/dl[dt/text() = \"Pieces\"]/dd/a/text()' MINIFIGS_SELECTOR = './/dl[dt/text() = \"Minifigs\"]/dd[2]/a/text()' IMAGE_SELECTOR = 'img ::attr(src)' yield { 'name': brickset.css(NAME_SELECTOR).extract_first(), 'pieces': brickset.xpath(PIECES_SELECTOR).extract_first(), 'minifigs': brickset.xpath(MINIFIGS_SELECTOR).extract_first(), 'image': brickset.css(IMAGE_SELECTOR).extract_first(), } NEXT_PAGE_SELECTOR = '.next a ::attr(href)' next_page = response.css(NEXT_PAGE_SELECTOR).extract_first() if next_page: yield scrapy.Request( response.urljoin(next_page), callback=self.parse ) . First, we define a selector for the “next page” link, extract the first match, and check if it exists. The scrapy.Request is a value that we return saying “Hey, crawl this page”, and callback=self.parse says “once you’ve gotten the HTML from this page, pass it back to this method so we can parse it, extract the data, and find the next page.“ . This means that once we go to the next page, we’ll look for a link to the next page there, and on that page we’ll look for a link to the next page, and so on, until we don’t find a link for the next page. This is the key piece of web scraping: finding and following links. In this example, it’s very linear; one page has a link to the next page until we’ve hit the last page, But you could follow links to tags, or other search results, or any other URL you’d like. Now, if you save your code and run the spider again you’ll see that it doesn’t just stop once it iterates through the first page of sets. It keeps on going through all 779 matches on 23 pages! In the grand scheme of things it’s not a huge chunk of data, but now you know the process by which you automatically find new pages to scrape. Save this as scrapers/scraper3.py and run the file . !scrapy runspider scrapers/scraper3.py . 2021-10-21 11:30:50 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot) 2021-10-21 11:30:50 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.9.7 (default, Sep 16 2021, 08:50:36) - [Clang 10.0.0 ], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l 24 Aug 2021), cryptography 35.0.0, Platform macOS-10.16-x86_64-i386-64bit 2021-10-21 11:30:50 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor 2021-10-21 11:30:50 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True} 2021-10-21 11:30:50 [scrapy.extensions.telnet] INFO: Telnet Password: 49053a2ee67de37f 2021-10-21 11:30:50 [scrapy.middleware] INFO: Enabled extensions: ['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.telnet.TelnetConsole', 'scrapy.extensions.memusage.MemoryUsage', 'scrapy.extensions.logstats.LogStats'] 2021-10-21 11:30:50 [scrapy.middleware] INFO: Enabled downloader middlewares: ['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware', 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware', 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware', 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware', 'scrapy.downloadermiddlewares.retry.RetryMiddleware', 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware', 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware', 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware', 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware', 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware', 'scrapy.downloadermiddlewares.stats.DownloaderStats'] 2021-10-21 11:30:50 [scrapy.middleware] INFO: Enabled spider middlewares: ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware', 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware', 'scrapy.spidermiddlewares.referer.RefererMiddleware', 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware', 'scrapy.spidermiddlewares.depth.DepthMiddleware'] 2021-10-21 11:30:50 [scrapy.middleware] INFO: Enabled item pipelines: [] 2021-10-21 11:30:50 [scrapy.core.engine] INFO: Spider opened 2021-10-21 11:30:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2021-10-21 11:30:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023 2021-10-21 11:30:51 [scrapy.core.engine] DEBUG: Crawled (403) &lt;GET http://brickset.com/sets/year-2016&gt; (referer: None) 2021-10-21 11:30:51 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response &lt;403 http://brickset.com/sets/year-2016&gt;: HTTP status code is not handled or not allowed 2021-10-21 11:30:51 [scrapy.core.engine] INFO: Closing spider (finished) 2021-10-21 11:30:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats: {'downloader/request_bytes': 230, 'downloader/request_count': 1, 'downloader/request_method_count/GET': 1, 'downloader/response_bytes': 2138, 'downloader/response_count': 1, 'downloader/response_status_count/403': 1, 'elapsed_time_seconds': 0.185113, 'finish_reason': 'finished', 'finish_time': datetime.datetime(2021, 10, 21, 15, 30, 51, 160618), 'httpcompression/response_bytes': 3136, 'httpcompression/response_count': 1, 'httperror/response_ignored_count': 1, 'httperror/response_ignored_status_count/403': 1, 'log_count/DEBUG': 1, 'log_count/INFO': 11, 'memusage/max': 228081664, 'memusage/startup': 228081664, 'response_received_count': 1, 'scheduler/dequeued': 1, 'scheduler/dequeued/memory': 1, 'scheduler/enqueued': 1, 'scheduler/enqueued/memory': 1, 'start_time': datetime.datetime(2021, 10, 21, 15, 30, 50, 975505)} 2021-10-21 11:30:51 [scrapy.core.engine] INFO: Spider closed (finished) . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html#step-3--crawling-multiple-pages",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html#step-3--crawling-multiple-pages"
  },"250": {
    "doc": "03.c Web scraping with Scrapy",
    "title": "Conclusion",
    "content": "In this tutorial you built a fully-functional spider that extracts data from web pages in less than thirty lines of code. That’s a great start, but there’s a lot of fun things you can do with this spider. Here are some ways you could expand the code you’ve written. They’ll give you some practice scraping data. | Right now we’re only parsing results from 2016, as you might have guessed from the 2016 part of http://brickset.com/sets/year-2016 — how would you crawl results from other years? | There’s a retail price included on most sets. How do you extract the data from that cell? How would you get a raw number out of it? Hint: you’ll find the data in a dt just like the number of pieces and minifigs. | Most of the results have tags that specify semantic data about the sets or their context. How do we crawl these, given that there are multiple tags for a single set? | . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html#conclusion",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html#conclusion"
  },"251": {
    "doc": "03.c Web scraping with Scrapy",
    "title": "03.c Web scraping with Scrapy",
    "content": " ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/03.c%20Web%20scraping%20with%20Scrapy.html"
  },"252": {
    "doc": "04 - Python Functions",
    "title": "Python Functions",
    "content": "Python Function Tutorial . Functions are used to encapsulate a set of relatred instructions that you want to use within your program to carry out a specific task. This helps with the organization of your code as well as code reusability. Oftent times functions accept parameters and return values, but that’s not always the case. There are three types of functions in Python: . | Built-in functions, such as help() to ask for help, min() to get the minimum value, print() to print an object to the terminal,… You can find an overview with more of these functions here. | User-Defined Functions (UDFs), which are functions that users create to help them out; And | Anonymous functions, which are also called lambda functions because they are not declared with the standard def keyword. | . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html#python-functions",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html#python-functions"
  },"253": {
    "doc": "04 - Python Functions",
    "title": "Functions vs. Methods",
    "content": "A method is a function that’s part of a class which we’ll discuss in another lecture. Keep in mind all methods are functions but not all functions are methods. ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html#functions-vs-methods",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html#functions-vs-methods"
  },"254": {
    "doc": "04 - Python Functions",
    "title": "Parameters vs. Arguments",
    "content": "Parameters are the names used when defining a function or a method, and into which arguments will be mapped. In other words, arguments are the things which are supplied to any function or method call, while the function or method code refers to the arguments by their parameter names. Consider the function . def sum(a, b): return a + b . sum has 2 parameters a and b. If you call the sum function with values 2 and 3 then 2 and 3 are the arguments. ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html#parameters-vs-arguments",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html#parameters-vs-arguments"
  },"255": {
    "doc": "04 - Python Functions",
    "title": "Defining User Functions",
    "content": "The four steps to defining a function in Python are the following: . | Use the keyword def to declare the function and follow this up with the function name. | Add parameters to the function: they should be within the parentheses of the function. End your line with a colon. | Add statements that the functions should execute. | End your function with a return statement if the function should output something. Without the return statement, your function will return an object None. | . # takes no parameters, returns none def say_hello(): print('hello') . say_hello() . hello . x = say_hello() print(type(x), x) . hello &lt;class 'NoneType'&gt; None . # takes parameters, returns none def say_something(message): print(message) . x = say_something('hello class') print(type(x), x) . hello class &lt;class 'NoneType'&gt; None . The return Statement . Sometimes it’s useful to reuturn values from functions. We’ll refactor our code to return values. def get_message(): return 'hello class' def say_something(): message = get_message() print(message) . x = get_message() print(type(x), x) . &lt;class 'str'&gt; hello class . say_something() . hello class . def ask_user_to_say_something(): message = input('say something') print(message) . ask_user_to_say_something() . say something welcome to class welcome to class . def say_anything(fn): message = fn() print(message) . fn = get_message say_anything(fn) . hello class . fn = input say_anything(fn) . python python . print(type(say_something())) . hello class &lt;class 'NoneType'&gt; . x = say_something() print(type(x), x) . hello class &lt;class 'NoneType'&gt; None . returning multiple values . In python you can return values in a variety of data types including primitive data structures such as integers, floats, strings, &amp; booleans as well as non-primitive data structures such as arrays, lists, tuples, dictionaries, sets, and files. # returning a list def get_messages(): return ['hello class', 'python is great', 'here we\\'re retuning a list'] messages = get_messages() print(type(messages), messages) for message in messages: print(type(message), message) . &lt;class 'list'&gt; ['hello class', 'python is great', \"here we're retuning a list\"] &lt;class 'str'&gt; hello class &lt;class 'str'&gt; python is great &lt;class 'str'&gt; here we're retuning a list . # returning a tuple... more on tuples later def get_message(): return ('hello class', 3) message = get_message() print(type(message), message) for i in range(0, message[1]): print(message[0]) . &lt;class 'tuple'&gt; ('hello class', 3) hello class hello class hello class . def get_message(): return 'hello class', 3 # ('s are optional message = get_message() print(type(message), message) for i in range(0, message[1]): print(message[0]) . &lt;class 'tuple'&gt; ('hello class', 3) hello class hello class hello class . message, iterations = get_message() print(type(message), message) for i in range(0, iterations): print(message) . &lt;class 'str'&gt; hello class hello class hello class hello class . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html#defining-user-functions",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html#defining-user-functions"
  },"256": {
    "doc": "04 - Python Functions",
    "title": "Function Arguments in Python",
    "content": "There are four types of arguments that Python functions can take: . | Default arguments | Required arguments | Keyword arguments | Variable number of arguments | . Default Arguments . Default arguments are those that take a default value if no argument value is passed during the function call. You can assign this default value by with the assignment operator =, just like in the following example: . import random def get_random_numbers(n=1): if n == 1: return random.random() elif n &gt; 1: numbers = [] for i in range(0, n): numbers.append(random.random()) return numbers w = get_random_numbers() print('w:', type(w), w) x = get_random_numbers(1) print('x:', type(x), x) y = get_random_numbers(n=3) print('y:', type(y), y) z = get_random_numbers(n=-1) print('z:', type(z), z) . w: &lt;class 'float'&gt; 0.9910565700695637 x: &lt;class 'float'&gt; 0.9558550589791632 y: &lt;class 'list'&gt; [0.42626170162229604, 0.5088767993123806, 0.8323423336075707] z: &lt;class 'NoneType'&gt; None . # note : this might be a better implementation def get_random_numbers(n=1): if n == 1: return [random.random()] elif n &gt; 1: numbers = [] for i in range(0, n): numbers.append(random.random()) return numbers else: return [] w = get_random_numbers() print('w:', type(w), w) x = get_random_numbers(1) print('x:', type(x), x) y = get_random_numbers(n=3) print('y:', type(y), y) z = get_random_numbers(n=-1) print('z:', type(z), z) . w: &lt;class 'list'&gt; [0.1019992349979667] x: &lt;class 'list'&gt; [0.773928378946725] y: &lt;class 'list'&gt; [0.6769995705574616, 0.5620051030258161, 0.6386635457018367] z: &lt;class 'list'&gt; [] . Required Arguments . Required arguments are mandatory and you will generate an error if they’re not present. Required arguments must be passed in precisely the right order, just like in the following example: . def say_something(message, number_of_times): for i in range(0, number_of_times): print(message) # arguments passed in the proper order say_something('hello', 3) . hello hello hello . # arguments passed incorrectly say_something(3, 'hello') . --------------------------------------------------------------------------- TypeError Traceback (most recent call last) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_91274/301183919.py in &lt;module&gt; 1 # arguments passed incorrectly ----&gt; 2 say_something(3, 'hello') /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_91274/2017276285.py in say_something(message, number_of_times) 1 def say_something(message, number_of_times): ----&gt; 2 for i in range(0, number_of_times): 3 print(message) 4 5 # arguments passed in the proper order TypeError: 'str' object cannot be interpreted as an integer . Keyword Arguments . You can use keyword arguments to make sure that you call all the parameters in the right order. You can do so by specifying their parameter name in the function call. say_something(message='hello', number_of_times=3) say_something(number_of_times=3, message='hello') . hello hello hello hello hello hello . Variable Number of Arguments . In cases where you don’t know the exact number of arguments that you want to pass to a function, you can use the following syntax with *args: . def add(*x): print(type(x), x) total = 0 for i in x: total += i return total total = add(1) print(total) total = add(1, 1) print(total) total = add(1, 2, 3, 4, 5) print(total) . &lt;class 'tuple'&gt; (1,) 1 &lt;class 'tuple'&gt; (1, 1) 2 &lt;class 'tuple'&gt; (1, 2, 3, 4, 5) 15 . The asterisk * is placed before the variable name that holds the values of all nonkeyword variable arguments. Note here that you might as well have passed *varint, *var_int_args or any other name to the plus() function. # You can spedify any combination of required, keyword, and variable arguments. def add(a, b, *args): total = a + b for arg in args: total += arg return total total = add(1, 1) print(total) total = add(1, 1, 2) print(total) total = add(1, 2, 3, 4, 5) print(total) . 2 4 15 . Global vs Local Variables . In general, variables that are defined inside a function body have a local scope, and those defined outside have a global scope. That means that local variables are defined within a function block and can only be accessed inside that function, while global variables can be obtained by all functions that might be in your script: . # global variable score = 0 def player_hit(): global score hit_points = -10 # local variable score += hit_points def enemy_hit(): global score hit_points = 5 # local variable score += hit_points enemy_hit() enemy_hit() enemy_hit() enemy_hit() player_hit() enemy_hit() player_hit() print(score) . 5 . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html#function-arguments-in-python",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html#function-arguments-in-python"
  },"257": {
    "doc": "04 - Python Functions",
    "title": "Anonymous Functions in Python",
    "content": "Anonymous functions are also called lambda functions in Python because instead of declaring them with the standard def keyword, you use the lambda keyword. def double(x): return x * 2 y = double(3) print(y) . 6 . d = lambda x: x * 2 . y = d(3) print(y) . 6 . sdlfjsdk = lambda x, n: x if n &lt; 5 else 0 result = sdlfjsdk(4, 6) print(result) . 0 . a = lambda x: x ** 2 if x &lt; 0 else x print(a(-1)) print(a(-2)) print(a(3)) . 1 4 3 . add = lambda x, y: x + y . x = add(2, 3) print(x) . 5 . You use anonymous functions when you require a nameless function for a short period of time, and that is created at runtime. Specific contexts in which this would be relevant is when you’re working with filter(), map() and reduce(): . | filter() function filters the original input list on the basis of a criterion &gt; 10. | map() applies a function to all items of the list | reduce() is part of the functools library. You use this function cumulatively to the items of the my_list list, from left to right and reduce the sequence to a single value. | . from functools import reduce my_list = [1,2,3,4,5,6,7,8,9,10] # Use lambda function with `filter()` filtered_list = list(filter(lambda x: (x * 2 &gt; 10), my_list)) # Use lambda function with `map()` mapped_list = list(map(lambda x: x * 2, my_list)) # Use lambda function with `reduce()` reduced_list = reduce(lambda x, y: x + y, my_list) print(filtered_list) print(mapped_list) print(reduced_list) . [6, 7, 8, 9, 10] [2, 4, 6, 8, 10, 12, 14, 16, 18, 20] 55 . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html#anonymous-functions-in-python",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html#anonymous-functions-in-python"
  },"258": {
    "doc": "04 - Python Functions",
    "title": "Using main() as a Function",
    "content": "You can easily define a main() function and call it just like you have done with all of the other functions above: . # Define `main()` function def main(): print(\"This is a main function\") main() . This is a main function . However, as it stands now, the code of your main() function will be called when you import it as a module. To make sure that this doesn’t happen, you call the main() function when __name__ == '__main__'. # Define `main()` function def start_here(): print(\"This is a main function\") # Execute `main()` function if __name__ == '__main__': start_here() . This is a main function . ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html#using-main-as-a-function",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html#using-main-as-a-function"
  },"259": {
    "doc": "04 - Python Functions",
    "title": "04 - Python Functions",
    "content": " ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics/04%20-%20Python%20Functions.html"
  },"260": {
    "doc": "04 - Python Modules and Packages",
    "title": "Python Modules",
    "content": "Python Modules . A module allows you to logically organize your Python code. Grouping related code into a module makes the code easier to understand and use. A module is a Python object with arbitrarily named attributes that you can bind and reference. Simply, a module is a file consisting of Python code. A module can define functions, classes and variables. A module can also include runnable code. The import Statement . You can use any Python source file as a module by executing an import statement in some other Python source file. The import has the following syntax . import module1[, module2[,... moduleN] . When the interpreter encounters an import statement, it imports the module if the module is present in the search path. A search path is a list of directories that the interpreter searches before importing a module. For example, to import the module support.py, you need to put the following command at the top of the script . # Import module support import support # Now you can call defined function that module as follows support.print_func(\"drk\") . Hello : drk . A module is loaded only once, regardless of the number of times it is imported. This prevents the module execution from happening over and over again if multiple imports occur. The from…import Statement . Python’s from statement lets you import specific attributes from a module into the current namespace. The from…import has the following syntax . from modname import name1[, name2[, ... nameN]] . For example, to import the function fibonacci from the module fib, use the following statement − . from fib import fibonacci . This statement does not import the entire module fib into the current namespace; it just introduces the item fibonacci from the module fib into the global symbol table of the importing module. The from…import * Statement . It is also possible to import all names from a module into the current namespace by using the following import statement . from modname import * . This provides an easy way to import all the items from a module into the current namespace; however, this statement should be used sparingly. Locating Modules . When you import a module, the Python interpreter searches for the module in the following sequences − . | The current directory. | If the module isn’t found, Python then searches each directory in the shell variable PYTHONPATH. | If all else fails, Python checks the default path. On UNIX, this default path is normally /usr/local/lib/python/. | . The module search path is stored in the system module sys as the sys.path variable. The sys.path variable contains the current directory, PYTHONPATH, and the installation-dependent default. ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/04%20-%20Python%20Modules%20and%20Packages.html#python-modules",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/04%20-%20Python%20Modules%20and%20Packages.html#python-modules"
  },"261": {
    "doc": "04 - Python Modules and Packages",
    "title": "Python Packages",
    "content": "A package is a hierarchical file directory structure that defines a single Python application environment that consists of modules and subpackages and sub-subpackages, and so on. Consider a file Pots.py available in Phone directory. This file has following line of source code − . def Pots(): print \"I'm Pots Phone\" . Similar way, we have another two files having different functions with the same name as above − . | Phone/Isdn.py file having function Isdn() | Phone/G3.py file having function G3() | Now, create one more file init.py in Phone directory | . To make all of your functions available when you’ve imported Phone, you need to put explicit import statements in init.py as follows . from Pots import Pots from Isdn import Isdn from G3 import G3 . After you add these lines to init.py, you have all of these classes available when you import the Phone package. # Now import your Phone Package. import Phone as p . p.Pots() . I'm a Pots Phone. p.Isdn() . I'm an Isdn phone. p.G3() . I'm a G3 phone. ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/04%20-%20Python%20Modules%20and%20Packages.html#python-packages",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/04%20-%20Python%20Modules%20and%20Packages.html#python-packages"
  },"262": {
    "doc": "04 - Python Modules and Packages",
    "title": "04 - Python Modules and Packages",
    "content": " ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/04%20-%20Python%20Modules%20and%20Packages.html",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/04%20-%20Python%20Modules%20and%20Packages.html"
  },"263": {
    "doc": "04 - Reading synthea data",
    "title": "Reading Synthea Data",
    "content": "Here we’ll walk through the different data files from the synthea data generator. Synthea can generate data in a number of formats including: . | CCDA | CSV | FHIR | Text | . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/04%20-%20Reading%20synthea%20data.html#reading-synthea-data",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/04%20-%20Reading%20synthea%20data.html#reading-synthea-data"
  },"264": {
    "doc": "04 - Reading synthea data",
    "title": "Reading Text Data",
    "content": "with open('../data/text/Abe604_Veum823_e841a5e8-9ace-437b-be32-b37d006aef87.txt') as f: for idx, line in enumerate(f.readlines()): print(f'{idx}: {line}') if idx &gt;= 5: break . 0: Abe604 Veum823 1: ============== 2: Race: Asian 3: Ethnicity: Non-Hispanic 4: Gender: M 5: Age: 69 . # find all lines . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/04%20-%20Reading%20synthea%20data.html#reading-text-data",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/04%20-%20Reading%20synthea%20data.html#reading-text-data"
  },"265": {
    "doc": "04 - Reading synthea data",
    "title": "Reading CSV Data",
    "content": "with open('../data/csv/providers.csv') as f: for idx, line in enumerate(f.readlines()): print(line) if idx &gt;= 5: break . Id,ORGANIZATION,NAME,GENDER,SPECIALITY,ADDRESS,CITY,STATE,ZIP,UTILIZATION 4f073dcc-c92a-455b-8b0c-be967da311b8,ef58ea08-d883-3957-8300-150554edc8fb,Noe500 Dibbert990,M,GENERAL PRACTICE,60 HOSPITAL ROAD,LEOMINSTER,MA,01453,362 7066c8e7-c63a-4de5-a6ed-2fe78ba2d484,69176529-fd1f-3b3f-abce-a0a3626769eb,Mariam937 Gleason633,F,GENERAL PRACTICE,330 MOUNT AUBURN STREET,CAMBRIDGE,MA,02138,334 2d6d2a74-e052-4546-8173-ac72a39b7365,5e765f2b-e908-3888-9fc7-df2cb87beb58,Dagny669 Schoen8,F,GENERAL PRACTICE,211 PARK STREET,ATTLEBORO,MA,02703,77 66ab043d-06d1-4f21-b837-2d74448feea7,f1fbcbfb-fcfa-3bd2-b7f4-df20f1b3c3a4,Tyron580 Torphy630,M,GENERAL PRACTICE,ONE GENERAL STREET,LAWRENCE,MA,01842,359 4e37e414-41b9-467f-be47-4293b6dea918,e002090d-4e92-300e-b41e-7d1f21dee4c6,Loren192 Fay398,M,GENERAL PRACTICE,1493 CAMBRIDGE STREET,CAMBRIDGE,MA,02138,7 . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/04%20-%20Reading%20synthea%20data.html#reading-csv-data",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/04%20-%20Reading%20synthea%20data.html#reading-csv-data"
  },"266": {
    "doc": "04 - Reading synthea data",
    "title": "Reading XML Data",
    "content": "with open('../data/ccda/Abe604_Veum823_e841a5e8-9ace-437b-be32-b37d006aef87.xml') as f: for idx, line in enumerate(f.readlines()): print(f'{idx}: {line}') if idx &gt;= 5: break . 0: &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; 1: &lt;ClinicalDocument xmlns=\"urn:hl7-org:v3\" xmlns:sdtc=\"urn:hl7-org:sdtc\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"urn:hl7-org:v3 http://xreg2.nist.gov:8080/hitspValidation/schema/cdar2c32/infrastructure/cda/C32_CDA.xsd\"&gt; 2: &lt;realmCode code=\"US\"/&gt; 3: &lt;typeId root=\"2.16.840.1.113883.1.3\" extension=\"POCD_HD000040\"/&gt; 4: &lt;templateId root=\"2.16.840.1.113883.10.20.22.1.1\" extension=\"2015-08-01\"/&gt; 5: &lt;templateId root=\"2.16.840.1.113883.10.20.22.1.2\" extension=\"2015-08-01\"/&gt; . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/04%20-%20Reading%20synthea%20data.html#reading-xml-data",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/04%20-%20Reading%20synthea%20data.html#reading-xml-data"
  },"267": {
    "doc": "04 - Reading synthea data",
    "title": "Reading JSON Data",
    "content": "with open('../data/fhir/Abe604_Veum823_e841a5e8-9ace-437b-be32-b37d006aef87.json') as f: for idx, line in enumerate(f.readlines()): print(f'{idx}: {line}') if idx &gt;= 5: break . 0: { 1: \"resourceType\": \"Bundle\", 2: \"type\": \"transaction\", 3: \"entry\": [ 4: { 5: \"fullUrl\": \"urn:uuid:df5f01e0-810b-4379-be90-bf53a6b3563d\", . ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/04%20-%20Reading%20synthea%20data.html#reading-json-data",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/04%20-%20Reading%20synthea%20data.html#reading-json-data"
  },"268": {
    "doc": "04 - Reading synthea data",
    "title": "04 - Reading synthea data",
    "content": " ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files/04%20-%20Reading%20synthea%20data.html",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files/04%20-%20Reading%20synthea%20data.html"
  },"269": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "The Ultimate Python Seaborn Tutorial: Gotta Catch ‘Em All",
    "content": "Source . In this step-by-step Seaborn tutorial, you’ll learn how to use one of Python’s most convenient libraries for data visualization. For those who’ve tinkered with Matplotlib before, you may have wondered, “why does it take me 10 lines of code just to make a decent-looking histogram?” . Well, if you’re looking for a simpler way to plot attractive charts, then you’ll love Seaborn. We’ll walk you through everything you need to know to get started, and we’ll use a fun Pokémon dataset (which you can download below). ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#the-ultimate-python-seaborn-tutorial-gotta-catch-em-all",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#the-ultimate-python-seaborn-tutorial-gotta-catch-em-all"
  },"270": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "Introduction to Seaborn",
    "content": "Seaborn provides a high-level interface to Matplotlib, a powerful but sometimes unwieldy Python visualization library. On Seaborn’s official website, they state: . If matplotlib “tries to make easy things easy and hard things possible”, seaborn tries to make a well-defined set of hard things easy too. We’ve found this to be a pretty good summary of Seaborn’s strengths. In practice, the “well-defined set of hard things” includes: . | Using default themes that are aesthetically pleasing. | Setting custom color palettes. | Making attractive statistical plots. | Easily and flexibly displaying distributions. | Visualizing information from matrices and DataFrames. | . Those last three points are why Seaborn is our tool of choice for Exploratory Analysis. It makes it very easy to “get to know” your data quickly and efficiently. However, Seaborn is a complement, not a substitute, for Matplotlib. There are some tweaks that still require Matplotlib, and we’ll cover how to do that as well. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#introduction-to-seaborn",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#introduction-to-seaborn"
  },"271": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "How to Learn Seaborn, the Self-Starter Way:",
    "content": "While Seaborn simplifies data visualization in Python, it still has many features. Therefore, the best way to learn Seaborn is to learn by doing. | First, understand the basics and paradigms of the library. | Each library approaches data visualization differently, so it’s important to understand how Seaborn “thinks about” the problem. | . | Then, fire up a dataset for practice. | Learning in context is the best way to master a new skill quickly. | . | Finally, refer to galleries to spark ideas and documentation to customize your charts. | Since you’ve already learned the library’s paradigms and had some hands-on practice, you’ll easily find what you need. | . | . This process will give you intuition about what you can do with Seaborn, leaving documentation to serve as further guidance. This is the fastest way to go from zero to proficient. A quick tip before we begin: . We tried to make this tutorial as streamlined as possible, which means we won’t go into too much detail for any one topic. It’s helpful to have the Seaborn documentation open beside you, in case you want to learn more about a feature. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#how-to-learn-seaborn-the-self-starter-way",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#how-to-learn-seaborn-the-self-starter-way"
  },"272": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "Seaborn Tutorial Contents",
    "content": "Instead of just showing you how to make a bunch of plots, we’re going to walk through the most important paradigms of the Seaborn library. Along the way, we’ll illustrate each concept with examples. Here are the steps we’ll cover in this tutorial: . | Installing Seaborn. | Importing libraries and dataset. | Seaborn’s plotting functions. | Scatter Plot | . | Customizing with Matplotlib. | The role of Pandas. | Box Plot | . | Seaborn themes. | Violin Plot | . | Color palettes. | Swarm Plot | . | Overlaying plots. | Putting it all together. | Pokédex (mini-gallery). | Heatmap | Histogram | Bar Plot | Factor Plot | Density Plot | Joint Distribution Plot | . | . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#seaborn-tutorial-contents",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#seaborn-tutorial-contents"
  },"273": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "Step 1: Installing Seaborn.",
    "content": ". | Python 3 | Pandas | Matplotlib | Seaborn | Jupyter Notebook (optional, but recommended) | . We strongly recommend installing the Anaconda Distribution, which comes with all of those packages. Simply follow the instructions on that download page. Once you have Anaconda installed, simply start Jupyter (either through the command line or the Navigator app) and open a new notebook . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-1-installing-seaborn",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-1-installing-seaborn"
  },"274": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "Step 2: Importing libraries and dataset.",
    "content": "Let’s start by importing Pandas, which is a great library for managing relational (i.e. table-format) datasets: . # disable warnings for lecture import warnings warnings.filterwarnings('ignore') . # Pandas for managing datasets import pandas as pd . Next, we’ll import Matplotlib, which will help us customize our plots further. | Tip: In Jupyter Notebook, you can also include %matplotlib inline to display your plots inside your notebook. | . # Matplotlib for additional customization from matplotlib import pyplot as plt %matplotlib inline . Then, we’ll import the Seaborn library, which is the star of today’s show. # Seaborn for plotting and styling import seaborn as sns . Now we’re ready to import our dataset. | Tip: we gave each of our imported libraries an alias. Later, we can invoke Pandas with pd, Matplotlib with plt, and Seaborn with sns. | . Today, we’ll be using a cool Pokémon dataset (first generation). Here’s the free download: . Dataset for this tutorial. | Pokemon.csv | . Once you’ve downloaded the CSV file, you can import it with Pandas. | Tip: The argument index_col=0 simply means we’ll treat the first column of the dataset as the ID column. | . # Read dataset df = pd.read_csv('Pokemon.csv', index_col=0) . # Display first 5 observations df.head() . | | Name | Type 1 | Type 2 | Total | HP | Attack | Defense | Sp. Atk | Sp. Def | Speed | Stage | Legendary | . | # | | | | | | | | | | | | | . | 1 | Bulbasaur | Grass | Poison | 318 | 45 | 49 | 49 | 65 | 65 | 45 | 1 | False | . | 2 | Ivysaur | Grass | Poison | 405 | 60 | 62 | 63 | 80 | 80 | 60 | 2 | False | . | 3 | Venusaur | Grass | Poison | 525 | 80 | 82 | 83 | 100 | 100 | 80 | 3 | False | . | 4 | Charmander | Fire | NaN | 309 | 39 | 52 | 43 | 60 | 50 | 65 | 1 | False | . | 5 | Charmeleon | Fire | NaN | 405 | 58 | 64 | 58 | 80 | 65 | 80 | 2 | False | . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-2-importing-libraries-and-dataset",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-2-importing-libraries-and-dataset"
  },"275": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "Step 3: Seaborn’s plotting functions.",
    "content": "One of Seaborn’s greatest strengths is its diversity of plotting functions. For instance, making a scatter plot is just one line of code using the lmplot() function. There are two ways you can do so. | The first way (recommended) is to pass your DataFrame to the data= argument, while passing column names to the axes arguments, x= and y=. | The second way is to directly pass in Series of data to the axes arguments. | . For example, let’s compare the Attack and Defense stats for our Pokémon: . # Recommended way sns.lmplot(x='Attack', y='Defense', data=df) . &lt;seaborn.axisgrid.FacetGrid at 0x7fd6c805d910&gt; . By the way, Seaborn doesn’t have a dedicated scatter plot function, which is why you see a diagonal line. We actually used Seaborn’s function for fitting and plotting a regression line. Thankfully, each plotting function has several useful options that you can set. Here’s how we can tweak the lmplot(): . | First, we’ll set fit_reg=False to remove the regression line, since we only want a scatter plot. | Then, we’ll set hue='Stage' to color our points by the Pokémon’s evolution stage. This hue argument is very useful because it allows you to express a third dimension of information using color. | . # Scatterplot arguments sns.lmplot(x='Attack', y='Defense', data=df, fit_reg=False, # No regression line hue='Stage') # Color by evolution stage . &lt;seaborn.axisgrid.FacetGrid at 0x7fd71abc7130&gt; . # Scatterplot arguments sns.lmplot(x='Attack', y='Defense', data=df, fit_reg=False, # No regression line hue='Type 1') # Color by evolution stage . &lt;seaborn.axisgrid.FacetGrid at 0x7fd719a289d0&gt; . Looking better, but we can improve this scatter plot further. For example, all of our Pokémon have positive Attack and Defense values, yet our axes limits fall below zero. Let’s see how we can fix that… . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-3-seaborns-plotting-functions",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-3-seaborns-plotting-functions"
  },"276": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "Step 4: Customizing with Matplotlib.",
    "content": "Remember, Seaborn is a high-level interface to Matplotlib. From our experience, Seaborn will get you most of the way there, but you’ll sometimes need to bring in Matplotlib. Setting your axes limits is one of those times, but the process is pretty simple: . | First, invoke your Seaborn plotting function as normal. | Then, invoke Matplotlib’s customization functions. In this case, we’ll use its ylim() and xlim() functions. | . Here’s our new scatter plot with sensible axes limits: . # Plot using Seaborn sns.lmplot(x='Attack', y='Defense', data=df, fit_reg=False, hue='Stage') # Tweak using Matplotlib plt.ylim(0, None) plt.xlim(0, None) . (0.0, 140.45) . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-4-customizing-with-matplotlib",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-4-customizing-with-matplotlib"
  },"277": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "Step 5: The role of Pandas.",
    "content": "Even though this is a Seaborn tutorial, Pandas actually plays a very important role. You see, Seaborn’s plotting functions benefit from a base DataFrame that’s reasonably formatted. For example, let’s say we wanted to make a box plot for our Pokémon’s combat stats: . plt.figure(figsize = (8, 8)) # Boxplot sns.boxplot(data=df) . &lt;AxesSubplot:&gt; . Well, that’s a reasonable start, but there are some columns we’d probably like to remove: . | We can remove the Total since we have individual stats. | We can remove the Stage and Legendary columns because they aren’t combat stats. | . In turns out that this isn’t easy to do within Seaborn alone. Instead, it’s much simpler to pre-format your DataFrame. Let’s create a new DataFrame called stats_df that only keeps the stats columns: . plt.figure(figsize = (8, 8)) # Pre-format DataFrame stats_df = df.drop(['Total', 'Stage', 'Legendary'], axis=1) # New boxplot using stats_df sns.boxplot(data=stats_df) . &lt;AxesSubplot:&gt; . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-5-the-role-of-pandas",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-5-the-role-of-pandas"
  },"278": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "Step 6: Seaborn themes.",
    "content": "Another advantage of Seaborn is that it comes with decent style themes right out of the box. The default theme is called ‘darkgrid’. Next, we’ll change the theme to ‘whitegrid’ while making a violin plot. | Violin plots are useful alternatives to box plots. | They show the distribution (through the thickness of the violin) instead of only the summary statistics. | . For example, we can visualize the distribution of Attack by Pokémon’s primary type: . plt.figure(figsize = (12, 12)) # Set theme sns.set_style('whitegrid') # Violin plot sns.violinplot(x='Type 1', y='Attack', data=df) . &lt;AxesSubplot:xlabel='Type 1', ylabel='Attack'&gt; . As you can see, Dragon types tend to have higher Attack stats than Ghost types, but they also have greater variance. Now, Pokémon fans might find something quite jarring about that plot: The colors are nonsensical. Why is the Grass type colored pink or the Water type colored orange? We must fix this! . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-6-seaborn-themes",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-6-seaborn-themes"
  },"279": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "Step 7: Color palettes.",
    "content": "Fortunately, Seaborn allows us to set custom color palettes. We can simply create an ordered Python list of color hex values. Let’s use Bulbapedia to help us create a new color palette: . pkmn_type_colors = ['#78C850', # Grass '#F08030', # Fire '#6890F0', # Water '#A8B820', # Bug '#A8A878', # Normal '#A040A0', # Poison '#F8D030', # Electric '#E0C068', # Ground '#EE99AC', # Fairy '#C03028', # Fighting '#F85888', # Psychic '#B8A038', # Rock '#705898', # Ghost '#98D8D8', # Ice '#7038F8', # Dragon ] . Wonderful. Now we can simply use the palette= argument to recolor our chart. plt.figure(figsize = (10, 10)) # Violin plot with Pokemon color palette sns.violinplot(x='Type 1', y='Attack', data=df, palette=pkmn_type_colors) # Set color palette . &lt;AxesSubplot:xlabel='Type 1', ylabel='Attack'&gt; . Much better! . Violin plots are great for visualizing distributions. However, since we only have 151 Pokémon in our dataset, we may want to simply display each point. That’s where the swarm plot comes in. This visualization will show each point, while “stacking” those with similar values: . plt.figure(figsize = (10, 10)) # Swarm plot with Pokemon color palette sns.swarmplot(x='Type 1', y='Attack', data=df, palette=pkmn_type_colors) . &lt;AxesSubplot:xlabel='Type 1', ylabel='Attack'&gt; . That’s handy, but can’t we combine our swarm plot and the violin plot? After all, they display similar information, right? . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-7-color-palettes",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-7-color-palettes"
  },"280": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "Step 8: Overlaying plots.",
    "content": "The answer is yes. It’s pretty straightforward to overlay plots using Seaborn, and it works the same way as with Matplotlib. Here’s what we’ll do: . | First, we’ll make our figure larger using Matplotlib. | Then, we’ll plot the violin plot. However, we’ll set inner=None to remove the bars inside the violins. | Next, we’ll plot the swarm plot. This time, we’ll make the points black so they pop out more. | Finally, we’ll set a title using Matplotlib. | . # Set figure size with matplotlib plt.figure(figsize=(10,6)) # Create plot sns.violinplot(x='Type 1', y='Attack', data=df, inner=None, # Remove the bars inside the violins palette=pkmn_type_colors) sns.swarmplot(x='Type 1', y='Attack', data=df, color='k', # Make points black alpha=0.7) # and slightly transparent # Set title with matplotlib plt.title('Attack by Type') . Text(0.5, 1.0, 'Attack by Type') . Awesome, now we have a pretty chart that tells us how Attack values are distributed across different Pokémon types. But what it we want to see all of the other stats as well? . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-8-overlaying-plots",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-8-overlaying-plots"
  },"281": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "Step 9: Putting it all together.",
    "content": "Well, we could certainly repeat that chart for each stat. But we can also combine the information into one chart… we just have to do some data wrangling with Pandas beforehand. First, here’s a reminder of our data format: . stats_df.head() . | | Name | Type 1 | Type 2 | HP | Attack | Defense | Sp. Atk | Sp. Def | Speed | . | # | | | | | | | | | | . | 1 | Bulbasaur | Grass | Poison | 45 | 49 | 49 | 65 | 65 | 45 | . | 2 | Ivysaur | Grass | Poison | 60 | 62 | 63 | 80 | 80 | 60 | . | 3 | Venusaur | Grass | Poison | 80 | 82 | 83 | 100 | 100 | 80 | . | 4 | Charmander | Fire | NaN | 39 | 52 | 43 | 60 | 50 | 65 | . | 5 | Charmeleon | Fire | NaN | 58 | 64 | 58 | 80 | 65 | 80 | . As you can see, all of our stats are in separate columns. Instead, we want to “melt” them into one column. To do so, we’ll use Pandas’s melt() function. It takes 3 arguments: . | First, the DataFrame to melt. | Second, ID variables to keep (Pandas will melt all of the other ones). | Finally, a name for the new, melted variable. | . Here’s the output: . # Melt DataFrame melted_df = pd.melt(stats_df, id_vars=[\"Name\", \"Type 1\", \"Type 2\"], # Variables to keep var_name=\"Stat\") # Name of melted variable melted_df.sort_values('Name').head(10) . | | Name | Type 1 | Type 2 | Stat | value | . | 364 | Abra | Psychic | NaN | Defense | 15 | . | 213 | Abra | Psychic | NaN | Attack | 20 | . | 817 | Abra | Psychic | NaN | Speed | 90 | . | 666 | Abra | Psychic | NaN | Sp. Def | 55 | . | 515 | Abra | Psychic | NaN | Sp. Atk | 105 | . | 62 | Abra | Psychic | NaN | HP | 25 | . | 443 | Aerodactyl | Rock | Flying | Defense | 65 | . | 594 | Aerodactyl | Rock | Flying | Sp. Atk | 60 | . | 745 | Aerodactyl | Rock | Flying | Sp. Def | 75 | . | 292 | Aerodactyl | Rock | Flying | Attack | 105 | . All 6 of the stat columns have been “melted” into one, and the new Stat column indicates the original stat (HP, Attack, Defense, Sp. Attack, Sp. Defense, or Speed). For example, it’s hard to see here, but Bulbasaur now has 6 rows of data. In fact, if you print the shape of these two DataFrames… . print( stats_df.shape ) print( melted_df.shape ) . (151, 9) (906, 5) . …you’ll find that melted_df has 6 times the number of rows as stats_df. Now we can make a swarm plot with melted_df. | But this time, we’re going to set x='Stat' and y='value' so our swarms are separated by stat. | Then, we’ll set hue='Type 1' to color our points by the Pokémon type. | . plt.figure(figsize=(14,10)) # Swarmplot with melted_df sns.swarmplot(x='Stat', y='value', data=melted_df, hue='Type 1') . &lt;AxesSubplot:xlabel='Stat', ylabel='value'&gt; . Finally, let’s make a few final tweaks for a more readable chart: . | Enlarge the plot. | Separate points by hue using the argument split=True. | Use our custom Pokemon color palette. | Adjust the y-axis limits to end at 0. | Place the legend to the right. | . # 1. Enlarge the plot plt.figure(figsize=(14,10)) sns.swarmplot(x='Stat', y='value', data=melted_df, hue='Type 1', split=True, # 2. Separate points by hue palette=pkmn_type_colors) # 3. Use Pokemon palette # 4. Adjust the y-axis plt.ylim(0, 260) # 5. Place legend to the right plt.legend(bbox_to_anchor=(1, 1), loc=2) . &lt;matplotlib.legend.Legend at 0x7fd6f8a04160&gt; . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-9-putting-it-all-together",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-9-putting-it-all-together"
  },"282": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "Step 10: Pokédex (mini-gallery).",
    "content": "We’re going to conclude this tutorial with a few quick-fire data visualizations, just to give you a sense of what’s possible with Seaborn. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-10-pok%C3%A9dex-mini-gallery",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#step-10-pokédex-mini-gallery"
  },"283": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "10.1 - Heatmap",
    "content": "Heatmaps help you visualize matrix-like data. plt.figure(figsize=(8,6)) # Calculate correlations corr = stats_df.corr() # Heatmap sns.heatmap(corr) . &lt;AxesSubplot:&gt; . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#101---heatmap",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#101---heatmap"
  },"284": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "10.2 - Histogram",
    "content": "Histograms allow you to plot the distributions of numeric variables. plt.figure(figsize=(8,6)) # Distribution Plot (a.k.a. Histogram) sns.distplot(df.Attack) . &lt;AxesSubplot:xlabel='Attack', ylabel='Density'&gt; . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#102---histogram",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#102---histogram"
  },"285": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "10.3 - Bar Plot",
    "content": "Bar plots help you visualize the distributions of categorical variables. plt.figure(figsize=(8,6)) # Count Plot (a.k.a. Bar Plot) sns.countplot(x='Type 1', data=df, palette=pkmn_type_colors) # Rotate x-labels plt.xticks(rotation=-45) . (array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]), [Text(0, 0, 'Grass'), Text(1, 0, 'Fire'), Text(2, 0, 'Water'), Text(3, 0, 'Bug'), Text(4, 0, 'Normal'), Text(5, 0, 'Poison'), Text(6, 0, 'Electric'), Text(7, 0, 'Ground'), Text(8, 0, 'Fairy'), Text(9, 0, 'Fighting'), Text(10, 0, 'Psychic'), Text(11, 0, 'Rock'), Text(12, 0, 'Ghost'), Text(13, 0, 'Ice'), Text(14, 0, 'Dragon')]) . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#103---bar-plot",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#103---bar-plot"
  },"286": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "10.4 - Factor Plot",
    "content": "Factor plots make it easy to separate plots by categorical classes. # Factor Plot g = sns.factorplot(x='Type 1', y='Attack', data=df, hue='Stage', # Color by stage col='Stage', # Separate by stage kind='swarm') # Swarmplot # Rotate x-axis labels g.set_xticklabels(rotation=-45) # Doesn't work because only rotates last plot # plt.xticks(rotation=-45) . &lt;seaborn.axisgrid.FacetGrid at 0x7fd729325c40&gt; . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#104---factor-plot",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#104---factor-plot"
  },"287": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "10.5 - Density Plot",
    "content": "Density plots display the distribution between two variables. plt.figure(figsize=(8,6)) # Density Plot sns.kdeplot(df.Attack, df.Defense) . &lt;AxesSubplot:xlabel='Attack', ylabel='Defense'&gt; . ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#105---density-plot",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#105---density-plot"
  },"288": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "10.6 - Joint Distribution Plot",
    "content": "Joint distribution plots combine information from scatter plots and histograms to give you detailed information for bi-variate distributions. plt.figure(figsize=(8,6)) # Joint Distribution Plot sns.jointplot(x='Attack', y='Defense', data=df) . &lt;seaborn.axisgrid.JointGrid at 0x7fd708a60f70&gt; &lt;Figure size 576x432 with 0 Axes&gt; . Congratulations… you’ve made it to the end of this Python Seaborn tutorial! . We’ve just concluded a tour of key Seaborn paradigms and showed you many examples along the way. Feel free to use this page along with the official Seaborn gallery as references for your projects going forward. ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#106---joint-distribution-plot",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html#106---joint-distribution-plot"
  },"289": {
    "doc": "04 - The Ultimate Python Seaborn Tutorial",
    "title": "04 - The Ultimate Python Seaborn Tutorial",
    "content": " ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202/04%20-%20The%20Ultimate%20Python%20Seaborn%20Tutorial.html"
  },"290": {
    "doc": "04 - Using Synthea",
    "title": "OSX",
    "content": "cd ~/Source/synthea ./run_synthea . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/04%20-%20Using%20Synthea.html#osx",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/04%20-%20Using%20Synthea.html#osx"
  },"291": {
    "doc": "04 - Using Synthea",
    "title": "Windows",
    "content": "cd c:\\users\\kolowitz\\synthea run_synthea.bat . Note: if running on Windows, use .\\run_synthea.bat instead of ./run_synthea – this guide uses ./run_synthea for brevity . import os import pathlib os.chdir(os.path.join(pathlib.Path.home(), 'Source/synthea')) . # generate a population of 5 patients !./run_synthea -p 5 . # generate a population of 5 patients in Pittsburgh Pennsylvania !./run_synthea -p 5 Pennsylvania Pittsburgh . # generate a population of 5 patients with a seed of 12345 !./run_synthea -p 5 -s 12345 . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/04%20-%20Using%20Synthea.html#windows",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/04%20-%20Using%20Synthea.html#windows"
  },"292": {
    "doc": "04 - Using Synthea",
    "title": "Common Configuration",
    "content": "Source . | Setting Name | Valid Values | Default | Description | . | exporter.ccda.export | true/false | false | Change this setting to true to enable exporting patients in CCDA format. | . | exporter.fhir.export | true/false | true | Change this setting to false to disable exporting patients in FHIR R4 format. | . | exporter.text.export | true/false | false | Change this setting to true to enable exporting patients in a simple text-based format. | . | exporter.csv.export | true/false | false | Change this setting to true to enable exporting patient data in a comma-separated value format. See the CSV File Data Dictionary. | . | exporter.years_of_history | Whole number | 10 | The number of years of patient history to include in patient records. For example, if set to 5, then all patient history older than 5 years old (from the time you execute the program) will not be included in the exported records. Note that conditions and medications that are currently active will still be exported, regardless of this setting. Set this to 0 to keep all history in the patient record. | . !cat ./src/main/resources/synthea.properties . cat: ./src/main/resources/synthea.properties: No such file or directory . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/04%20-%20Using%20Synthea.html#common-configuration",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/04%20-%20Using%20Synthea.html#common-configuration"
  },"293": {
    "doc": "04 - Using Synthea",
    "title": "04 - Using Synthea",
    "content": ". | Synthea Home | Synthea Introduction | Basic Setup and Running | . In a termainal, change the directory to where synthea is installed and execute the run_synthea script . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/04%20-%20Using%20Synthea.html",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/04%20-%20Using%20Synthea.html"
  },"294": {
    "doc": "04.a - Dictionaries",
    "title": "Python Dictionaries",
    "content": "Python Dictionary Tutorial . A dictionary in Python is a collection of items accessed by a specific key rather than by index. What does this mean? . Imagine a dictionary in the real world… when you need to look up the meaning of a word, you try to find the meaning using the word itself and not the possible index of the word. Python dictionaries work with the same concept, the word whose meaning you are looking for is the key and the meaning of the word is the value, you do not need to know the index of the word in a dictionary to find its meaning. Note: The keys in a dictionary have to be hashable. Hashing is the process of running an item through a specific kind of function. This function is called a “hash function”. This hash function returns a unique output for a unique input value. Integers, floating point numbers, strings, tuples, and frozensets are hashable. While lists, dictionaries, and sets other than frozensets are not. Hashing is a somewhat complex topic and this is only the basic concept behind hashing. You can initialize a dictionary in Python a variety of ways: . # Creating an empty dict using empty brackets word_frequency = {} # Creating an empty dict using dict() word_frequency = dict() print(word_frequency) . {} . # Creating Dictionaries with literals word_frequency = { \"Hello\" : 7, \"hi\" : 10, \"there\" : 45, \"at\" : 23, \"this\" : 77 } print(word_frequency) . {'Hello': 7, 'hi': 10, 'there': 45, 'at': 23, 'this': 77} . # Creating Dictionaries by passing parametrs in dict constructor word_frequency = dict(Hello = 7, hi = 10, there = 45, at = 23, this = 77) print(word_frequency) . {'Hello': 7, 'hi': 10, 'there': 45, 'at': 23, 'this': 77} . # Creating dictionaries with a List of tuples list_of_tuples = [(\"Hello\" , 7), (\"hi\" , 10), (\"there\" , 45),(\"at\" , 23),(\"this\" , 77)] word_frequency = dict(list_of_tuples) print(word_frequency) . {'Hello': 7, 'hi': 10, 'there': 45, 'at': 23, 'this': 77} . # create and Initialize a dictionary by this list elements as keys and with same value 0 list_of_strings = [\"Hello\", \"hi\", \"there\", \"at\", \"this\"] word_frequency = dict.fromkeys(list_of_strings,0 ) print(word_frequency) . {'Hello': 0, 'hi': 0, 'there': 0, 'at': 0, 'this': 0} . # Creating a Dictionary by a two lists list_of_strings = [\"Hello\", \"hi\", \"there\", \"at\", \"this\"] list_of_ints = [7, 10, 45, 23, 77, 99, 100, 101] # Merge the two lists to create a dictionary word_frequency = dict(zip(list_of_strings, list_of_ints)) print(word_frequency) . {'Hello': 7, 'hi': 10, 'there': 45, 'at': 23, 'this': 77} . You can then access the values by specifying the dictionary key . # or this way a = {'apple': 'fruit', 'beetroot': 'vegetable', 'cake': 'dessert'} print(a['apple']) . fruit . The items in a dictionary can have any data type. Check out some more examples of a dictionary to get a hang of it: . a = {'one': 1, 'two': 'to', 'three': 3.0, 'four': [4,4.0]} print(a) . {'one': 1, 'two': 'to', 'three': 3.0, 'four': [4, 4.0]} . # Update a dictionary a['one'] = 1.0 print(a) . {'one': 1.0, 'two': 'to', 'three': 3.0, 'four': [4, 4.0]} . # Delete a single element del a['one'] print(a) . {'two': 'to', 'three': 3.0, 'four': [4, 4.0]} . # Delete all elements in the dictionary a.clear() print(a) . {} . # Delete the dictionary del a print(a) . --------------------------------------------------------------------------- NameError Traceback (most recent call last) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_93358/2899747350.py in &lt;module&gt; 1 # Delete the dictionary 2 del a ----&gt; 3 print(a) NameError: name 'a' is not defined . It is important to remember is that a key has to be unique in a dictionary, no duplicates are allowed. However, in case of duplicate keys rather than giving an error, Python will take the last instance of the key to be valid and simply ignore the first key-value pair. sweet_dict = {'a1': 'cake', 'a2':'cookie', 'a1': 'icecream'} print(sweet_dict) print(sweet_dict['a1']) . {'a1': 'icecream', 'a2': 'cookie'} icecream . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.a%20-%20Dictionaries.html#python-dictionaries",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.a%20-%20Dictionaries.html#python-dictionaries"
  },"295": {
    "doc": "04.a - Dictionaries",
    "title": "04.a - Dictionaries",
    "content": " ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.a%20-%20Dictionaries.html",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.a%20-%20Dictionaries.html"
  },"296": {
    "doc": "04.a - More Web Scraping with BeautifulSoup",
    "title": "Webscraping with BeautifulSoup",
    "content": "Source . Additional References . | https://css-tricks.com/how-css-selectors-work/ | https://scrapy.org/ | https://www.seleniumhq.org/ | https://automatetheboringstuff.com/chapter11/ | . Lets Scrape the wall of shame from the U.S. Department of Health and Human Services Office for Civil Rights (You could just download the data file, but where is the fun in that!) . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.a%20-%20More%20Web%20Scraping%20with%20BeautifulSoup.html#webscraping-with-beautifulsoup",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.a%20-%20More%20Web%20Scraping%20with%20BeautifulSoup.html#webscraping-with-beautifulsoup"
  },"297": {
    "doc": "04.a - More Web Scraping with BeautifulSoup",
    "title": "Step 1 - Import all of the libraries you’ll need",
    "content": ". | requests | pandas | numpy | matplotlib.pyplot | Beautiful Soup | . import requests, pandas, numpy, matplotlib.pyplot from bs4 import BeautifulSoup . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.a%20-%20More%20Web%20Scraping%20with%20BeautifulSoup.html#step-1---import-all-of-the-libraries-youll-need",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.a%20-%20More%20Web%20Scraping%20with%20BeautifulSoup.html#step-1---import-all-of-the-libraries-youll-need"
  },"298": {
    "doc": "04.a - More Web Scraping with BeautifulSoup",
    "title": "Step 2 - Download the data",
    "content": "2.a Use the requests library to grab the content of this page . https://ocrportal.hhs.gov/ocr/breach/breach_report.jsf . page = requests.get(\"https://ocrportal.hhs.gov/ocr/breach/breach_report.jsf\") . page.content.decode('utf-8')[:1000] . '&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\" dir=\"ltr\"&gt;&lt;head id=\"j_idt2\"&gt;\\n\\t\\t\\t&lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=9\" /&gt;&lt;link type=\"text/css\" rel=\"stylesheet\" href=\"/ocr/javax.faces.resource/theme.css.jsf?ln=primefaces-aristo\" /&gt;&lt;link type=\"text/css\" rel=\"stylesheet\" href=\"/ocr/javax.faces.resource/components.css.jsf;jsessionid=AB290E52061CD17E470AE5AD3A2B9512?ln=primefaces&amp;amp;v=6.1\" /&gt;&lt;script type=\"text/javascript\" src=\"/ocr/javax.faces.resource/jquery/jquery.js.jsf;jsessionid=AB290E52061CD17E470AE5AD3A2B9512?ln=primefaces&amp;amp;v=6.1\"&gt;&lt;/script&gt;&lt;script type=\"text/javascript\" src=\"/ocr/javax.faces.resource/jquery/jquery-plugins.js.jsf;jsessionid=AB290E52061CD17E470AE5AD3A2B9512?ln=primefaces&amp;amp;v=6.1\"&gt;&lt;/script&gt;&lt;script type=\"text/javascript\" src=\"/ocr/javax.faces.resource/core.js.jsf;jsessionid=AB290E52061CD17E470AE5AD3A2B9512?ln=primefaces&amp;amp;v=6.1\"&gt;&lt;/script&gt;&lt;script type=\"text/javascript\" src=\"/ocr/javax.faces.resource/components.js.jsf;j' . 2.b Check thte status code of the page and print it to the screen . page.status_code . 200 . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.a%20-%20More%20Web%20Scraping%20with%20BeautifulSoup.html#step-2---download-the-data",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.a%20-%20More%20Web%20Scraping%20with%20BeautifulSoup.html#step-2---download-the-data"
  },"299": {
    "doc": "04.a - More Web Scraping with BeautifulSoup",
    "title": "Step 3 - BeautifulSoup",
    "content": "3.a Load the page content into a BeautifulSoup object named soup . soup = BeautifulSoup(page.content, 'html.parser') . 3.b Display the soup object to visually interrogate . print(type(soup), '\\n', soup) . &lt;class 'bs4.BeautifulSoup'&gt; &lt;!DOCTYPE html&gt; &lt;html dir=\"ltr\" lang=\"en\" xml:lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;head id=\"j_idt2\"&gt; &lt;meta content=\"IE=9\" http-equiv=\"X-UA-Compatible\"/&gt;&lt;link href=\"/ocr/javax.faces.resource/theme.css.jsf?ln=primefaces-aristo\" rel=\"stylesheet\" type=\"text/css\"/&gt;&lt;link href=\"/ocr/javax.faces.resource/components.css.jsf;jsessionid=AB290E52061CD17E470AE5AD3A2B9512?ln=primefaces&amp;amp;v=6.1\" rel=\"stylesheet\" type=\"text/css\"/&gt;&lt;script src=\"/ocr/javax.faces.resource/jquery/jquery.js.jsf;jsessionid=AB290E52061CD17E470AE5AD3A2B9512?ln=primefaces&amp;amp;v=6.1\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script src=\"/ocr/javax.faces.resource/jquery/jquery-plugins.js.jsf;jsessionid=AB290E52061CD17E470AE5AD3A2B9512?ln=primefaces&amp;amp;v=6.1\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script src=\"/ocr/javax.faces.resource/core.js.jsf;jsessionid=AB290E52061CD17E470AE5AD3A2B9512?ln=primefaces&amp;amp;v=6.1\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script src=\"/ocr/javax.faces.resource/components.js.jsf;jsessionid=AB290E52061CD17E470AE5AD3A2B9512?ln=primefaces&amp;amp;v=6.1\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script type=\"text/javascript\"&gt;if(window.PrimeFaces){PrimeFaces.settings.locale='en';}&lt;/script&gt; &lt;meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/&gt; &lt;title&gt;U.S. Department of Health &amp;amp; Human Services - Office for Civil Rights&lt;/title&gt; &lt;link href=\"/ocr/images/favicon.ico\" rel=\"Shortcut Icon\" type=\"image/x-icon\"/&gt; &lt;link href=\"/ocr/css/ocr-portal.css\" rel=\"stylesheet\" type=\"text/css\"/&gt; &lt;link href=\"/ocr/css/navigation_ocr.css\" rel=\"stylesheet\" type=\"text/css\"/&gt; &lt;link href=\"/ocr/css/primary.css\" rel=\"stylesheet\" type=\"text/css\"/&gt; &lt;link href=\"/ocr/css/skip.css\" rel=\"stylesheet\" type=\"text/css\"/&gt; &lt;link href=\"/ocr/css/headerNavigation.css\" rel=\"stylesheet\" type=\"text/css\"/&gt; &lt;link href=\"/ocr/css/content.css\" rel=\"stylesheet\" type=\"text/css\"/&gt; &lt;link href=\"/ocr/css/breach.css\" rel=\"stylesheet\" type=\"text/css\"/&gt; &lt;script src=\"/ocr/js/util.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;script src=\"/ocr/js/locale.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;script src=\"/ocr/js/ocr-portal.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"top\"&gt;&lt;/div&gt; &lt;div id=\"container\"&gt; &lt;form action=\"/ocr/breach/breach_report.jsf;jsessionid=AB290E52061CD17E470AE5AD3A2B9512\" enctype=\"application/x-www-form-urlencoded\" id=\"headerNavigation\" method=\"post\" name=\"headerNavigation\"&gt; &lt;input name=\"headerNavigation\" type=\"hidden\" value=\"headerNavigation\"/&gt; &lt;div class=\"\" id=\"skip\"&gt; &lt;ul id=\"skiplinks\"&gt; &lt;li&gt;&lt;a href=\"#bodyContent\" tabindex=\"0\" title=\"Skip\"&gt;Skip to content&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div id=\"level1\"&gt; &lt;div class=\"header\"&gt; &lt;div id=\"navigationNew\"&gt; &lt;div class=\"site-controls-greybar\"&gt;&lt;img alt=\"\" height=\"28\" src=\"/ocr/images/cap_left_dkgrey_systemutil.gif;jsessionid=AB290E52061CD17E470AE5AD3A2B9512\" style=\"border: 0; vertical-align: middle\" width=\"19\"/&gt;&lt;script src=\"/ocr/javax.faces.resource/jsf.js.jsf;jsessionid=AB290E52061CD17E470AE5AD3A2B9512?ln=javax.faces\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;a class=\"headerLink\" href=\"#\" onclick=\"mojarra.jsfcljs(document.getElementById('headerNavigation'),{'headerNavigation:j_idt9':'headerNavigation:j_idt9'},'');return false\"&gt;File a Breach&lt;/a&gt; | &lt;a class=\"headerLink\" href=\"http://www.hhs.gov\"&gt;HHS&lt;/a&gt; | &lt;a class=\"headerLink\" href=\"http://www.hhs.gov/ocr/office/index.html\"&gt;Office for Civil Rights&lt;/a&gt; | &lt;a class=\"headerLink\" href=\"https://www.hhs.gov/ocr/about-us/contact-us/index.html\"&gt;Contact Us&lt;/a&gt; &lt;/div&gt; &lt;div class=\"site-controls-greybar-lt\"&gt;&lt;img alt=\"\" height=\"28\" src=\"/ocr/images/back_left_ltgrey_systemutil_sm.gif;jsessionid=AB290E52061CD17E470AE5AD3A2B9512\" style=\"border: 0; vertical-align: middle\" width=\"25\"/&gt; &lt;div class=\"site-controls-greybar-lt-bkgrnd\"&gt;&lt;img alt=\"\" height=\"28\" src=\"/ocr/images/clear.gif;jsessionid=AB290E52061CD17E470AE5AD3A2B9512\" style=\"border: 0; vertical-align: middle\" width=\"1\"/&gt; &lt;img align=\"absmiddle\" alt=\"user icon\" border=\"0\" class=\"sc-icon\" height=\"16\" src=\"/ocr/images/icons/icon_user.gif\" title=\"Welcome\" width=\"16\"/&gt; Welcome &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;h1 class=\"h1\" id=\"header-logo\"&gt; &lt;div id=\"titleContent\"&gt; U.S. Department of Health and Human Services&lt;br/&gt; Office for Civil Rights&lt;br/&gt; &lt;div class=\"titleDivider\"&gt;&lt;/div&gt; Breach Portal: Notice to the Secretary of HHS Breach of Unsecured Protected Health Information &lt;/div&gt; &lt;/h1&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=\"bannerSection\"&gt; &lt;div id=\"bannerImageWrapper\"&gt; &lt;img alt=\"Person sitting at a laptop\" class=\"bannerImage\" height=\"100\" src=\"/ocr/images/1680x100sittingAtComputer2.jpg\" width=\"1680\"/&gt; &lt;/div&gt; &lt;/div&gt;&lt;input autocomplete=\"off\" id=\"j_id1:javax.faces.ViewState:0\" name=\"javax.faces.ViewState\" type=\"hidden\" value=\"9200193373227482523:-7770887650549839413\"/&gt; &lt;/form&gt; &lt;div class=\"divider\"&gt;&lt;/div&gt; &lt;div id=\"mainContent\"&gt; &lt;div class=\"pageInstructions\"&gt; &lt;a name=\"bodyContent\"&gt;&lt;/a&gt;&lt;div aria-live=\"polite\" class=\"ui-messages ui-widget\" id=\"ocrMessage\"&gt;&lt;/div&gt; &lt;div class=\"content\"&gt; &lt;form action=\"/ocr/breach/breach_report.jsf;jsessionid=AB290E52061CD17E470AE5AD3A2B9512\" enctype=\"multipart/form-data\" id=\"ocrForm\" method=\"post\" name=\"ocrForm\"&gt; &lt;input name=\"ocrForm\" type=\"hidden\" value=\"ocrForm\"/&gt; &lt;script&gt; function doArchiveButtonClicked() { setTimeout( \"PF('archiveRptButton').disable()\", 100 ); PF('waitDlgVar').show(); return true; } function doUnderInvButtonClicked() { setTimeout( \"PF('underInvRptButton').disable()\", 100 ); PF('waitDlgVar').show(); return true; } &lt;/script&gt;&lt;div class=\"ui-panel ui-widget ui-widget-content ui-corner-all banner\" data-widget=\"widget_ocrForm_j_idt22\" id=\"ocrForm:j_idt22\"&gt;&lt;div class=\"ui-panel-content ui-widget-content\" id=\"ocrForm:j_idt22_content\"&gt;&lt;button class=\"ui-button ui-widget ui-state-default ui-corner-all ui-button-text-only\" id=\"ocrForm:j_idt23\" name=\"ocrForm:j_idt23\" onclick=\"PrimeFaces.bcn(this,event,[function(event){return doUnderInvButtonClicked();},function(event){PrimeFaces.onPost();}]);\" type=\"submit\"&gt;&lt;span class=\"ui-button-text ui-c\"&gt;Under Investigation&lt;/span&gt;&lt;/button&gt;&lt;script id=\"ocrForm:j_idt23_s\" type=\"text/javascript\"&gt;PrimeFaces.cw(\"CommandButton\",\"underInvRptButton\",{id:\"ocrForm:j_idt23\"});&lt;/script&gt;&lt;button class=\"ui-button ui-widget ui-state-default ui-corner-all ui-button-text-only\" id=\"ocrForm:j_idt24\" name=\"ocrForm:j_idt24\" onclick=\"PrimeFaces.bcn(this,event,[function(event){return doArchiveButtonClicked();},function(event){PrimeFaces.onPost();}]);\" type=\"submit\"&gt;&lt;span class=\"ui-button-text ui-c\"&gt;Archive&lt;/span&gt;&lt;/button&gt;&lt;script id=\"ocrForm:j_idt24_s\" type=\"text/javascript\"&gt;PrimeFaces.cw(\"CommandButton\",\"archiveRptButton\",{id:\"ocrForm:j_idt24\"});&lt;/script&gt;&lt;button class=\"ui-button ui-widget ui-state-default ui-corner-all ui-button-text-only\" id=\"ocrForm:j_idt25\" name=\"ocrForm:j_idt25\" onclick='PrimeFaces.ab({s:\"ocrForm:j_idt25\",u:\"ocrForm:breachReports ocrForm:results\"});return false;' type=\"submit\"&gt;&lt;span class=\"ui-button-text ui-c\"&gt;Help for Consumers&lt;/span&gt;&lt;/button&gt;&lt;script id=\"ocrForm:j_idt25_s\" type=\"text/javascript\"&gt;PrimeFaces.cw(\"CommandButton\",\"widget_ocrForm_j_idt25\",{id:\"ocrForm:j_idt25\"});&lt;/script&gt;&lt;/div&gt;&lt;/div&gt;&lt;script id=\"ocrForm:j_idt22_s\" type=\"text/javascript\"&gt;PrimeFaces.cw(\"Panel\",\"widget_ocrForm_j_idt22\",{id:\"ocrForm:j_idt22\"});&lt;/script&gt;&lt;br/&gt;&lt;br/&gt;&lt;span id=\"ocrForm:breachReports\"&gt;&lt;span id=\"ocrForm:underInvestigationFilterPanel\" style=\"margin-bottom:10px;\"&gt; &lt;style&gt; /* Calendar widget's size is too big. Limit it. */ div.ui-datepicker{ font-size:0.9em; } &lt;/style&gt; &lt;script&gt; function applyFilterButtonClicked() { setTimeout( \"PF('applyFilterButton').disable()\", 100 ); PF('waitDlgVar').show(); return true; } &lt;/script&gt;&lt;table class=\"ui-panelgrid ui-widget noBorders\" id=\"ocrForm:j_idt29\" role=\"grid\"&gt;&lt;tbody&gt;&lt;tr class=\"ui-widget-content ui-panelgrid-even\" role=\"row\"&gt;&lt;td class=\"ui-panelgrid-cell\" colspan=\"4\" role=\"gridcell\"&gt;As required by section 13402(e)(4) of the HITECH Act, the Secretary must post a list of breaches of unsecured protected health information affecting 500 or more individuals. The following breaches have been reported to the Secretary: &lt;br/&gt;&lt;br/&gt;&lt;span style=\"font-size: 1.5em; font-weight: bold\"&gt;Cases Currently Under Investigation&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-panelgrid-odd\" role=\"row\"&gt;&lt;td class=\"ui-panelgrid-cell\" colspan=\"4\" role=\"gridcell\"&gt;This page lists all breaches reported within the last 24 months that are currently under investigation by the Office for Civil Rights.&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-panelgrid-even\" role=\"row\"&gt;&lt;td class=\"ui-panelgrid-cell\" colspan=\"4\" role=\"gridcell\"&gt;  &lt;a class=\"ui-commandlink ui-widget\" href=\"#\" id=\"ocrForm:j_idt41\" onclick='PrimeFaces.ab({s:\"ocrForm:j_idt41\",p:\"ocrForm:j_idt41\",u:\"ocrForm:underInvestigationAdvancedOptionsTable ocrForm:j_idt41\",ps:true});return false;'&gt;Show Advanced Options&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;span id=\"ocrForm:underInvestigationAdvancedOptionsTable\"&gt;&lt;/span&gt;&lt;div class=\"ui-dialog ui-widget ui-widget-content ui-corner-all ui-shadow ui-hidden-container\" id=\"ocrForm:waitDlgIN\"&gt;&lt;div class=\"ui-dialog-content ui-widget-content\"&gt;&lt;span id=\"ocrForm:waitDlgIN_title\"&gt;&lt;/span&gt;We are generating the report for you. Please wait......&lt;/div&gt;&lt;/div&gt;&lt;script id=\"ocrForm:waitDlgIN_s\" type=\"text/javascript\"&gt;$(function(){PrimeFaces.cw(\"Dialog\",\"waitDlgINI\",{id:\"ocrForm:waitDlgIN\",resizable:false,modal:true});});&lt;/script&gt;&lt;/span&gt;&lt;/span&gt;&lt;span id=\"ocrForm:results\"&gt; &lt;br/&gt;&lt;span id=\"ocrForm:resultsPanel\" style=\"margin-bottom:10px;\"&gt; &lt;style&gt; .column1{width: 20%} .column2{width: 80%} &lt;/style&gt;&lt;span style=\"float: right\"&gt;&lt;a href=\"#\" onclick=\"mojarra.jsfcljs(document.getElementById('ocrForm'),{'ocrForm:j_idt360':'ocrForm:j_idt360'},'');return false\"&gt;&lt;img alt=\"Excel\" id=\"ocrForm:j_idt361\" src=\"/ocr/images/icons/excel.png;jsessionid=AB290E52061CD17E470AE5AD3A2B9512?pfdrid_c=true\" style=\"border-style: none\" title=\"Export as Excel\" width=\"24\"/&gt;&lt;/a&gt;&lt;a href=\"#\" onclick=\"mojarra.jsfcljs(document.getElementById('ocrForm'),{'ocrForm:j_idt362':'ocrForm:j_idt362'},'');return false\"&gt;&lt;img alt=\"PDF\" id=\"ocrForm:j_idt363\" src=\"/ocr/images/icons/pdf.png;jsessionid=AB290E52061CD17E470AE5AD3A2B9512?pfdrid_c=true\" style=\"border-style: none\" title=\"Export as PDF\" width=\"24\"/&gt;&lt;/a&gt;&lt;a href=\"#\" onclick=\"mojarra.jsfcljs(document.getElementById('ocrForm'),{'ocrForm:j_idt364':'ocrForm:j_idt364'},'');return false\"&gt;&lt;img alt=\"CSV\" id=\"ocrForm:j_idt365\" src=\"/ocr/images/icons/csv.png;jsessionid=AB290E52061CD17E470AE5AD3A2B9512?pfdrid_c=true\" style=\"border-style: none\" title=\"Export as CSV\" width=\"24\"/&gt;&lt;/a&gt;&lt;a href=\"#\" onclick=\"mojarra.jsfcljs(document.getElementById('ocrForm'),{'ocrForm:j_idt366':'ocrForm:j_idt366'},'');return false\"&gt;&lt;img alt=\"XML\" id=\"ocrForm:j_idt367\" src=\"/ocr/images/icons/xml.png;jsessionid=AB290E52061CD17E470AE5AD3A2B9512?pfdrid_c=true\" style=\"border-style: none\" title=\"Export as XML\" width=\"24\"/&gt;&lt;/a&gt;&lt;/span&gt;&lt;div class=\"ui-datatable ui-widget\" id=\"ocrForm:reportResultTable\" style=\"margin-bottom:20px\"&gt;&lt;div class=\"ui-datatable-header ui-widget-header ui-corner-top\"&gt; Breach Report Results &lt;/div&gt;&lt;div class=\"ui-datatable-tablewrapper\"&gt;&lt;table role=\"grid\" style=\"table-layout: auto;\"&gt;&lt;caption class=\"ui-helper-hidden-accessible\"&gt;&lt;/caption&gt;&lt;thead id=\"ocrForm:reportResultTable_head\"&gt;&lt;tr role=\"row\"&gt;&lt;th class=\"ui-state-default\" id=\"ocrForm:reportResultTable:j_idt369\" role=\"columnheader\" scope=\"col\" style=\"width:50px\"&gt;&lt;span class=\"ui-column-title\"&gt;&lt;a class=\"ui-commandlink ui-widget\" href=\"#\" id=\"ocrForm:reportResultTable:j_idt370\" onclick='PrimeFaces.ab({s:\"ocrForm:reportResultTable:j_idt370\",u:\"ocrForm:reportResultTable:j_idt370 ocrForm:reportResultTable\"});return false;'&gt;Expand All&lt;/a&gt;&lt;/span&gt;&lt;/th&gt;&lt;th aria-label=\"Name of Covered Entity\" class=\"ui-state-default ui-sortable-column\" id=\"ocrForm:reportResultTable:j_idt372\" role=\"columnheader\" scope=\"col\"&gt;&lt;span class=\"ui-column-title\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Name of Covered Entity&lt;/span&gt;&lt;/span&gt;&lt;span class=\"ui-sortable-column-icon ui-icon ui-icon-carat-2-n-s\"&gt;&lt;/span&gt;&lt;/th&gt;&lt;th aria-label=\"State\" class=\"ui-state-default ui-sortable-column\" id=\"ocrForm:reportResultTable:j_idt375\" role=\"columnheader\" scope=\"col\"&gt;&lt;span class=\"ui-column-title\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;State&lt;/span&gt;&lt;/span&gt;&lt;span class=\"ui-sortable-column-icon ui-icon ui-icon-carat-2-n-s\"&gt;&lt;/span&gt;&lt;/th&gt;&lt;th aria-label=\"Covered Entity Type\" class=\"ui-state-default ui-sortable-column\" id=\"ocrForm:reportResultTable:j_idt378\" role=\"columnheader\" scope=\"col\"&gt;&lt;span class=\"ui-column-title\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Covered Entity Type&lt;/span&gt;&lt;/span&gt;&lt;span class=\"ui-sortable-column-icon ui-icon ui-icon-carat-2-n-s\"&gt;&lt;/span&gt;&lt;/th&gt;&lt;th aria-label=\"Individuals Affected\" class=\"ui-state-default ui-sortable-column\" id=\"ocrForm:reportResultTable:j_idt381\" role=\"columnheader\" scope=\"col\"&gt;&lt;span class=\"ui-column-title\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Individuals Affected&lt;/span&gt;&lt;/span&gt;&lt;span class=\"ui-sortable-column-icon ui-icon ui-icon-carat-2-n-s\"&gt;&lt;/span&gt;&lt;/th&gt;&lt;th aria-label=\"Breach Submission Date\" class=\"ui-state-default ui-sortable-column\" id=\"ocrForm:reportResultTable:j_idt384\" role=\"columnheader\" scope=\"col\"&gt;&lt;span class=\"ui-column-title\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Breach Submission Date&lt;/span&gt;&lt;/span&gt;&lt;span class=\"ui-sortable-column-icon ui-icon ui-icon-carat-2-n-s\"&gt;&lt;/span&gt;&lt;/th&gt;&lt;th aria-label=\"Type of Breach\" class=\"ui-state-default\" id=\"ocrForm:reportResultTable:j_idt387\" role=\"columnheader\" scope=\"col\"&gt;&lt;span class=\"ui-column-title\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Type of Breach&lt;/span&gt;&lt;/span&gt;&lt;/th&gt;&lt;th aria-label=\"Location of Breached Information\" class=\"ui-state-default\" id=\"ocrForm:reportResultTable:j_idt390\" role=\"columnheader\" scope=\"col\"&gt;&lt;span class=\"ui-column-title\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Location of Breached Information&lt;/span&gt;&lt;/span&gt;&lt;/th&gt;&lt;th aria-label=\"Business Associate Present\" class=\"ui-state-default\" id=\"ocrForm:reportResultTable:j_idt396\" role=\"columnheader\" scope=\"col\" style=\"display: none\"&gt;&lt;span class=\"ui-column-title\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Business Associate Present&lt;/span&gt;&lt;/span&gt;&lt;/th&gt;&lt;th aria-label=\"Web Description\" class=\"ui-state-default\" id=\"ocrForm:reportResultTable:j_idt399\" role=\"columnheader\" scope=\"col\" style=\"display: none\"&gt;&lt;span class=\"ui-column-title\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Web Description&lt;/span&gt;&lt;/span&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody class=\"ui-datatable-data ui-widget-content\" id=\"ocrForm:reportResultTable_data\"&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"0\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Syracuse ASC, LLC&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;NY&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;24891&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/14/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"1\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Jackson County Health Department&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;IL&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;1000&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/13/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"2\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Orange County Health Authority&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;CA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Health Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;4732&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/12/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Paper/Films&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"3\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;University Hospital&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;NJ&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;9329&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/08/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Electronic Medical Record, Other&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"4\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Senior Living, LLC and Pilgrim River, LLC &lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;NJ&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;3952&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/07/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"5\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Carteret Health Care&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;NC&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;1235&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/06/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Theft&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Paper/Films&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"6\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Drs. Kelley &amp;amp; McDowell PA&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;SC&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;6204&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/06/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"7\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Wirt County Health Services Association d/b/a Coplin Health Systems&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;WV&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;2643&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/05/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Paper/Films&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"8\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Rockbridge Area Community Services Board&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;VA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;2000&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/04/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"9\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Professional Dental Alliance of Florida, LLC&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;PA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;18626&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/01/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"10\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Professional Dental Alliance of Georgia, PLLC&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;PA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;23974&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/01/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"11\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Professional Dental Alliance of Illinois, PLLC&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;PA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;16673&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/01/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"12\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Professional Dental Alliance of Massachusetts&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;PA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;607&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/01/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"13\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Professional Dental Alliance of Michigan, PLLC&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;PA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;26054&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/01/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"14\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Professional Dental Alliance, LLC&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;PA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;47173&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/01/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"15\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;NADG Hopewell, Inc.&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;PA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;1143&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/01/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"16\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Professional Dental Alliance of Indiana, PLLC&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;PA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;7359&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/01/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"17\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Professional Dental Alliance of Connecticut, PLLC&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;PA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;6237&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/01/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"18\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Professional Dental Alliance of Tennessee, LLC&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;PA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;11217&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/01/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"19\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;OSF HealthCare System&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;IL&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;53907&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/01/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"20\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Professional Dental Alliance of New York, PLLC&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;PA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10778&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/01/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"21\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Professional Dental Alliance of Texas, PLLC&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;PA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;4235&lt;/td&gt;&lt;td role=\"gridcell\"&gt;10/01/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"22\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Premier Management Company &lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;TX&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;37636&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/30/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Laptop, Other, Other Portable Electronic Device&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"23\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Humana Inc&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;KY&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Health Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;948&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/28/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Paper/Films&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"24\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Lakewood Manor Baptist Retirement Community, Inc. d/b/a Lakewood &lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;VA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;1205&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/27/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"25\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Zenith American Solutions&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;WA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Business Associate&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;1907&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/24/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Paper/Films&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"26\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Digital Insurance, LLC doing business as OneDigital&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;GA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Health Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;895&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/24/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"27\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Griffith Energy Services Inc. Welfare Benefit Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;MD&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Health Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;500&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/24/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"28\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Epilepsy Foundation of Texas &lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;TX&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;2824&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/24/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"29\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;The Menninger Clinic&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;TX&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;1365&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/24/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"30\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Navistar, Inc. Health Plan and the Navistar, Inc. Retiree Health Benefit and Life Insurance Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;IL&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Health Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;49000&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/24/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"31\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;elizabeth ortof MD&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;NY&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;5000&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/23/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Theft&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Laptop&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"32\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;State of Alaska Department of Health &amp;amp; Social Services&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;AK&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Health Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;500000&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/22/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Desktop Computer, Laptop, Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"33\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Aetna ACE&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;CT&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Health Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;1011&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/22/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"34\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Cerebral Medical Group&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;FL&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;875&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/21/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Other&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"35\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;McAllen Surgical Specialty Center, Ltd.&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;TX&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;29227&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/20/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Desktop Computer, Electronic Medical Record, Laptop, Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"36\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Illinois Department of Human Services and Illinois Department of Healthcare and Family Services &lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;IL&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;1960&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/20/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"37\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Georgia Department of Human Services &lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;GA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Clearing House&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;500&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/17/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"38\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Horizon House, Inc.&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;PA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;27823&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/17/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"39\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Orlick &amp;amp; Kasper, M.D.'s, P.A.&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;FL&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;30000&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/17/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Theft&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Desktop Computer, Electronic Medical Record, Other Portable Electronic Device&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"40\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Welfare &amp;amp; Pension Administration Service, Inc.&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;WA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Health Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;545&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/17/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"41\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Famous Enterprises Inc. Employee Benefit Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;OH&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Health Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;528&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/17/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"42\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Mankato Clinic&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;MN&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;535&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/16/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"43\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Samaritan Center of Puget Sound&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;WA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;20866&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/15/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Theft&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Desktop Computer, Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"44\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Directions for Living&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;FL&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;19494&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/15/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"45\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Multnomah County &lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;OR&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;709&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/14/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Theft&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Paper/Films&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"46\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Simon Eye Management&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;DE&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;144373&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/14/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"47\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Eastern Los Angeles Regional Center&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;CA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Business Associate&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;12921&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/13/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"48\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Indian Creek Foundation&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;PA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;2405&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/13/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"49\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Abeo Solutions Inc. d/b/a Crystal Practice Management &lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;TX&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Business Associate&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;4774&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/13/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"50\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Buddhist Tzu Chi Medical Foundation&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;CA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;18968&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/13/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"51\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Consumer Direct Care Network Arizona&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;AZ&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;504&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/10/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"52\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Talbert House&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;OH&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;45000&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/10/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"53\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Rehabilitation Support Services, Inc.&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;NY&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;23907&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/10/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"54\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Central Texas Medical Specialists, PLLC dba Austin Cancer Centers&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;TX&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;36503&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/10/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"55\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;CVS Pharmacy&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;RI&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;826&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/10/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Theft&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Paper/Films&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"56\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Vista Radiology, P.C.&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;TN&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;3634&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/09/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"57\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;HBP Financial Services Group, LTD&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;CT&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Business Associate&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;938&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/08/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"58\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Pathology Consultants of New London, P.C.&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;CT&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;835&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/08/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"59\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Radiology Associates of Albuquerque, PA and Advanced Imaging, LLC&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;NM&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;501&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/08/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"60\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;City of Joplin&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;MO&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Health Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;513&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/08/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"61\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Resource Anesthesiology Association of California, a Medical Corporation&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;CA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;2835&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/05/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Theft&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Laptop, Paper/Films&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"62\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Asarco Health, Dental, Vision, Flexible Spending, Non-Union Employee Benefits, and Retiree Medical Plans&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;AZ&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Health Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;28000&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/03/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"63\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Eastern Connecticut Pathology Consultants, P.C.&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;CT&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;2500&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/03/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"64\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;ADEC, Inc.&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;IN&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;2000&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/03/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"65\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;USV Optical, Inc.&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;NJ&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;180000&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/03/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"66\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;California Department of State Hospitals - Coalinga&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;CA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;1738&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/03/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"67\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Sequoia Concepts, Inc.&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;CA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Business Associate&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;2230&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/02/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"68\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Dan L. Beaupre&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;WI&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Business Associate&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;2000&lt;/td&gt;&lt;td role=\"gridcell\"&gt;09/01/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server, Paper/Films&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"69\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Thomas Eye Group&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;GA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;500&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/30/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"70\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Ijaola, LLC d/b/a Mercy Grace Private Practice &lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;AZ&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;4450&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/30/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"71\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Queen Creek Medical Center d/b/a Desert Wells Family Medicine&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;AZ&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;35000&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/30/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"72\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthpointe Medical Group, Inc.&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;CA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;11000&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/30/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"73\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Easterseals Delaware &amp;amp; Maryland's Eastern Shore&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;DE&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;501&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/30/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"74\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;DuPage Medical Group, Ltd. &lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;IL&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;655384&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/30/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"75\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;CareATC, Inc.&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;OK&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;98774&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/27/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"76\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Beaumont Health&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;MI&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;1568&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/27/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"77\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Family Medical Center of Michigan &lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;MI&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;21988&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/26/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"78\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Medical Business Management&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;AL&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Business Associate&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;1750&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/25/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"79\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;LifeLong Medical Care&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;CA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;115448&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/25/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"80\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;San Andreas Regional Center&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;CA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Business Associate&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;57244&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/25/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"81\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;The Wedge Medical Center&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;PA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;29000&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/24/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"82\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Denton County, Texas&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;TX&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;326417&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/24/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"83\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;CarePointe ENT&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;IN&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;48742&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/23/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"84\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Gregory P. Vannucci DDS&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;IA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;26144&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/20/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"85\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;South Florida Community Care Network LLC d/b/a Community Care Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;FL&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Health Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;48344&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/20/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"86\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Central Utah Clinic, P.C. dba Revere Health&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;UT&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;12433&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/19/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"87\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Nova Biomedical Corporation&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;MA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Health Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;3774&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/17/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"88\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Nashua Regional Cancer Center dba Radiation Center of Greater Nashua&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;NH&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;520&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/17/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"89\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Metro Infectious Disease Consultants&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;IL&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;171740&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/16/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"90\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;UTMP Surgical Oncology&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;TN&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;1310&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/16/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Paper/Films&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"91\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Texoma Community Center&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;TX&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;24021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/16/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"92\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Alluma, Inc&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;MN&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;4117&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/15/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Paper/Films&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"93\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;McCoy Consulting Services LLC&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;WV&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Business Associate&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;5500&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/13/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"94\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Fairfax Medical Facilities, Inc.&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;OK&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;4970&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/13/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"95\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;University Medical Center Southern Nevada&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;NV&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;1300000&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/13/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"96\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;North Country Healthcare&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;NH&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;3550&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/12/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Yes&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"97\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Frisco Fertility Center, PLLC dba Dallas IVF&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;TX&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;552&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/12/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Unauthorized Access/Disclosure&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Electronic Medical Record&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-even\" data-ri=\"98\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Waste Management Health and Welfare Benefits Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;TX&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Health Plan&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;922&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/12/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Network Server&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr class=\"ui-widget-content ui-datatable-odd\" data-ri=\"99\" role=\"row\"&gt;&lt;td role=\"gridcell\" style=\"width:50px\"&gt;&lt;div aria-expanded=\"false\" aria-label=\"Toggle Row\" class=\"ui-row-toggler ui-icon ui-icon-circle-triangle-e\" role=\"button\" tabindex=\"0\"&gt;&lt;/div&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Children's Hospital of The King's Daughters&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;VA&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Healthcare Provider&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;914&lt;/td&gt;&lt;td role=\"gridcell\"&gt;08/11/2021&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Hacking/IT Incident&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;Email&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;No&lt;/span&gt;&lt;/td&gt;&lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;&lt;div aria-label=\"Pagination\" class=\"ui-paginator ui-paginator-bottom ui-widget-header ui-corner-bottom\" id=\"ocrForm:reportResultTable_paginator_bottom\" role=\"navigation\"&gt;&lt;span class=\"ui-paginator-current\"&gt;(Displaying 1 - 100 of 849)&lt;/span&gt; &lt;a aria-label=\"First Page\" class=\"ui-paginator-first ui-state-default ui-corner-all ui-state-disabled\" href=\"#\" tabindex=\"-1\"&gt;&lt;span class=\"ui-icon ui-icon-seek-first\"&gt;F&lt;/span&gt;&lt;/a&gt;&lt;a aria-label=\"Previous Page\" class=\"ui-paginator-prev ui-state-default ui-corner-all ui-state-disabled\" href=\"#\" tabindex=\"-1\"&gt;&lt;span class=\"ui-icon ui-icon-seek-prev\"&gt;P&lt;/span&gt;&lt;/a&gt;&lt;span class=\"ui-paginator-pages\"&gt;&lt;a class=\"ui-paginator-page ui-state-default ui-state-active ui-corner-all\" href=\"#\" tabindex=\"0\"&gt;1&lt;/a&gt;&lt;a class=\"ui-paginator-page ui-state-default ui-corner-all\" href=\"#\" tabindex=\"0\"&gt;2&lt;/a&gt;&lt;a class=\"ui-paginator-page ui-state-default ui-corner-all\" href=\"#\" tabindex=\"0\"&gt;3&lt;/a&gt;&lt;a class=\"ui-paginator-page ui-state-default ui-corner-all\" href=\"#\" tabindex=\"0\"&gt;4&lt;/a&gt;&lt;a class=\"ui-paginator-page ui-state-default ui-corner-all\" href=\"#\" tabindex=\"0\"&gt;5&lt;/a&gt;&lt;a class=\"ui-paginator-page ui-state-default ui-corner-all\" href=\"#\" tabindex=\"0\"&gt;6&lt;/a&gt;&lt;a class=\"ui-paginator-page ui-state-default ui-corner-all\" href=\"#\" tabindex=\"0\"&gt;7&lt;/a&gt;&lt;a class=\"ui-paginator-page ui-state-default ui-corner-all\" href=\"#\" tabindex=\"0\"&gt;8&lt;/a&gt;&lt;a class=\"ui-paginator-page ui-state-default ui-corner-all\" href=\"#\" tabindex=\"0\"&gt;9&lt;/a&gt;&lt;/span&gt;&lt;a aria-label=\"Next Page\" class=\"ui-paginator-next ui-state-default ui-corner-all\" href=\"#\" tabindex=\"0\"&gt;&lt;span class=\"ui-icon ui-icon-seek-next\"&gt;N&lt;/span&gt;&lt;/a&gt;&lt;a aria-label=\"Last Page\" class=\"ui-paginator-last ui-state-default ui-corner-all\" href=\"#\" tabindex=\"0\"&gt;&lt;span class=\"ui-icon ui-icon-seek-end\"&gt;E&lt;/span&gt;&lt;/a&gt;&lt;label class=\"ui-paginator-rpp-label ui-helper-hidden\" for=\"ocrForm:reportResultTable:j_id2\" id=\"ocrForm:reportResultTable_rppLabel\"&gt;Rows Per Page&lt;/label&gt;&lt;select aria-labelledby=\"ocrForm:reportResultTable_rppLabel\" autocomplete=\"off\" class=\"ui-paginator-rpp-options ui-widget ui-state-default ui-corner-left\" id=\"ocrForm:reportResultTable:j_id2\" name=\"ocrForm:reportResultTable_rppDD\" value=\"100\"&gt;&lt;option value=\"10\"&gt;10&lt;/option&gt;&lt;option value=\"20\"&gt;20&lt;/option&gt;&lt;option value=\"50\"&gt;50&lt;/option&gt;&lt;option selected=\"selected\" value=\"100\"&gt;100&lt;/option&gt;&lt;/select&gt;&lt;/div&gt;&lt;/div&gt;&lt;script id=\"ocrForm:reportResultTable_s\" type=\"text/javascript\"&gt;$(function(){PrimeFaces.cw(\"DataTable\",\"widget_ocrForm_reportResultTable\",{id:\"ocrForm:reportResultTable\",paginator:{id:['ocrForm:reportResultTable_paginator_bottom'],rows:100,rowCount:849,page:0,currentPageTemplate:'(Displaying {startRecord} - {endRecord} of {totalRecords})'},expansion:true,rowExpandMode:\"multiple\",groupColumnIndexes:[],behaviors:{page:function(ext,event) {PrimeFaces.ab({s:\"ocrForm:reportResultTable\",e:\"page\",p:\"ocrForm:reportResultTable\"},ext);}}});});&lt;/script&gt;&lt;/span&gt;&lt;/span&gt;&lt;div class=\"ui-dialog ui-widget ui-widget-content ui-corner-all ui-shadow ui-hidden-container\" id=\"ocrForm:waitDlg\"&gt;&lt;div class=\"ui-dialog-content ui-widget-content\"&gt;&lt;span id=\"ocrForm:waitDlg_title\"&gt;&lt;/span&gt;We are generating the report for you. Please wait......&lt;/div&gt;&lt;/div&gt;&lt;script id=\"ocrForm:waitDlg_s\" type=\"text/javascript\"&gt;$(function(){PrimeFaces.cw(\"Dialog\",\"waitDlgVar\",{id:\"ocrForm:waitDlg\",resizable:false,modal:true});});&lt;/script&gt;&lt;input autocomplete=\"off\" id=\"j_id1:javax.faces.ViewState:1\" name=\"javax.faces.ViewState\" type=\"hidden\" value=\"9200193373227482523:-7770887650549839413\"/&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div&gt;  &lt;/div&gt; &lt;div id=\"footerNew\"&gt; &lt;div id=\"footerContent\"&gt; U.S. Department of Health &amp;amp; Human Services - 200 Independence Avenue, S.W. - Washington, D.C. 20201 &lt;br/&gt;&lt;span style=\"color : #4d4947\"&gt;OCR Portal CS16 Production Server (Port1). Build Date: 05/28/2021 16:51&lt;/span&gt; &lt;/div&gt; &lt;div id=\"footerPadding\"&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/body&gt; &lt;/html&gt; . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.a%20-%20More%20Web%20Scraping%20with%20BeautifulSoup.html#step-3---beautifulsoup",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.a%20-%20More%20Web%20Scraping%20with%20BeautifulSoup.html#step-3---beautifulsoup"
  },"300": {
    "doc": "04.a - More Web Scraping with BeautifulSoup",
    "title": "Step 4 - Parse the Breach data",
    "content": "After inspecting the page we found that &lt;td role=\"gridcell\"&gt; usually precedes the breach records. 4.a find the gridcells and store in a variable named . ```python gridCells = soup.find_all(role=\"gridcell\") . print(gridCells[:2]) . [&lt;td class=\"ui-panelgrid-cell\" colspan=\"4\" role=\"gridcell\"&gt;As required by section 13402(e)(4) of the HITECH Act, the Secretary must post a list of breaches of unsecured protected health information affecting 500 or more individuals. The following breaches have been reported to the Secretary: &lt;br/&gt;&lt;br/&gt;&lt;span style=\"font-size: 1.5em; font-weight: bold\"&gt;Cases Currently Under Investigation&lt;/span&gt;&lt;/td&gt;, &lt;td class=\"ui-panelgrid-cell\" colspan=\"4\" role=\"gridcell\"&gt;This page lists all breaches reported within the last 24 months that are currently under investigation by the Office for Civil Rights.&lt;/td&gt;] . tr = soup.find_all('tr') . len(tr) . 104 . 4.b display the gridCells to inspect . gridCells[0] . &lt;td class=\"ui-panelgrid-cell\" colspan=\"4\" role=\"gridcell\"&gt;As required by section 13402(e)(4) of the HITECH Act, the Secretary must post a list of breaches of unsecured protected health information affecting 500 or more individuals. The following breaches have been reported to the Secretary: &lt;br/&gt;&lt;br/&gt;&lt;span style=\"font-size: 1.5em; font-weight: bold\"&gt;Cases Currently Under Investigation&lt;/span&gt;&lt;/td&gt; . gridCells[-1] . &lt;td role=\"gridcell\" style=\"display: none\"&gt;&lt;span style=\"white-space:pre-line;\"&gt;&lt;/span&gt;&lt;/td&gt; . We’ve decided that we want to store the contents of the breach data into lists named: . singleBreachList - contains the tokenized breach data from the web page . We’ll use singleBreachList to construct the following lists . nameCoveredEntityList - singleBreachList item 0 stateList - singleBreachList item 1 coveredEntityTypeList - singleBreachList item 2 affectIndividualsList - singleBreachList item 3 breachSubmittedDateList - singleBreachList item 4 typeOfBreachList - singleBreachList item 5 locationOfBreachedInformationList - singleBreachList item 6 . Also notice that not all of “gridcell” was breach data, we need to skip a couple lines to get to the good stuff. Use a counter to do this, specifically . skipTheBoringStuffCounter . There also appears to be a pattern in how the gridcell data is organized, there is a new event every 8 lines. Use a counter for that as well, specifically . theGoodStuffCounter . 4.c Initialize all of the lists to empty lists and the counters to zero. nameCoveredEntityList = [] stateList = [] coveredEntityTypeList = [] affectIndividualsList = [] breachSubmittedDateList = [] typeOfBreachList = [] locationOfBreachedInformationList = [] singleBreachList = [] skipTheBoringStuffCounter = 0 theGoodStuffCounter = 0 . 4.d Parse the Data . for c in gridCells: if skipTheBoringStuffCounter &lt;= 4: skipTheBoringStuffCounter = skipTheBoringStuffCounter + 1 if skipTheBoringStuffCounter &gt; 4: c = c.get_text().strip() if c != '': singleBreachList.append(c) theGoodStuffCounter = theGoodStuffCounter + 1 if theGoodStuffCounter == 8: nameCoveredEntityList.append(singleBreachList[0]) stateList.append(singleBreachList[1]) coveredEntityTypeList.append(singleBreachList[2]) affectIndividualsList.append(singleBreachList[3]) breachSubmittedDateList.append(singleBreachList[4]) typeOfBreachList.append(singleBreachList[5]) locationOfBreachedInformationList.append(singleBreachList[6]) theGoodStuffCounter = 0 singleBreachList = [] . nameCoveredEntityList[0:3] . ['Syracuse ASC, LLC', 'Jackson County Health Department', 'Orange County Health Authority'] . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.a%20-%20More%20Web%20Scraping%20with%20BeautifulSoup.html#step-4---parse-the-breach-data",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.a%20-%20More%20Web%20Scraping%20with%20BeautifulSoup.html#step-4---parse-the-breach-data"
  },"301": {
    "doc": "04.a - More Web Scraping with BeautifulSoup",
    "title": "Step 5 - Create a Dataframe and Visualize the Data",
    "content": "# create a dataframe using the newly scraped data breachDF = pandas.DataFrame({ \"Company Name\":nameCoveredEntityList, \"State\":stateList, \"Company Type\":coveredEntityTypeList, \"Affected Individuals\":affectIndividualsList, \"Breach Date\":breachSubmittedDateList, \"Breach Type\":typeOfBreachList, \"Data location\":locationOfBreachedInformationList }) . breachDF.head() . | | Company Name | State | Company Type | Affected Individuals | Breach Date | Breach Type | Data location | . | 0 | Syracuse ASC, LLC | NY | Healthcare Provider | 24891 | 10/14/2021 | Hacking/IT Incident | Network Server | . | 1 | Jackson County Health Department | IL | Healthcare Provider | 1000 | 10/13/2021 | Unauthorized Access/Disclosure | Email | . | 2 | Orange County Health Authority | CA | Health Plan | 4732 | 10/12/2021 | Unauthorized Access/Disclosure | Paper/Films | . | 3 | University Hospital | NJ | Healthcare Provider | 9329 | 10/08/2021 | Unauthorized Access/Disclosure | Electronic Medical Record, Other | . | 4 | Senior Living, LLC and Pilgrim River, LLC | NJ | Healthcare Provider | 3952 | 10/07/2021 | Hacking/IT Incident | Network Server | . # group by the Breach Type breachDF.groupby('Breach Type').size() . Breach Type Hacking/IT Incident 64 Theft 7 Unauthorized Access/Disclosure 29 dtype: int64 . # allow matplotlib graphs to show in the notebook %matplotlib inline . # group by the Breach Type breachDist = breachDF.groupby('Breach Type').size() . # plot the data breachDist.plot(kind='bar') . &lt;AxesSubplot:xlabel='Breach Type'&gt; . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.a%20-%20More%20Web%20Scraping%20with%20BeautifulSoup.html#step-5---create-a-dataframe-and-visualize-the-data",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.a%20-%20More%20Web%20Scraping%20with%20BeautifulSoup.html#step-5---create-a-dataframe-and-visualize-the-data"
  },"302": {
    "doc": "04.a - More Web Scraping with BeautifulSoup",
    "title": "04.a - More Web Scraping with BeautifulSoup",
    "content": " ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.a%20-%20More%20Web%20Scraping%20with%20BeautifulSoup.html",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.a%20-%20More%20Web%20Scraping%20with%20BeautifulSoup.html"
  },"303": {
    "doc": "04.b - Dictionary Comprehensions",
    "title": "Python Dictionary Comprehensions",
    "content": "Python Dictionary Comprehension Tutorial . You’ll learn: . | What it is, why it is important and how it can serve as an alternative to for loops and lambda functions. | How to add conditionals into dictionary comprehensions: you will work with if conditions, multiple if conditions and also if-else statements. | What nested dictionary comprehension is, how you can use it and how you can potentially rewrite it with for loops. | . Dictionary comprehension is a method for transforming one dictionary into another dictionary. During this transformation, items within the original dictionary can be conditionally included in the new dictionary and each item can be transformed as needed. A good list comprehension can make your code more expressive and thus, easier to read. The key with creating comprehensions is to not let them get so complex that your head spins when you try to decipher what they are actually doing. Keeping the idea of “easy to read” alive. The way to do dictionary comprehension in Python is to be able to access the key objects and the value objects of a dictionary. dict1 = {'a': 1, 'b': 2, 'c': 3, 'd': 4} . # Put all keys of `dict1` in a list and returns the list keys = dict1.keys() print(type(keys), keys) . &lt;class 'dict_keys'&gt; dict_keys(['a', 'b', 'c', 'd']) . # Put all values saved in `dict1` in a list and returns the list vals = dict1.values() print(type(vals), vals) . &lt;class 'dict_values'&gt; dict_values([1, 2, 3, 4]) . z = { 'x': 1, 'y': None, None: 'a'} print(z) print(type(None)) . {'x': 1, 'y': None, None: 'a'} &lt;class 'NoneType'&gt; . So, now that you know how to access all the keys and their values in a dictionary. You can also access each key-value pair within a dictionary using the items() method: . items = dict1.items() print(type(items), items) . &lt;class 'dict_items'&gt; dict_items([('a', 1), ('b', 2), ('c', 3), ('d', 4)]) . dict1 = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5} . This is the general template you can follow for dictionary comprehension. This can serve as the basic and the most simple template which can get more and more complex as you add conditionalities to it. dict_variable = {key:value for (key,value) in dictonary.items()} . # Double each value in the dictionary double_dict1 = {} for (k,v) in dict1.items(): double_dict1[k] = v*2 print(double_dict1) . {'a': 2, 'b': 4, 'c': 6, 'd': 8, 'e': 10} . # Double each value in the dictionary double_dict1 = {k:v*2 for (k,v) in dict1.items()} print(double_dict1) . {'a': 2, 'b': 4, 'c': 6, 'd': 8, 'e': 10} . # Double each value in the dictionary double_dict1 = {x[0]:x[1]*2 for x in dict1.items()} print(double_dict1) . {'a': 2, 'b': 4, 'c': 6, 'd': 8, 'e': 10} . double_dict1 = {k:dict1[k]*2 for k in dict1.keys()} print(double_dict1) . {'a': 2, 'b': 4, 'c': 6, 'd': 8, 'e': 10} . You can also make changes to the key values. For example, let’s create the same dictionary as above but also change the names of the key. dict1_keys = {k*4:v for (k,v) in dict1.items()} print(dict1_keys) . {'aaaa': 1, 'bbbb': 2, 'cccc': 3, 'dddd': 4, 'eeee': 5} . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.b%20-%20Dictionary%20Comprehensions.html#python-dictionary-comprehensions",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.b%20-%20Dictionary%20Comprehensions.html#python-dictionary-comprehensions"
  },"304": {
    "doc": "04.b - Dictionary Comprehensions",
    "title": "Why Use Dictionary Comprehension?",
    "content": "Dictionary comprehension is a powerful concept and can be used to substitute for loops and lambda functions. However, not all for loop can be written as a dictionary comprehension but all dictionary comprehension can be written with a for loop. Consider the following problem, where you want to create a new dictionary where the key is a number divisible by 2 in a range of 0-10 and it’s value is the square of the number. Alternative to for loops . For loops are used to repeat a certain operation or a block of instructions in a program for a given number of times. However, nested for loops (for loop inside another for loop) can get confusing and complex. Dictionary comprehensions are better in such situations and can simplify the readability and your understanding of the code. Let’s see how the same probem can be solved using a for loop and dictionary comprehension: . numbers = range(10) new_dict_for = {} # Add values to `new_dict` using for loop for n in numbers: if n % 2 == 0: new_dict_for[n] = n**2 print(new_dict_for) . {0: 0, 2: 4, 4: 16, 6: 36, 8: 64} . # Use dictionary comprehension new_dict_comp = {n:n**2 for n in numbers if n % 2 == 0} print(new_dict_comp) . {0: 0, 2: 4, 4: 16, 6: 36, 8: 64} . Alternative to lambda functions . Lambda functions are a way of creating small anonymous functions. They are functions without a name. These functions are throw-away functions, which are only needed where they have been created. Lambda functions are mainly used in combination with the functions filter(), map() and reduce(). Let’s look at the lambda function along with the map() function: . # Initialize `fahrenheit` dictionary fahrenheit = {'t1':-30, 't2':-20, 't3':-10, 't4':0} #Get the corresponding `celsius` values celsius = map(lambda x: (float(5)/9)*(x-32), fahrenheit.values()) celsius = list(celsius) #Create the `celsius` dictionary celsius_dict = dict(zip(fahrenheit.keys(), celsius)) print(celsius_dict) . {'t1': -34.44444444444444, 't2': -28.88888888888889, 't3': -23.333333333333336, 't4': -17.77777777777778} . Now, let’s try to solve the same problem using dictionary comprehension: . # Initialize the `fahrenheit` dictionary fahrenheit = {'t1': -30,'t2': -20,'t3': -10,'t4': 0} # Get the corresponding `celsius` values and create the new dictionary celsius = {k:(float(5)/9)*(v-32) for (k,v) in fahrenheit.items()} print(celsius_dict) . {'t1': -34.44444444444444, 't2': -28.88888888888889, 't3': -23.333333333333336, 't4': -17.77777777777778} . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.b%20-%20Dictionary%20Comprehensions.html#why-use-dictionary-comprehension",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.b%20-%20Dictionary%20Comprehensions.html#why-use-dictionary-comprehension"
  },"305": {
    "doc": "04.b - Dictionary Comprehensions",
    "title": "Adding Conditionals to Dictionary Comprehension",
    "content": "You often need to add conditions to a solution while tackling problems. Let’s explore how you can add conditionals into dictionary comprehension to make it more powerful. If Condition . Let’s suppose you need to create a new dictionary from a given dictionary but with items that are greater than 2. This means that you need to add a condition to the original template you saw above… . dict1 = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5} # Check for items greater than 2 dict1_cond = {k:v for (k,v) in dict1.items() if v &gt; 2} print(dict1_cond) . {'c': 3, 'd': 4, 'e': 5} . Multiple If Conditions . In the problem above, what if you have to not only get the items greater than 2 but also need to check if they are multiples of 2 at the same time. dict1_doubleCond = {k:v for (k,v) in dict1.items() if v&gt;2 if v%2 == 0} print(dict1_doubleCond) . {'d': 4} . dict1_doubleCond = {k:v for (k,v) in dict1.items() if v&gt;2 and v%2 == 0} print(dict1_doubleCond) . {'d': 4} . dict1_doubleCond = {k:v for (k,v) in dict1.items() if v&gt;2 or v%2 == 0} print(dict1_doubleCond) . {'b': 2, 'c': 3, 'd': 4, 'e': 5} . The solution to adding multiple conditionals is as easy as simply adding the conditions one after another in your comprehension. However, you need to be careful about what you are trying to do in the problem. Remember, that the consecutive if statements work as if they had and clauses between them. Lets see one more example with three conditionals: . dict1 = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f':6} dict1_tripleCond = {k:v for (k,v) in dict1.items() if v&gt;2 if v%2 == 0 if v%3 == 0} print(dict1_tripleCond) . {'f': 6} . In a for loop, this will correspond to: . dict1_tripleCond = {} for (k,v) in dict1.items(): if (v&gt;=2 and v%2 == 0 and v%3 == 0): dict1_tripleCond[k] = v print(dict1_tripleCond) . {'f': 6} . dict1_tripleCond = {} for (k,v) in dict1.items(): if v&gt;=2: if v%2 == 0: if v%3 == 0: dict1_tripleCond[k] = v print(dict1_tripleCond) . {'f': 6} . If-Else Conditions . Dealing with an if-else condition is also easy with dictionary comprehension. Check out the following example to see it for yourself: . dict1 = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f':6} # Identify odd and even entries dict1_tripleCond = {k:('even' if v%2==0 else 'odd') for (k,v) in dict1.items()} print(dict1_tripleCond) . {'a': 'odd', 'b': 'even', 'c': 'odd', 'd': 'even', 'e': 'odd', 'f': 'even'} . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.b%20-%20Dictionary%20Comprehensions.html#adding-conditionals-to-dictionary-comprehension",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.b%20-%20Dictionary%20Comprehensions.html#adding-conditionals-to-dictionary-comprehension"
  },"306": {
    "doc": "04.b - Dictionary Comprehensions",
    "title": "Nested Dictionary Comprehension",
    "content": "Nesting is a programming concept where data is organized in layers, or where objects contain other similar objects. You must have often seen a nested ‘if’ structure, which is an if condition inside another if condition. Similarly, dictionaries can be nested and thus their comprehensions can be nested as well. Let’s see what this means: . nested_dict = {'first':{'a':1}, 'second':{'b':2}} float_dict = { outer_k: {inner_k:float(inner_v) for (inner_k, inner_v) in outer_v.items()} for (outer_k, outer_v) in nested_dict.items() } print(float_dict) . {'first': {'a': 1.0}, 'second': {'b': 2.0}} . This is an example of a nested dictionary. The nested_dict is a dictionary with the keys: first and second, which hold dictionary objects in their values. The code works with the inner dictionary values and converts them to float and then combines the outer keys with the new float inner values into a new dictionary. The code also has a nested dictionary comprehension, which is dictionary comprehension inside another one. The dictionary comprehension when nested as you can see can get pretty hard to read as well as understand, which takes away the whole point of using comprehensions in the first place. As the structure of the dictionary you are working with gets complicated, the dictionary comprehension starts to get complicated as well. For such situations, you might be better off not using complicated comprehensions in your code. Note that you can rewrite the above code chunk also with a nested for loop: . nested_dict = {'first':{'a':1}, 'second':{'b':2}} for (outer_k, outer_v) in nested_dict.items(): for (inner_k, inner_v) in outer_v.items(): outer_v.update({inner_k: float(inner_v)}) nested_dict.update({outer_k:outer_v}) print(nested_dict) . {'first': {'a': 1.0}, 'second': {'b': 2.0}} . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.b%20-%20Dictionary%20Comprehensions.html#nested-dictionary-comprehension",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.b%20-%20Dictionary%20Comprehensions.html#nested-dictionary-comprehension"
  },"307": {
    "doc": "04.b - Dictionary Comprehensions",
    "title": "04.b - Dictionary Comprehensions",
    "content": " ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.b%20-%20Dictionary%20Comprehensions.html",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/04.b%20-%20Dictionary%20Comprehensions.html"
  },"308": {
    "doc": "04.b - Even More Web Scraping with BeautifulSoup",
    "title": "Using Python BeautifulSoup to scrape DataCamp Tutorials &amp; Analyze",
    "content": "Source . In this tutorial, we are going to scrape the tutorials section of the DataCamp website and try to get some insights. | Most contributing authors | Timeline of contributors (How it all started!) | Comparing upvotes vs. number of articles published | . Before that, the website will be scraped using python’s BeautifulSoup package. To understand the page structure, Chrome browser developer tools will need to be used. This is done to identify the Classes that will be searched to get the required information. The following information will be gathered from the page: . | Author | Publish Date | Title | Description | Up Votes | . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#using-python-beautifulsoup-to-scrape-datacamp-tutorials--analyze",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#using-python-beautifulsoup-to-scrape-datacamp-tutorials--analyze"
  },"309": {
    "doc": "04.b - Even More Web Scraping with BeautifulSoup",
    "title": "Importing Libraries",
    "content": "We’ll start by importing the necessary libraries as follows: . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline import re import time from datetime import datetime import matplotlib.dates as mdates import matplotlib.ticker as ticker from urllib.request import urlopen from bs4 import BeautifulSoup . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#importing-libraries",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#importing-libraries"
  },"310": {
    "doc": "04.b - Even More Web Scraping with BeautifulSoup",
    "title": "Determining Pages to be Scraped",
    "content": "A sample URL that we’re going to loop and scrape is the following https://www.datacamp.com/community/tutorials?page=2. As we can see, the page=2 argument changes for each page. In order to loop through all the pages to get the necessary dataset, we need to find out the number of pages. The following lines of code do just that. url = \"https://www.datacamp.com/community/tutorials\" html = urlopen(url) soup = BeautifulSoup(html, 'html') pages = [i.text for i in soup.find_all('a') if 'community/tutorials?page=' in str(i)] lastpage = pages[-1] . print(lastpage) . 22 . The illustration is as follows: . | Specified the url to a variable | Opened the url using urlopen which was imported earlier | Scraped the specified page and assigned it to soup variable | Identified all hyperlinks on the page using list comprehension and filtered for those having community/tutorials?page= in it | The text value of the last found url is the last page that needs to be scraped | . We proceed by declaring list variables that will hold the scraped values for the columns we intend as mentioned earlier . description=[] upvote=[] author=[] publishdate=[] title=[] . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#determining-pages-to-be-scraped",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#determining-pages-to-be-scraped"
  },"311": {
    "doc": "04.b - Even More Web Scraping with BeautifulSoup",
    "title": "Doing the Actual Scrape",
    "content": "Now that we know how many pages we need to scrape and have declared our variables, we will now use a for loop and go through each page one by one to get our fields of interest as shown below. Note that we will end up having list of lists for each column of interest, but later we will be flattening the list further so that it can be used for Data Frames. for cp in np.arange(1,int(lastpage)+1): url = \"https://www.datacamp.com/community/tutorials?page=\" + str(cp) html = urlopen(url) soup = BeautifulSoup(html, 'html') description.append([i.text for i in soup.find_all(class_='jsx-379356511 blocText description')]) # upvote.append([i.text for i in soup.find_all(class_='jsx-4192737526 voted')]) upvote.append([i.text for i in soup.find_all(class_='jsx-1972554161 voted')]) author.append([i.text for i in soup.find_all(class_='jsx-566588255 name')]) publishdate.append([i.text for i in soup.find_all(class_='jsx-566588255 date')]) title.append([i.text for i in soup.find_all(class_='jsx-379356511 blue')]) time.sleep(3) print (\"Done!\") . Done! . Here is what happened in the above code segment . | Set the url to a variable | Opened the url using urlopen which was imported earlier | Scraped the specified page and assigned it to soup variable | Identified and extracted values for Description, Up Vote, Author, Publish Date, Title by using their relevant class names. These class names were found using Developer Tools | The time function has been used to be easy on the website this time :)### 3.b Display the soup object to visually interrogate | . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#doing-the-actual-scrape",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#doing-the-actual-scrape"
  },"312": {
    "doc": "04.b - Even More Web Scraping with BeautifulSoup",
    "title": "Flattening List of Lists",
    "content": "Since the values we got are list of lists, they will now be flatted using the following code segment: . descriptionflat = [y for x in description for y in x] len(descriptionflat) . 312 . upvoteflat = [y for x in upvote for y in x] len(upvoteflat) . 312 . authorflat = [y for x in author for y in x] len(authorflat) . 312 . publishdateflat = [y for x in publishdate for y in x] len(publishdateflat) . 312 . titleflat = [y for x in title for y in x] len(titleflat) . 312 . publishdateformatted = [datetime.strptime(re.sub('rd, ', ', ', re.sub('st, ', ', ', re.sub('nd, ', ', ', re.sub('th, ',', ',a)))), \"%B %d, %Y\") for a in publishdateflat] len(publishdateformatted) . 312 . The last statement in the cell above converts the date values (which are currently in String Format) to DateTime. ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#flattening-list-of-lists",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#flattening-list-of-lists"
  },"313": {
    "doc": "04.b - Even More Web Scraping with BeautifulSoup",
    "title": "Making a Data Frame and Saving as CSV File",
    "content": "The lists will now be grouped into a dictionary, and a data frame will be created for further analysis. The last command saves the data frame to a CSV file so that it can be used later on. cdata = {\"author\":authorflat, \"publishdate\":publishdateformatted, \"title\":titleflat, \"description\":descriptionflat, \"upvote\":upvoteflat} df = pd.DataFrame(data=cdata) df.to_csv(\"datacamp.csv\", header=True, index=False) . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#making-a-data-frame-and-saving-as-csv-file",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#making-a-data-frame-and-saving-as-csv-file"
  },"314": {
    "doc": "04.b - Even More Web Scraping with BeautifulSoup",
    "title": "Reading CSV File",
    "content": "Now we are attempting to read into the collected dataset from the CSV file we just created. data = pd.read_csv(\"datacamp.csv\", parse_dates=[\"publishdate\"], infer_datetime_format=True) . data.shape . (311, 5) . The above command tells us we’re dealing with a dataset of 176 rows and 5 columns. data.head() . | | author | publishdate | title | description | upvote | . | 0 | Aditya Sharma | 2019-04-26 | Graphs in Spreadsheets | In this tutorial, you'll learn how to create v... | 9 | . | 1 | Sayak Paul | 2019-04-24 | Cleaning Data in SQL | In this tutorial, you'll learn techniques on h... | 4 | . | 2 | Francisco Javier Carrera Arias | 2019-04-19 | SQLite in R | In this tutorial, you will learn about using S... | 4 | . | 3 | Parul Pandey | 2019-04-18 | Data Visualization with Power BI | Learn how to analyze and display data using Po... | 8 | . | 4 | Sayak Paul | 2019-04-17 | Aggregate Functions in SQL | Learn how to use aggregate functions for summa... | 4 | . Showing the first 5 rows of the dataset above using the head function. data['publishyymm'] = data['publishdate'].dt.strftime(\"%Y-%b\") data[\"posts\"] = 1 . | The first line in the above code section creates a new column with the publish date formatted as a Year-Month format. | The second line assigns value 1 to a new column posts being used later. | . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#reading-csv-file",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#reading-csv-file"
  },"315": {
    "doc": "04.b - Even More Web Scraping with BeautifulSoup",
    "title": "Tutorials Count by Time",
    "content": "Here we will organize the count of Tutorials over Timeline of Year and Month: . datacamp.groupby([datacamp['publishdate'].dt.year, datacamp['publishdate'].dt.month]).size().plot(kind='bar', figsize=(15,7), color='b') . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1a2bda90&gt; . Since 2017 January . Since the duration from 2013 till 2016 represents very few posts, we will be ignoring them from now and considering posts starting Jan 2017. A filter will be applied for that as shown below . data[data[\"publishdate\"]&gt;='2017-01-01'] \\ .sort_values(by=\"publishdate\", ascending=True) \\ .groupby([data['publishyymm']],sort=False) \\ .size() \\ .plot(kind='bar', figsize=(15,7), color='b') . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1627b828&gt; . Tutorials have paced up in 2018 with especially starting March onwards with a consistent upwards pace. Since this data was pulled in mid-August and almost crossing the middle line of July’s tutorials count, we might be having August as the month with highest posts so far this year! . Top Authors Graph . While we’re pacing up on all these tutorials, who have been contributing on them along the way? Here we go with a simple bar chart highlighting this very fact. data[data[\"publishdate\"]&gt;='2017-01-01'][\"author\"] \\ .value_counts(sort=True, ascending=False)[:10] \\ .plot(kind='bar') . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1b218f60&gt; . Top Authors List . Let’s build a list of these as well while we are at it. We’ll be using this soon below: . topauthors = data[data[\"publishdate\"]&gt;='2017-01-01'][\"author\"] \\ .value_counts(sort=True, ascending=False)[:10] \\ .index . This is what happened in the code section above. | Limited Result set to Tutorials since 2017 January | Select only Author field | Aggregated results using the value_counts function | Sorted the result set in descending order and limited it to the first 10 rows | . Tutorials Paced over Timeline . Now what we’re going to focus on is since when and at what pace have these top 10 contributors been posting tutorials. For this, we will be using the list we just created along with some transformations to come up with a stacked bar chart that we need. dh = data[data[\"publishdate\"]&gt;='2017-01-01'] \\ .sort_values(by=\"publishdate\", ascending=True) \\ # .set_index([\"publishdate\"]) . dh.head() . | | author | publishdate | title | description | upvote | publishyymm | posts | . | 289 | Karlijn Willems | 2017-01-10 | 15 Easy Solutions To Your Data Frame Problems ... | Discover how to create a data frame in R, chan... | 49 | 2017-Jan | 1 | . | 288 | Ted Kwartler | 2017-01-12 | Web Scraping and Parsing Data in R | Explorin... | Learn how to scrape data from the web, preproc... | 14 | 2017-Jan | 1 | . | 287 | Ted Kwartler | 2017-01-19 | Exploring H-1B Data with R: Part 2 | Learn even more about exploratory data analysi... | 4 | 2017-Jan | 1 | . | 286 | Ted Kwartler | 2017-01-26 | Exploring H-1B Data with R: Part 3 | Learn how to geocode locations and map them wi... | 6 | 2017-Jan | 1 | . | 285 | Yao-Jen Kuo | 2017-01-27 | Scikit-Learn 教學：Python 與機器學習 | 簡單易懂的 scikit-learn 教學，適合想要使用 Python 實作機器學習的初學者閱讀。 | 36 | 2017-Jan | 1 | . dh[\"publishdateone\"] = pd.to_datetime(dh.publishdate.astype(str).str[0:7]+'-01') . This is what happened in the code section above. | Limited Result set to Tutorials since 2017 January | Sorted by Publish Date | Making Publish Date as the Index column | . Now that we’re going to visualize using a stacked bar chart, the data set will now be pivoted by having the date field as the Index, Posts as values which are to be aggregated, and Authors as columns. dhp = dh[dh[\"author\"].isin(topauthors)] \\ .pivot_table(index=\"publishdateone\",values=\"posts\",columns=\"author\", aggfunc=np.sum) . fig, ax = plt.subplots(figsize=(15,7)) dhp.plot(ax=ax, kind='bar', stacked=True) ticklabels = [item.strftime('%Y %b') for item in dhp.index] ax.xaxis.set_major_formatter(ticker.FixedFormatter(ticklabels)) . Here is what we possibly get out of the above Chart along with considering the earlier visualization of Tutorial Counts by Authors. Looking at Upvotes vs. Tutorials . When a reader likes a tutorial, an upvote is signaled respectively. Let’s see who’s managed to get a good amount of upvotes vs. number of tutorials they have posted! We’ll be considering the top 10 contributors in this case as well. This will be done by using a scatter plot. upvotes = dh[dh[\"author\"].isin(topauthors)] \\ .groupby(['author'], as_index=False) \\ .agg({'posts':\"sum\", 'upvote': \"sum\"}) . sns.lmplot('posts', 'upvote', data=upvotes, fit_reg=False, hue=\"author\", scatter_kws={\"marker\": \"D\", \"s\": 100}) . &lt;seaborn.axisgrid.FacetGrid at 0x1a1a985c18&gt; . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#tutorials-count-by-time",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#tutorials-count-by-time"
  },"316": {
    "doc": "04.b - Even More Web Scraping with BeautifulSoup",
    "title": "Conclusion",
    "content": "In this tutorial, we have managed to achieve the following: . | Scrape tutorials list across all pages | Create a Data Frame and save it as CSV for later reference and Analysis | Explored it using Pandas and Matplotlib along with some transformations | Used Line, Bar, Stacked Bar and Scatter Plots to visualize | . ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#conclusion",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html#conclusion"
  },"317": {
    "doc": "04.b - Even More Web Scraping with BeautifulSoup",
    "title": "04.b - Even More Web Scraping with BeautifulSoup",
    "content": " ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping/04.b%20-%20Even%20More%20Web%20Scraping%20with%20BeautifulSoup.html"
  },"318": {
    "doc": "05 - Generating Other Synthetic Data",
    "title": "Generating Other Synthetic Data",
    "content": "Source . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#generating-other-synthetic-data",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#generating-other-synthetic-data"
  },"319": {
    "doc": "05 - Generating Other Synthetic Data",
    "title": "What kind of data may be needed for a rich learning experience?",
    "content": "Imagine you are tinkering with a cool machine learning algorithm like SVM or a deep neural net. What kind of dataset you should practice them on? If you are learning from scratch, the advice is to start with simple, small-scale datasets which you can plot in two dimensions to understand the patterns visually and see for yourself the working of the ML algorithm in an intuitive fashion. For example, here is an excellent article on various datasets you can try at various level of learning. ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#what-kind-of-data-may-be-needed-for-a-rich-learning-experience",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#what-kind-of-data-may-be-needed-for-a-rich-learning-experience"
  },"320": {
    "doc": "05 - Generating Other Synthetic Data",
    "title": "What is a synthetic dataset?",
    "content": "As the name suggests, quite obviously, a synthetic dataset is a repository of data that is generated programmatically. So, it is not collected by any real-life survey or experiment. Its main purpose, therefore, is to be flexible and rich enough to help an ML practitioner conduct fascinating experiments with various classification, regression, and clustering algorithms. Desired properties are, . | It can be numerical, binary, or categorical (ordinal or non-ordinal), | The number of features and length of the dataset should be arbitrary | It should preferably be random and the user should be able to choose a wide variety of statistical distribution to base this data upon i.e. the underlying random process can be precisely controlled and tuned, | If it is used for classification algorithms, then the degree of class separation should be controllable to make the learning problem easy or hard, | Random noise can be interjected in a controllable manner | For a regression problem, a complex, non-linear generative process can be used for sourcing the data | . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#what-is-a-synthetic-dataset",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#what-is-a-synthetic-dataset"
  },"321": {
    "doc": "05 - Generating Other Synthetic Data",
    "title": "What about privacy concerns?",
    "content": "Although in this article, we keep our discussions limited to synthetic data for better ML algorithms, its purpose can be far reaching in cases where it helps get around security and privacy concerns with real datasets, that cannot be used or acquired for learning purpose. For example, think about medical or military data. Here is an excellent summary article about such methods. import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#what-about-privacy-concerns",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#what-about-privacy-concerns"
  },"322": {
    "doc": "05 - Generating Other Synthetic Data",
    "title": "Regression problem generation",
    "content": "Regression problem generation: Scikit-learn’s dataset.make_regression function can create random regression problem with arbitrary number of input features, output targets, and controllable degree of informative coupling between them. It can also mix Gaussian noise. from sklearn.datasets import make_regression . data1 = make_regression(n_samples=20, n_features=4, n_informative=2, n_targets=1, bias=0.0, effective_rank=None,tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None) df1 = pd.DataFrame(data1[0],columns=['x'+str(i) for i in range(1,5)]) df1['y'] = data1[1] . df1.head() . | | x1 | x2 | x3 | x4 | y | . | 0 | 2.326839 | 2.203741 | -0.152775 | -0.682349 | 70.815411 | . | 1 | -0.584208 | 0.097921 | -0.045379 | 1.043689 | 1.670565 | . | 2 | 0.100703 | -0.141215 | 1.752348 | 1.166683 | 62.111800 | . | 3 | 0.308948 | 1.741082 | 1.159611 | -1.625781 | 104.917778 | . | 4 | 1.320515 | 2.320092 | 1.882030 | -1.058409 | 152.690260 | . Plot . plt.figure(figsize=(15,10)) for i in range(1,5): fit = np.polyfit(df1[df1.columns[i-1]],df1['y'],1) fit_fn = np.poly1d(fit) plt.subplot(2,2,i) plt.scatter(df1[df1.columns[i-1]],df1['y'],s=200,c='orange',edgecolor='k') plt.plot(df1[df1.columns[i-1]],fit_fn(df1[df1.columns[i-1]]),'b-',lw=3) plt.grid(True) . Data with Gaussian noise . data2 = make_regression(n_samples=20, n_features=4, n_informative=2, n_targets=1, bias=0.0, effective_rank=None,tail_strength=0.5, noise=2.0, shuffle=True, coef=False, random_state=None) df2 = pd.DataFrame(data2[0],columns=['x'+str(i) for i in range(1,5)]) df2['y'] = data2[1] . Plot . plt.figure(figsize=(15,10)) for i in range(1,5): fit = np.polyfit(df2[df2.columns[i-1]],df2['y'],1) fit_fn = np.poly1d(fit) plt.subplot(2,2,i) plt.scatter(df2[df2.columns[i-1]],df2['y'],s=200,c='orange',edgecolor='k') plt.plot(df2[df2.columns[i-1]],fit_fn(df2[df2.columns[i-1]]),'b-',lw=3) plt.grid(True) . Plot datasets with varying degree of noise . plt.figure(figsize=(15,6)) df2 = pd.DataFrame(data=np.zeros((20,1))) for i in range(3): data2 = make_regression(n_samples=20, n_features=1, n_informative=1, n_targets=1, bias=0.0, effective_rank=None,tail_strength=0.5, noise=i*10, shuffle=True, coef=False, random_state=None) df2['x'+str(i+1)]=data2[0] df2['y'+str(i+1)] = data2[1] for i in range(3): fit = np.polyfit(df2['x'+str(i+1)],df2['y'+str(i+1)],1) fit_fn = np.poly1d(fit) plt.subplot(1,3,i+1) plt.scatter(df2['x'+str(i+1)],df2['y'+str(i+1)],s=200,c='orange',edgecolor='k') plt.plot(df2['x'+str(i+1)],fit_fn(df2['x'+str(i+1)]),'b-',lw=3) plt.grid(True) . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#regression-problem-generation",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#regression-problem-generation"
  },"323": {
    "doc": "05 - Generating Other Synthetic Data",
    "title": "Classification problem generation",
    "content": "Classification problem generation: Similar to the regression function above, dataset.make_classification generates a random multi-class classification problem (dataset) with controllable class separation and added noise. You can also randomly flip any percentage of output signs to create a harder classification dataset if you want. from sklearn.datasets import make_classification . data3 = make_classification(n_samples=20, n_features=4, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None) df3 = pd.DataFrame(data3[0],columns=['x'+str(i) for i in range(1,5)]) df3['y'] = data3[1] . df3.head() . | | x1 | x2 | x3 | x4 | y | . | 0 | -0.937800 | 0.620428 | 1.710371 | -1.335103 | 0 | . | 1 | -1.843944 | 1.498890 | -0.950427 | -0.315925 | 0 | . | 2 | -0.670669 | 0.493982 | -0.687241 | -1.206348 | 0 | . | 3 | 3.140552 | -2.195994 | 1.571735 | -0.247905 | 1 | . | 4 | 1.602336 | -2.151781 | 4.427198 | -4.207194 | 0 | . Plot . from itertools import combinations from math import ceil lst_var=list(combinations(df3.columns[:-1],2)) len_var = len(lst_var) plt.figure(figsize=(18,10)) for i in range(1,len_var+1): plt.subplot(2,ceil(len_var/2),i) var1 = lst_var[i-1][0] var2 = lst_var[i-1][1] plt.scatter(df3[var1],df3[var2],s=200,c=df3['y'],edgecolor='k') plt.xlabel(var1,fontsize=14) plt.ylabel(var2,fontsize=14) plt.grid(True) . Making class separation easy by tweaking class_sep . data3 = make_classification(n_samples=20, n_features=4, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=3.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None) df3 = pd.DataFrame(data3[0],columns=['x'+str(i) for i in range(1,5)]) df3['y'] = data3[1] . from itertools import combinations from math import ceil lst_var=list(combinations(df3.columns[:-1],2)) len_var = len(lst_var) plt.figure(figsize=(18,10)) for i in range(1,len_var+1): plt.subplot(2,ceil(len_var/2),i) var1 = lst_var[i-1][0] var2 = lst_var[i-1][1] plt.scatter(df3[var1],df3[var2],s=200,c=df3['y'],edgecolor='k') plt.xlabel(var1,fontsize=14) plt.ylabel(var2,fontsize=14) plt.grid(True) . Making class separation hard by tweaking class_sep . data3 = make_classification(n_samples=20, n_features=4, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=0.5, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None) df3 = pd.DataFrame(data3[0],columns=['x'+str(i) for i in range(1,5)]) df3['y'] = data3[1] . from itertools import combinations from math import ceil lst_var=list(combinations(df3.columns[:-1],2)) len_var = len(lst_var) plt.figure(figsize=(18,10)) for i in range(1,len_var+1): plt.subplot(2,ceil(len_var/2),i) var1 = lst_var[i-1][0] var2 = lst_var[i-1][1] plt.scatter(df3[var1],df3[var2],s=200,c=df3['y'],edgecolor='k') plt.xlabel(var1,fontsize=14) plt.ylabel(var2,fontsize=14) plt.grid(True) . Making data noisy by increasing flip_y . plt.figure(figsize=(18,10)) for i in range(6): data3 = make_classification(n_samples=20, n_features=4, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.1*i, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=False, random_state=101) df3 = pd.DataFrame(data3[0],columns=['x'+str(i) for i in range(1,5)]) df3['y'] = data3[1] plt.subplot(2,3,i+1) plt.title(f\"Plot for flip_y={round(0.1*i,2)}\") plt.scatter(df3['x1'],df3['x2'],s=200,c=df3['y'],edgecolor='k') plt.xlabel('x1',fontsize=14) plt.ylabel('x2',fontsize=14) plt.grid(True) . Plot datasets with varying degree of class separation . plt.figure(figsize=(18,5)) df2 = pd.DataFrame(data=np.zeros((20,1))) for i in range(3): data2 = make_classification(n_samples=20, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0, class_sep=i+0.5, hypercube=True, shift=0.0, scale=1.0, shuffle=False, random_state=101) df2['x'+str(i+1)+'1']=data2[0][:,0] df2['x'+str(i+1)+'2']=data2[0][:,1] df2['y'+str(i+1)] = data2[1] for i in range(3): plt.subplot(1,3,i+1) plt.scatter(df2['x'+str(i+1)+'1'],df2['x'+str(i+1)+'2'],s=200,c=df2['y'+str(i+1)],edgecolor='k') plt.grid(True) . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#classification-problem-generation",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#classification-problem-generation"
  },"324": {
    "doc": "05 - Generating Other Synthetic Data",
    "title": "Clustering problem generation",
    "content": "Clustering problem generation: There are quite a few functions for generating interesting clusters. The most straightforward one is datasets.make_blobs, which generates arbitrary number of clusters with controllable distance parameters. from sklearn.datasets import make_blobs . data4 = make_blobs(n_samples=60, n_features=4, centers=3, cluster_std=1.0, center_box=(-5.0, 5.0), shuffle=True, random_state=None) df4 = pd.DataFrame(data4[0],columns=['x'+str(i) for i in range(1,5)]) df4['y'] = data4[1] . from itertools import combinations from math import ceil lst_var=list(combinations(df4.columns[:-1],2)) len_var = len(lst_var) plt.figure(figsize=(18,10)) for i in range(1,len_var+1): plt.subplot(2,ceil(len_var/2),i) var1 = lst_var[i-1][0] var2 = lst_var[i-1][1] plt.scatter(df4[var1],df4[var2],s=200,c=df4['y'],edgecolor='k') plt.xlabel(var1,fontsize=14) plt.ylabel(var2,fontsize=14) plt.grid(True) . Making clusters compact and easily separable by tweaking cluster_std . data4 = make_blobs(n_samples=60, n_features=4, centers=3, cluster_std=0.3, center_box=(-5.0, 5.0), shuffle=True, random_state=None) df4 = pd.DataFrame(data4[0],columns=['x'+str(i) for i in range(1,5)]) df4['y'] = data4[1] . from itertools import combinations from math import ceil lst_var=list(combinations(df4.columns[:-1],2)) len_var = len(lst_var) plt.figure(figsize=(18,10)) for i in range(1,len_var+1): plt.subplot(2,ceil(len_var/2),i) var1 = lst_var[i-1][0] var2 = lst_var[i-1][1] plt.scatter(df4[var1],df4[var2],s=200,c=df4['y'],edgecolor='k') plt.xlabel(var1,fontsize=14) plt.ylabel(var2,fontsize=14) plt.grid(True) . Making clusters spread out and difficult to separate by tweaking cluster_std . data4 = make_blobs(n_samples=60, n_features=4, centers=3, cluster_std=2.5, center_box=(-5.0, 5.0), shuffle=True, random_state=None) df4 = pd.DataFrame(data4[0],columns=['x'+str(i) for i in range(1,5)]) df4['y'] = data4[1] . from itertools import combinations from math import ceil lst_var=list(combinations(df4.columns[:-1],2)) len_var = len(lst_var) plt.figure(figsize=(18,10)) for i in range(1,len_var+1): plt.subplot(2,ceil(len_var/2),i) var1 = lst_var[i-1][0] var2 = lst_var[i-1][1] plt.scatter(df4[var1],df4[var2],s=200,c=df4['y'],edgecolor='k') plt.xlabel(var1,fontsize=14) plt.ylabel(var2,fontsize=14) plt.grid(True) . Making anisotropically distributed clustering problem . Anisotropic cluster generation: With a simple transformation using matrix multiplication, you can generate clusters which is aligned along certain axis or anisotropically distributed. data5 = make_blobs(n_samples=50, n_features=2, centers=3,cluster_std=1.5) . transformation = [[0.5, -0.5], [-0.4, 0.8]] . data5_0=np.dot(data5[0],transformation) df5 = pd.DataFrame(data5_0,columns=['x'+str(i) for i in range(1,3)]) df5['y'] = data5[1] . plt.figure(figsize=(8,5)) plt.scatter(df5['x1'],df5['x2'],c=df5['y'],s=200,edgecolors='k') plt.xlabel('x1',fontsize=14) plt.ylabel('x2',fontsize=14) plt.grid(True) plt.show() . Making concentric circle clusters . Concentric ring cluster data generation: For testing affinity based clustering algorithm or Gaussian mixture models, it is useful to have clusters generated in a special shape. We can use datasets.make_circles function to accomplish that. from sklearn.datasets import make_circles . data6 = make_circles(n_samples=50, shuffle=True, noise=None, random_state=None, factor=0.6) df6 = pd.DataFrame(data6[0],columns=['x'+str(i) for i in range(1,3)]) df6['y'] = data6[1] . plt.figure(figsize=(8,5)) plt.scatter(df6['x1'],df6['x2'],c=df6['y'],s=200,edgecolors='k') plt.xlabel('x1',fontsize=14) plt.ylabel('x2',fontsize=14) plt.grid(True) plt.show() . Introdue noise in the circle clusters . data6 = make_circles(n_samples=50, shuffle=True, noise=0.15, random_state=None, factor=0.6) df6 = pd.DataFrame(data6[0],columns=['x'+str(i) for i in range(1,3)]) df6['y'] = data6[1] . plt.figure(figsize=(8,5)) plt.scatter(df6['x1'],df6['x2'],c=df6['y'],s=200,edgecolors='k') plt.xlabel('x1',fontsize=14) plt.ylabel('x2',fontsize=14) plt.grid(True) plt.show() . Make moon shape clusters . Moon-shaped cluster data generation: We can also generate moon-shaped cluster data for testing algorithms, with controllable noise using datasets.make_moons function. from sklearn.datasets import make_moons . data7 = make_moons(n_samples=50, shuffle=True, noise=None, random_state=None) df7 = pd.DataFrame(data7[0],columns=['x'+str(i) for i in range(1,3)]) df7['y'] = data7[1] . plt.figure(figsize=(8,5)) plt.scatter(df7['x1'],df7['x2'],c=df7['y'],s=200,edgecolors='k') plt.xlabel('x1',fontsize=14) plt.ylabel('x2',fontsize=14) plt.grid(True) plt.show() . Introduce noise in the moon-shaped clusters . data7 = make_moons(n_samples=50, shuffle=True, noise=0.1, random_state=None) df7 = pd.DataFrame(data7[0],columns=['x'+str(i) for i in range(1,3)]) df7['y'] = data7[1] . plt.figure(figsize=(8,5)) plt.scatter(df7['x1'],df7['x2'],c=df7['y'],s=200,edgecolors='k') plt.xlabel('x1',fontsize=14) plt.ylabel('x2',fontsize=14) plt.grid(True) plt.show() . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#clustering-problem-generation",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#clustering-problem-generation"
  },"325": {
    "doc": "05 - Generating Other Synthetic Data",
    "title": "Random regression/classification problem generation using symbolic function",
    "content": "from Symbolic_regression_classification_generator import gen_regression_symbolic, gen_classification_symbolic . Generate regression data with a symbolic expression of: . \\(\\frac{x_1^2}{2}-3x_2+20.\\text{sin}(x_3)\\) . data8 = gen_regression_symbolic(m='((x1^2)/2-3*x2)+20*sin(x3)',n_samples=50,noise=0.01) df8=pd.DataFrame(data8, columns=['x'+str(i) for i in range(1,4)]+['y']) . df8.head() . | | x1 | x2 | x3 | y | . | 0 | -1.995143 | 1.969439 | -6.585953 | -9.86940209041576 | . | 1 | 6.694431 | 10.646075 | -9.944827 | 0.416231121214657 | . | 2 | 0.824618 | -2.154371 | 1.861316 | 25.9776280245960 | . | 3 | -3.778498 | 4.509513 | 11.273044 | -25.6259760456950 | . | 4 | -1.664324 | 3.435822 | -4.291008 | 9.33521803074928 | . plt.figure(figsize=(18,5)) for i in range(1,4): plt.subplot(1,3,i) plt.scatter(df8[df8.columns[i-1]],df8['y'],s=200,c='orange',edgecolor='k') plt.grid(True) . Generate regression data with a symbolic expression of: . \\(x_1^2*sin(x_1)\\) . data8 = 0.1*gen_regression_symbolic(m='x1^2*sin(x1)',n_samples=200,noise=0.05) df8=pd.DataFrame(data8, columns=['x'+str(i) for i in range(1,2)]+['y']) . plt.figure(figsize=(8,5)) plt.scatter(df8['x1'],df8['y'],s=100,c='orange',edgecolor='k') plt.grid(True) . Generate classification data with a symbolic expression of: . \\(\\frac{x_1^2}{3}-\\frac{x_2^2}{15}\\) . data9 = gen_classification_symbolic(m='((x1^2)/3-(x2^2)/15)',n_samples=500,flip_y=0.01) df9=pd.DataFrame(data9, columns=['x'+str(i) for i in range(1,3)]+['y']) . df9.head() . | | x1 | x2 | y | . | 0 | 0.602170 | -6.029703 | 0.0 | . | 1 | 3.255014 | 0.452398 | 1.0 | . | 2 | 7.986863 | 8.692522 | 1.0 | . | 3 | 4.936118 | 3.508951 | 1.0 | . | 4 | 4.838671 | 4.002539 | 1.0 | . plt.figure(figsize=(8,5)) plt.scatter(df9['x1'],df9['x2'],c=df9['y'],s=100,edgecolors='k') plt.xlabel('x1',fontsize=14) plt.ylabel('x2',fontsize=14) plt.grid(True) plt.show() . Generate classification data with a symbolic expression of: . \\(x_1-3.\\text{sin}\\frac{x_2}{2}\\) . data9 = gen_classification_symbolic(m='x1-3*sin(x2/2)',n_samples=500,flip_y=0.01) df9=pd.DataFrame(data9, columns=['x'+str(i) for i in range(1,3)]+['y']) . plt.figure(figsize=(8,5)) plt.scatter(df9['x1'],df9['x2'],c=df9['y'],s=100,edgecolors='k') plt.xlabel('x1',fontsize=14) plt.ylabel('x2',fontsize=14) plt.grid(True) plt.show() . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#random-regressionclassification-problem-generation-using-symbolic-function",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#random-regressionclassification-problem-generation-using-symbolic-function"
  },"326": {
    "doc": "05 - Generating Other Synthetic Data",
    "title": "Categorical data generation using “pydbgen” library",
    "content": "While many high-quality real-life datasets are available on the web for trying out cool machine learning techniques, from my personal experience, I found that the same is not true when it comes to learning SQL. For data science expertise, having a basic familiarity of SQL is almost as important as knowing how to write code in Python or R. But access to a large enough database with real categorical data (such as name, age, credit card, SSN, address, birthday, etc.) is not nearly as common as access to toy datasets on Kaggle, specifically designed or curated for machine learning task. Apart from the beginners in data science, even seasoned software testers may find it useful to have a simple tool where with a few lines of code they can generate arbitrarily large data sets with random (fake) yet meaningful entries. Enter pydbgen. Read the docs here. It is a lightweight, pure-python library to generate random useful entries (e.g. name, address, credit card number, date, time, company name, job title, license plate number, etc.) and save them in either Pandas dataframe object, or as a SQLite table in a database file, or in a MS Excel file. ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#categorical-data-generation-using-pydbgen-library",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#categorical-data-generation-using-pydbgen-library"
  },"327": {
    "doc": "05 - Generating Other Synthetic Data",
    "title": "Generate name, address, phone number, email etc. using Faker package",
    "content": "This is a package you need to install first with pip install faker. from faker import Faker fake = Faker() . Generate a fake name . fake.name() . 'Valerie Lynch' . Generate a license-plate (US style) . fake.license_plate() . '577-EQG' . Generate a full data frame with random name, street address, SSN, email, date . df = pd.DataFrame() for i in range(20): data = { 'name': fake.name(), 'street_address': fake.street_address(), 'city': fake.city(), 'zip_code': fake.zipcode(), 'ssn': fake.ssn(), 'email': fake.email(), 'date': fake.date() } df = df.append(data, True) . df.head(5) . | | name | street_address | city | zip_code | ssn | email | date | . | 0 | Molly Cook | 5539 Gray Inlet Suite 416 | Whitestad | 23319 | 524-03-0773 | anoble@example.com | 1987-03-17 | . | 1 | Katherine Jones | 070 Christina Wells Suite 498 | North Daniel | 04028 | 586-43-5250 | esolis@example.net | 1984-05-21 | . | 2 | Julia Reese | 790 Teresa Cove Suite 479 | Espinozaton | 17207 | 848-13-7165 | jhanson@example.org | 1974-11-02 | . | 3 | Christine Rogers | 0797 Gibson Cove Suite 290 | Silvachester | 52803 | 406-94-4118 | williamschad@example.net | 1983-05-08 | . | 4 | Dennis Mccoy | 109 Michelle Rest | Gomezville | 39863 | 572-81-2883 | fosterjames@example.org | 1982-05-23 | . ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#generate-name-address-phone-number-email-etc-using-faker-package",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html#generate-name-address-phone-number-email-etc-using-faker-package"
  },"328": {
    "doc": "05 - Generating Other Synthetic Data",
    "title": "05 - Generating Other Synthetic Data",
    "content": " ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data/05%20-%20Generating%20Other%20Synthetic%20Data.html"
  },"329": {
    "doc": "05 - Getting Data from Web APIs",
    "title": "Python API tutorial - An Introduction to using APIs",
    "content": "source . Application Program Interfaces, or APIs, are commonly used to retrieve data from remote websites. Sites like Reddit, Twitter, and Facebook all offer certain data through their APIs. To use an API, you make a request to a remote web server, and retrieve the data you need. But why use an API instead of a static dataset you can download? APIs are useful in the following cases: . | The data is changing quickly. An example of this is stock price data. It doesn’t really make sense to regenerate a dataset and download it every minute – this will take a lot of bandwidth, and be pretty slow. | You want a small piece of a much larger set of data. Reddit comments are one example. What if you want to just pull your own comments on Reddit? It doesn’t make much sense to download the entire Reddit database, then filter just your own comments. | There is repeated computation involved. Spotify has an API that can tell you the genre of a piece of music. You could theoretically create your own classifier, and use it to categorize music, but you’ll never have as much data as Spotify does. | . In cases like the ones above, an API is the right solution. In this blog post, we’ll be querying a simple API to retrieve data about the International Space Station (ISS). Using an API will save us time and effort over doing all the computation ourselves. ",
    "url": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#python-api-tutorial---an-introduction-to-using-apis",
    "relUrl": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#python-api-tutorial---an-introduction-to-using-apis"
  },"330": {
    "doc": "05 - Getting Data from Web APIs",
    "title": "API Requests",
    "content": "APIs are hosted on web servers. When you type www.google.com in your browser’s address bar, your computer is actually asking the www.google.com server for a webpage, which it then returns to your browser. APIs work much the same way, except instead of your web browser asking for a webpage, your program asks for data. This data is usually returned in JSON format (for more on this, checkout our tutorial on working with JSON data). In order to get the data, we make a request to a webserver. The server then replies with our data. In Python, we’ll use the requests library to do this. In this Python API tutorial we’ll be using Python 3.4 for all of our examples. ",
    "url": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#api-requests",
    "relUrl": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#api-requests"
  },"331": {
    "doc": "05 - Getting Data from Web APIs",
    "title": "Type of requests",
    "content": "There are many different types of requests. The most commonly used one, a GET request, is used to retrieve data. We can use a simple GET request to retrieve information from the OpenNotify API. OpenNotify has several API endpoints. An endpoint is a server route that is used to retrieve different data from the API. For example, the /comments endpoint on the Reddit API might retrieve information about comments, whereas the /users endpoint might retrieve data about users. To access them, you would add the endpoint to the base url of the API. The first endpoint we’ll look at on OpenNotify is the iss-now.json endpoint. This endpoint gets the current latitude and longitude of the International Space Station. As you can see, retrieving this data isn’t a great fit for a dataset, because it involves some calculation on the server, and changes quickly. You can see a listing of all the endpoints on OpenNotify here. The base url for the OpenNotify API is http://api.open-notify.org, so we’ll add this to the beginning of all of our endpoints. import requests # Make a get request to get the latest position of the international space station from the opennotify api. response = requests.get(\"http://api.open-notify.org/iss-now.json\") # Print the status code of the response. print(response.status_code) print(response.content) . 200 b'{\"iss_position\": {\"latitude\": \"32.4451\", \"longitude\": \"-148.5181\"}, \"timestamp\": 1634828637, \"message\": \"success\"}' . ",
    "url": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#type-of-requests",
    "relUrl": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#type-of-requests"
  },"332": {
    "doc": "05 - Getting Data from Web APIs",
    "title": "Status codes",
    "content": "The request we just made had a status code of 200. Status codes are returned with every request that is made to a web server. Status codes indicate information about what happened with a request. Here are some codes that are relevant to GET requests: . | 200 – everything went okay, and the result has been returned (if any) | 301 – the server is redirecting you to a different endpoint. This can happen when a company switches domain names, or an endpoint name is changed. | 401 – the server thinks you’re not authenticated. This happens when you don’t send the right credentials to access an API (we’ll talk about authentication in a later post). | 400 – the server thinks you made a bad request. This can happen when you don’t send along the right data, among other things. | 403 – the resource you’re trying to access is forbidden – you don’t have the right permissions to see it. | 404 – the resource you tried to access wasn’t found on the server. | . We’ll now make a GET request to http://api.open-notify.org/iss-pass, an endpoint that doesn’t exist, per the API documentation. response = requests.get(\"http://api.open-notify.org/iss-pass\") print(response.status_code) . 404 . ",
    "url": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#status-codes",
    "relUrl": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#status-codes"
  },"333": {
    "doc": "05 - Getting Data from Web APIs",
    "title": "Hitting the right endpoint",
    "content": "iss-pass wasn’t a valid endpoint, so we got a 404 status code in response. We forgot to add .json at the end, as the API documentation states. We’ll now make a GET request to http://api.open-notify.org/iss-pass.json. response = requests.get(\"http://api.open-notify.org/iss-pass.json\") print(response.status_code) . 400 . ",
    "url": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#hitting-the-right-endpoint",
    "relUrl": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#hitting-the-right-endpoint"
  },"334": {
    "doc": "05 - Getting Data from Web APIs",
    "title": "Query parameters",
    "content": "You’ll see that in the last example, we got a 400 status code, which indicates a bad request. If you look at the documentation for the OpenNotify API, we see that the ISS Pass endpoint requires two parameters. The ISS Pass endpoint returns when the ISS will next pass over a given location on earth. In order to compute this, we need to pass the coordinates of the location to the API. We do this by passing two parameters – latitude and longitude. We can do this by adding an optional keyword argument, params, to our request. In this case, there are two parameters we need to pass: . | lat – The latitude of the location we want. | lon – The longitude of the location we want. | . We can make a dictionary with these parameters, and then pass them into the requests.get function. We can also do the same thing directly by adding the query parameters to the url, like this: http://api.open-notify.org/iss-pass.json?lat=40.71&amp;lon=-74. It’s almost always preferable to setup the parameters as a dictionary, because requests takes care of some things that come up, like properly formatting the query parameters. We’ll make a request using the coordinates of New York City, and see what response we get. # Set up the parameters we want to pass to the API. # This is the latitude and longitude of New York City. parameters = {\"lat\": 40.71, \"lon\": -74} # Make a get request with the parameters. response = requests.get(\"http://api.open-notify.org/iss-pass.json\", params=parameters) # response = requests.get(\"http://api.open-notify.org/iss-pass.json?lat={0}&amp;lon={1}\".format(parameters['lat'], parameters['lon'])) # Print the content of the response (the data the server returned) print(response.json()) # This gets the same data as the command above # response = requests.get(\"http://api.open-notify.org/iss-pass.json?lat=40.71&amp;lon=-74\") # print(response.content) . {'message': 'success', 'request': {'altitude': 100, 'datetime': 1634828641, 'latitude': 40.71, 'longitude': -74.0, 'passes': 5}, 'response': [{'duration': 591, 'risetime': 1634829288}, {'duration': 652, 'risetime': 1634835096}, {'duration': 558, 'risetime': 1634840931}, {'duration': 614, 'risetime': 1634895303}, {'duration': 644, 'risetime': 1634901090}]} . ",
    "url": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#query-parameters",
    "relUrl": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#query-parameters"
  },"335": {
    "doc": "05 - Getting Data from Web APIs",
    "title": "Working with JSON data",
    "content": "You may have noticed that the content of the response earlier was a string. Although it was shown as a bytes object, we can easily convert the content to a string using response.content.decode(\"utf-8\"). Strings are the way that we pass information back and forth to APIs, but it’s hard to get the information we want out of them. How do we know how to decode the string that we get back and work with it in Python? How do we figure out the altitude of the ISS from the string response? . Luckily, there’s a format called JavaScript Object Notation (JSON). JSON is a way to encode data structures like lists and dictionaries to strings that ensures that they are easily readable by machines. JSON is the primary format in which data is passed back and forth to APIs, and most API servers will send their responses in JSON format. Python has great JSON support, with the json package. The json package is part of the standard library, so we don’t have to install anything to use it. We can both convert lists and dictionaries to JSON, and convert strings to lists and dictionaries. In the case of our ISS Pass data, it is a dictionary encoded to a string in JSON format. The json library has two main methods: . | dumps – Takes in a Python object, and converts it to a string. | loads – Takes a JSON string, and converts it to a Python object. | . # Make a list of fast food chains. best_food_chains = [\"Taco Bell\", \"Shake Shack\", \"Chipotle\"] # This is a list. print(type(best_food_chains)) . &lt;class 'list'&gt; . # Import the json library import json . # Use json.dumps to convert best_food_chains to a string. best_food_chains_string = json.dumps(best_food_chains) print('***', best_food_chains_string) . *** [\"Taco Bell\", \"Shake Shack\", \"Chipotle\"] . # We've successfully converted our list to a string. print(type(best_food_chains_string)) . &lt;class 'str'&gt; . # Convert best_food_chains_string back into a list print(type(json.loads(best_food_chains_string))) . &lt;class 'list'&gt; . # Make a dictionary fast_food_franchise = { \"Subway\": 24722, \"McDonalds\": 14098, \"Starbucks\": 10821, \"Pizza Hut\": 7600 } . # We can also dump a dictionary to a string and load it. fast_food_franchise_string = json.dumps(fast_food_franchise) print(type(fast_food_franchise_string)) . &lt;class 'str'&gt; . ",
    "url": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#working-with-json-data",
    "relUrl": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#working-with-json-data"
  },"336": {
    "doc": "05 - Getting Data from Web APIs",
    "title": "Getting JSON from an API request",
    "content": "You can get the content of a response as a python object by using the .json() method on the response. # Make the same request we did earlier, but with the coordinates of San Francisco instead. parameters = {\"lat\": 37.78, \"lon\": -122.41} response = requests.get(\"http://api.open-notify.org/iss-pass.json\", params=parameters) print(type(response.content)) . &lt;class 'bytes'&gt; . # Get the response data as a python object. Verify that it's a dictionary. data = response.json() print(type(data)) print(data) print('message is ', data['message']) print('altitude is ', data['request']['altitude']) . &lt;class 'dict'&gt; {'message': 'success', 'request': {'altitude': 100, 'datetime': 1634828645, 'latitude': 37.78, 'longitude': -122.41, 'passes': 5}, 'response': [{'duration': 603, 'risetime': 1634828665}, {'duration': 496, 'risetime': 1634834593}, {'duration': 531, 'risetime': 1634840465}, {'duration': 639, 'risetime': 1634846267}, {'duration': 600, 'risetime': 1634852086}]} message is success altitude is 100 . ",
    "url": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#getting-json-from-an-api-request",
    "relUrl": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#getting-json-from-an-api-request"
  },"337": {
    "doc": "05 - Getting Data from Web APIs",
    "title": "Content type",
    "content": "The server doesn’t just send a status code and the data when it generates a response. It also sends metadata containing information on how the data was generated and how to decode it. This is stored in the response headers. In Python, we can access this with the headers property of a response object. The headers will be shown as a dictionary. Within the headers, content-type is the most important key for now. It tells us the format of the response, and how to decode it. For the OpenNotify API, the format is JSON, which is why we could decode it with the json package earlier. # Headers is a dictionary print(response.headers) # Get the content-type from the dictionary. print(response.headers[\"content-type\"]) . {'Server': 'nginx/1.10.3', 'Date': 'Thu, 21 Oct 2021 15:04:05 GMT', 'Content-Type': 'application/json', 'Content-Length': '521', 'Connection': 'keep-alive', 'Via': '1.1 vegur'} application/json . ",
    "url": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#content-type",
    "relUrl": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#content-type"
  },"338": {
    "doc": "05 - Getting Data from Web APIs",
    "title": "Finding the number of people in space",
    "content": "OpenNotify has one more API endpoint, astros.json. It tells you how many people are currently in space. The format of the responses can be found here. # Get the response from the API endpoint. response = requests.get(\"http://api.open-notify.org/astros.json\") data = response.json() . # people are currently in space. print(data[\"number\"]) print(data) . 10 {'number': 10, 'people': [{'craft': 'ISS', 'name': 'Mark Vande Hei'}, {'craft': 'ISS', 'name': 'Pyotr Dubrov'}, {'craft': 'ISS', 'name': 'Thomas Pesquet'}, {'craft': 'ISS', 'name': 'Megan McArthur'}, {'craft': 'ISS', 'name': 'Shane Kimbrough'}, {'craft': 'ISS', 'name': 'Akihiko Hoshide'}, {'craft': 'ISS', 'name': 'Anton Shkaplerov'}, {'craft': 'Shenzhou 13', 'name': 'Zhai Zhigang'}, {'craft': 'Shenzhou 13', 'name': 'Wang Yaping'}, {'craft': 'Shenzhou 13', 'name': 'Ye Guangfu'}], 'message': 'success'} . ",
    "url": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#finding-the-number-of-people-in-space",
    "relUrl": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html#finding-the-number-of-people-in-space"
  },"339": {
    "doc": "05 - Getting Data from Web APIs",
    "title": "05 - Getting Data from Web APIs",
    "content": " ",
    "url": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html",
    "relUrl": "/lectures/Topic%2008%20-%20Making%20Web%20Requests/05%20-%20Getting%20Data%20from%20Web%20APIs.html"
  },"340": {
    "doc": "05 - Python Generators",
    "title": "Python Generators",
    "content": "Source . todo : add content . def infinite_sequence(): num = 0 while True: num += 1 return num . x = infinite_sequence() print(x) . --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_93378/1323376258.py in &lt;module&gt; ----&gt; 1 x = infinite_sequence() 2 print(x) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_93378/3775151451.py in infinite_sequence() 2 num = 0 3 while True: ----&gt; 4 num += 1 5 return num KeyboardInterrupt: . def infinite_sequence(): num = 0 while True: yield num num += 1 . seq = infinite_sequence() print(next(seq)) for i in range(0, 10): print(next(seq)) . for index, value in enumerate(seq): print(value) if index &gt; 10: break . print(next(seq)) . import random def my_sequence(): num = 0 while True: yield num num += random.randint(0, 10) if num &gt; 20: break . seq = my_sequence() print(next(seq)) . print(next(seq)) . print(next(seq)) . print(next(seq)) . 12 . print(next(seq)) . 14 . print(next(seq)) . 20 . print(next(seq)) . --------------------------------------------------------------------------- StopIteration Traceback (most recent call last) &lt;ipython-input-20-d9775cae3656&gt; in &lt;module&gt; ----&gt; 1 print(next(seq)) StopIteration: . seq2 = my_sequence() print(next(seq2)) . 0 . image_db = [ 1, 2, 5, 7, 10] meds_db = [ 3, 5, 7, 9 ] i = get_next_image_patient() m = get_next_med_patient() def get_next_patient() while no_match: if i == m: yield i elif i &gt; m: i = get_next_image_patient() elif m &lt; i: m = get_next_med_patient() for patient in get_next_patient(): # do something . ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/05%20-%20Python%20Generators.html#python-generators",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/05%20-%20Python%20Generators.html#python-generators"
  },"341": {
    "doc": "05 - Python Generators",
    "title": "05 - Python Generators",
    "content": " ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/05%20-%20Python%20Generators.html",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators/05%20-%20Python%20Generators.html"
  },"342": {
    "doc": "05 - Start a process in Python",
    "title": "Start a process in Python",
    "content": "Start a process in Python . Sometimes you might want to spawn a process from python and interact with the data. You might use a process to generate some files that your python application will read, or read the output of the process within your application. Doing so is relatively simple in python using the subprocess library,. ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/05%20-%20Start%20a%20process%20in%20Python.html#start-a-process-in-python",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/05%20-%20Start%20a%20process%20in%20Python.html#start-a-process-in-python"
  },"343": {
    "doc": "05 - Start a process in Python",
    "title": "subprocess.Popen",
    "content": "You can start a process in Python using the Popen function call. Open a pipe to or from command. The return value is an open file object connected to the pipe, which can be read or written depending on whether mode is ‘r’ (default) or ‘w’. from subprocess import Popen, PIPE process = Popen(['cat', 'scores.csv'], stdout=PIPE, stderr=PIPE) stdout, stderr = process.communicate() print(stdout.decode('utf-8')) . The process.communicate() call reads input and output from the process. stdout is the process output. stderr will be written only if an error occurs. If you want to wait for the program to finish you can call Popen.wait(). from subprocess import Popen, PIPE process = Popen(['cat', 'scores.csv'], stdout=PIPE, stderr=PIPE) stdout, stderr = process.communicate() status = process.wait() print(stdout.decode('utf-8')) . ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/05%20-%20Start%20a%20process%20in%20Python.html#subprocesspopen",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/05%20-%20Start%20a%20process%20in%20Python.html#subprocesspopen"
  },"344": {
    "doc": "05 - Start a process in Python",
    "title": "subprocess.call",
    "content": "Subprocess has a method call() which can be used to start a program. This is basically just like the Popen class and takes all of the same arguments, but it simply wait until the command completes and gives us the return code. import subprocess subprocess.call(['ls','-l']) . total 192 -rw-r--r-- 1 bk staff 7977 Oct 21 09:46 01 - Getting Started.ipynb -rw-r--r-- 1 bk staff 7400 Oct 17 20:44 02 - Terminal Application.ipynb -rw-r--r-- 1 bk staff 10581 Oct 17 20:44 03 - Argparse.ipynb -rw-r--r-- 1 bk staff 14711 Oct 17 20:44 04 - Start a process in Python.ipynb -rw-r--r-- 1 bk staff 149 Aug 26 15:35 argparse_1.py -rw-r--r-- 1 bk staff 202 Aug 26 15:35 argparse_2.py -rw-r--r-- 1 bk staff 299 Aug 26 15:35 argparse_3.py -rw-r--r-- 1 bk staff 309 Aug 26 15:35 argparse_4.py -rw-r--r-- 1 bk staff 219 Aug 26 15:35 argparse_5.py -rw-r--r-- 1 bk staff 580 Aug 26 15:35 argparse_6.py -rw-r--r-- 1 bk staff 372 Aug 26 15:35 terminal_1.py -rw-r--r-- 1 bk staff 370 Aug 26 15:35 terminal_2.py -rw-r--r-- 1 bk staff 198 Aug 26 15:35 test_class.py -rw-r--r-- 1 bk staff 100 Aug 26 15:35 test_sample.py -rw-r--r-- 1 bk staff 143 Aug 26 15:35 test_sysexit.py -rw-r--r-- 1 bk staff 90 Oct 17 20:44 test_tmpdir.p 0 y -rw-r--r-- 1 bk staff 294 Oct 17 20:44 test_tmppath.py . total 181 drwxr-xr-x 2 root root 4096 Mar 3 2012 bin drwxr-xr-x 4 root root 1024 Oct 26 2012 boot . The command line arguments are passed as a list of strings, which avoids the need for escaping quotes or other special characters that might be interpreted by the shell. ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/05%20-%20Start%20a%20process%20in%20Python.html#subprocesscall",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/05%20-%20Start%20a%20process%20in%20Python.html#subprocesscall"
  },"345": {
    "doc": "05 - Start a process in Python",
    "title": "subprocess.check_call()",
    "content": "The check_call() function works like call() except that the exit code is checked, and if it indicates an error happened then a CalledProcessError exception is raised. import subprocess subprocess.check_call(['false']) . --------------------------------------------------------------------------- CalledProcessError Traceback (most recent call last) /var/folders/jd/pq0swyt521jb2424d6fvth840000gn/T/ipykernel_91211/359612823.py in &lt;module&gt; 1 import subprocess ----&gt; 2 subprocess.check_call(['false']) ~/opt/miniconda3/envs/cmu39/lib/python3.9/subprocess.py in check_call(*popenargs, **kwargs) 371 if cmd is None: 372 cmd = popenargs[0] --&gt; 373 raise CalledProcessError(retcode, cmd) 374 return 0 375 CalledProcessError: Command '['false']' returned non-zero exit status 1. ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/05%20-%20Start%20a%20process%20in%20Python.html#subprocesscheck_call",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/05%20-%20Start%20a%20process%20in%20Python.html#subprocesscheck_call"
  },"346": {
    "doc": "05 - Start a process in Python",
    "title": "subprocess.check_output()",
    "content": "The standard input and output channels for the process started by call() are bound to the parent’s input and output. That means the calling program cannot capture the output of the command. To capture the output, we can use check_output() for later processing. import subprocess output = subprocess.check_output(['ls','-l']) print(output.decode('utf-8')) . total 192 -rw-r--r-- 1 bk staff 7977 Oct 21 09:46 01 - Getting Started.ipynb -rw-r--r-- 1 bk staff 7400 Oct 17 20:44 02 - Terminal Application.ipynb -rw-r--r-- 1 bk staff 10581 Oct 17 20:44 03 - Argparse.ipynb -rw-r--r-- 1 bk staff 14711 Oct 17 20:44 04 - Start a process in Python.ipynb -rw-r--r-- 1 bk staff 149 Aug 26 15:35 argparse_1.py -rw-r--r-- 1 bk staff 202 Aug 26 15:35 argparse_2.py -rw-r--r-- 1 bk staff 299 Aug 26 15:35 argparse_3.py -rw-r--r-- 1 bk staff 309 Aug 26 15:35 argparse_4.py -rw-r--r-- 1 bk staff 219 Aug 26 15:35 argparse_5.py -rw-r--r-- 1 bk staff 580 Aug 26 15:35 argparse_6.py -rw-r--r-- 1 bk staff 372 Aug 26 15:35 terminal_1.py -rw-r--r-- 1 bk staff 370 Aug 26 15:35 terminal_2.py -rw-r--r-- 1 bk staff 198 Aug 26 15:35 test_class.py -rw-r--r-- 1 bk staff 100 Aug 26 15:35 test_sample.py -rw-r--r-- 1 bk staff 143 Aug 26 15:35 test_sysexit.py -rw-r--r-- 1 bk staff 90 Oct 17 20:44 test_tmpdir.py -rw-r--r-- 1 bk staff 294 Oct 17 20:44 test_tmppath.py . ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/05%20-%20Start%20a%20process%20in%20Python.html#subprocesscheck_output",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/05%20-%20Start%20a%20process%20in%20Python.html#subprocesscheck_output"
  },"347": {
    "doc": "05 - Start a process in Python",
    "title": "05 - Start a process in Python",
    "content": " ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/05%20-%20Start%20a%20process%20in%20Python.html",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/05%20-%20Start%20a%20process%20in%20Python.html"
  },"348": {
    "doc": "06.a - Writing Terminal Application",
    "title": "Basic Terminal Apps",
    "content": "Basic Terminal Apps . # Show a simple message print('hello class') . hello class . ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/06.a%20-%20Writing%20Terminal%20Application.html#basic-terminal-apps",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/06.a%20-%20Writing%20Terminal%20Application.html#basic-terminal-apps"
  },"349": {
    "doc": "06.a - Writing Terminal Application",
    "title": "Print a title bar",
    "content": "import os # Display a title bar. print(\"**********************************************\") print(\"*** Hello to everyone in class today! ***\") print(\"**********************************************\") # Display a bunch of output, representing a long-running program. for week in range(1, 8): print(f\"We will learn interesting things in Week {week:02}\") print(f\"We will learn interesting things in Week {week:02}\") . ********************************************** *** Hello to everyone in class today! *** ********************************************** We will learn interesting things in Week 01 We will learn interesting things in Week 01 We will learn interesting things in Week 02 We will learn interesting things in Week 02 We will learn interesting things in Week 03 We will learn interesting things in Week 03 We will learn interesting things in Week 04 We will learn interesting things in Week 04 We will learn interesting things in Week 05 We will learn interesting things in Week 05 We will learn interesting things in Week 06 We will learn interesting things in Week 06 We will learn interesting things in Week 07 We will learn interesting things in Week 07 . ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/06.a%20-%20Writing%20Terminal%20Application.html#print-a-title-bar",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/06.a%20-%20Writing%20Terminal%20Application.html#print-a-title-bar"
  },"350": {
    "doc": "06.a - Writing Terminal Application",
    "title": "The sleep function",
    "content": "import os from time import sleep # Display a title bar. print(\"**********************************************\") print(\"*** Hello to everyone in class today! ***\") print(\"**********************************************\") # Display a bunch of output, representing a long-running program. for week in range(1,8): sleep(1) print(f\"We will learn interesting things in Week {week:02}\") . ********************************************** *** Hello to everyone in class today! *** ********************************************** We will learn interesting things in Week 01 We will learn interesting things in Week 02 We will learn interesting things in Week 03 We will learn interesting things in Week 04 We will learn interesting things in Week 05 We will learn interesting things in Week 06 We will learn interesting things in Week 07 . ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/06.a%20-%20Writing%20Terminal%20Application.html#the-sleep-function",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/06.a%20-%20Writing%20Terminal%20Application.html#the-sleep-function"
  },"351": {
    "doc": "06.a - Writing Terminal Application",
    "title": "Accepting arguments",
    "content": "You can pass arguments into your application to dynamically change the behavior at run time. See terminal_1.py . import sys from time import sleep adjective = sys.argv[1] num_weeks = int(sys.argv[2]) print(\"**********************************************\") print(\"*** Hello to everyone in class today! ***\") print(\"**********************************************\") for week in range(1, num_weeks + 1): sleep(1) print(f\"We will learn {adjective} things in Week {week:02}\") . Note: . | sys.argv[1] is the 1st parameter passed in. | sys.argv[0] represents the path of the file being run | all arguments are strings by default, you can cast these to other data types | . !python terminal_1.py fun 3 . ********************************************** *** Hello to everyone in class today! *** ********************************************** We will learn fun things in Week 01 We will learn fun things in Week 02 We will learn fun things in Week 03 . Here we slightly modify the code to process a list of arguments. Since all arguments are strings we’ll pass in a comma separated list of values. See terminal_2.py . import sys from time import sleep adjectives = sys.argv[1] print(\"**********************************************\") print(\"*** Hello to everyone in class today! ***\") print(\"**********************************************\") for week, adjective in enumerate(adjectives.split(',')): sleep(1) print(f\"We will learn {adjective} things in Week {(week + 1):02}\") . Note: . | adjectives is a string which we’ll split on commas to create a list | we use enumerate to keep track of the index we’re currently processing | since index is zero based, we’ll add 1 in the print statement to associate the index with the correct week | . !python terminal_2.py \"fun,interesting,exciting\" . ********************************************** *** Hello to everyone in class today! *** ********************************************** We will learn fun things in Week 01 We will learn interesting things in Week 02 We will learn exciting things in Week 03 . ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/06.a%20-%20Writing%20Terminal%20Application.html#accepting-arguments",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/06.a%20-%20Writing%20Terminal%20Application.html#accepting-arguments"
  },"352": {
    "doc": "06.a - Writing Terminal Application",
    "title": "06.a - Writing Terminal Application",
    "content": " ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/06.a%20-%20Writing%20Terminal%20Application.html",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/06.a%20-%20Writing%20Terminal%20Application.html"
  },"353": {
    "doc": "06.b - Argparse",
    "title": "Argparse",
    "content": " ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/06.b%20-%20Argparse.html#argparse",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/06.b%20-%20Argparse.html#argparse"
  },"354": {
    "doc": "06.b - Argparse",
    "title": "What is Argparse?",
    "content": "Argparse Tutorial . Argparse is a parser for command-line options, arguments and subcommands. This library makes it easy to write user-friendly command-line interfaces by: . | defines what arguments it requires | and figuring out how to parse those out of sys.argv. | . The argparse module automatically generates help and usage messages and issues errors when users give the program invalid arguments. ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/06.b%20-%20Argparse.html#what-is-argparse",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/06.b%20-%20Argparse.html#what-is-argparse"
  },"355": {
    "doc": "06.b - Argparse",
    "title": "Getting Started",
    "content": "See agrparse_1.py . # import the library import argparse # initialize the parser parser = argparse.ArgumentParser() # parse arguments from sys.argv parser.parse_args() . # run the code and ask for help !python argparse_1.py --help . usage: argparse_1.py [-h] optional arguments: -h, --help show this help message and exit . Positional Arguments . See argparse_2.py . import argparse parser = argparse.ArgumentParser() parser.add_argument(\"name\") parser.add_argument(\"age\") parser.add_argument(\"city\") args = parser.parse_args() print(args.name, args.age, args.city) . !python argparse_2.py --help . usage: argparse_2.py [-h] name age city positional arguments: name age city optional arguments: -h, --help show this help message and exit . !python argparse_2.py Ben 37 Pittsburgh . Ben 37 Pittsburgh . !python argparse_2.py Brian \"??\" Pittsburgh . Brian ?? Pittsburgh . Extending the help text . See argparse_3.py . import argparse parser = argparse.ArgumentParser() parser.add_argument(\"name\", help=\"the name of the person you want to find\") parser.add_argument(\"age\", help=\"the age of the person you'd like to find\") parser.add_argument(\"city\", help=\"the city you'd like to search\") args = parser.parse_args() . !python argparse_3.py --help . usage: argparse_3.py [-h] name age city positional arguments: name the name of the person you want to find age the age of the person you'd like to find city the city you'd like to search optional arguments: -h, --help show this help message and exit . Changing the default argument type . Argparse treats all arguments as strings by default. You can change the expected data type when you add each argument. See argparse_4.py . import argparse parser = argparse.ArgumentParser() parser.add_argument(\"name\", help=\"the name of the person you want to find\") parser.add_argument(\"age\", help=\"the age of the person you'd like to find\", type=int) parser.add_argument(\"city\", help=\"the city you'd like to search\") args = parser.parse_args() . !python argparse_4.py Ben abc Pittsburgh . usage: argparse_4.py [-h] name age city argparse_4.py: error: argument age: invalid int value: 'abc' . !python argparse_4.py Ben 37 Pittsburgh . Optional arguments . See argparse_5.py . import argparse parser = argparse.ArgumentParser() parser.add_argument(\"--verbose\", help=\"increase output verbosity\", action=\"store_true\") args = parser.parse_args() if args.verbose: print(\"verbosity turned on\") . An optional argument (or option) is (by default) given None as a value when its not being used. | Using the –verbosity option, only two values are actually useful, True or False. | The keyword “action” is being given the value “store_true” which means that if the option is specifed, then assign the value “True” to args.verbose | Not specifying the option implies False. | . !python argparse_5.py --help . usage: argparse_5.py [-h] [--verbose] optional arguments: -h, --help show this help message and exit --verbose increase output verbosity . !python argparse_5.py --verbose . verbosity turned on . !python argparse_5.py . Short options . See argparse_6.py . import argparse parser = argparse.ArgumentParser() parser.add_argument(\"-n\", \"--name\", help=\"the name of the person you want to find\") parser.add_argument(\"-a\", \"--age\", help=\"the age of the person you'd like to find\", type=int) parser.add_argument(\"-c\", \"--city\", help=\"the city you'd like to search\") parser.add_argument(\"-v\", \"--verbose\", help=\"increase output verbosity\", action=\"store_true\") args = parser.parse_args() if args.verbose: print(f\"Searching for {args.name} {args.age} years of age in or around {args.city}\") else: print(f\"Searching for {args.name}\") . !python argparse_6.py --help . usage: argparse_6.py [-h] [-n NAME] [-a AGE] [-c CITY] [-v] optional arguments: -h, --help show this help message and exit -n NAME, --name NAME the name of the person you want to find -a AGE, --age AGE the age of the person you'd like to find -c CITY, --city CITY the city you'd like to search -v, --verbose increase output verbosity . !python argparse_6.py -n Ben -a 37 -c Pittsburgh . Searching for Ben . !python argparse_6.py --name Ben --age 37 --city Pittsburgh . Searching for Ben . !python argparse_6.py --name Ben --age 37 --city Pittsburgh --verbose . Searching for Ben 37 years of age in or around Pittsburgh . ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/06.b%20-%20Argparse.html#getting-started",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/06.b%20-%20Argparse.html#getting-started"
  },"356": {
    "doc": "06.b - Argparse",
    "title": "06.b - Argparse",
    "content": " ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started/06.b%20-%20Argparse.html",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started/06.b%20-%20Argparse.html"
  },"357": {
    "doc": "Data Focused Python",
    "title": "Description",
    "content": "Course Site . This course focuses on the fundamentals of computer programming using the Python interpreted programming language. Students will develop his or her problem solving skills using the divide and-conquer and top-down approaches to build real-world based software applications. Students will also learn the basics of the software development life-cycle: planning, development, testing, implementation and maintenance. Assignments will include individual coding assignments and weekly fundamental checkpoint quizzes culminating with an individual final project to test essential programming and problem solving skills. Note: The course schedule and assignments are subject to change. Please see your Enterprise Learning Management System (e.g. Canvas, Blackboard, Desire2Learn) for the official schedule. ",
    "url": "/#description",
    "relUrl": "/#description"
  },"358": {
    "doc": "Data Focused Python",
    "title": "Lectures",
    "content": "Lectures will contain a mixture of content form this site and others. ",
    "url": "/#lectures",
    "relUrl": "/#lectures"
  },"359": {
    "doc": "Data Focused Python",
    "title": "Quizzes",
    "content": "Quizzes will be administered approximately every week on material from prior weeks. All quizzes are closed book and no materials may be used. | Week 2 - Quiz 1 | Week 3 - Quiz 2 | Week 4 - Quiz 3 | Week 5 - Quiz 4 | Week 6 - Quiz 5 | . ",
    "url": "/#quizzes",
    "relUrl": "/#quizzes"
  },"360": {
    "doc": "Data Focused Python",
    "title": "Assignments",
    "content": "The project is a collection of individual programming assignments building towards delivery of a final product. | Week 2 - Assignment 1 | Week 3 - Assignment 2 | Week 4 - Assignment 3 | Week 6 - Assignment 4 | . Grading . All assignments will be evaluated by 2 criteria: . | Correctness: The program produces the expected output or data. | Proficiency: The program demonstrates a high degree of competence or skill. | . ",
    "url": "/#assignments",
    "relUrl": "/#assignments"
  },"361": {
    "doc": "Data Focused Python",
    "title": "Data Focused Python",
    "content": " ",
    "url": "/",
    "relUrl": "/"
  },"362": {
    "doc": "Topic 01 - Getting Started",
    "title": "Topic 01 - Getting Started",
    "content": " ",
    "url": "/lectures/Topic%2001%20-%20Getting%20Started.html",
    "relUrl": "/lectures/Topic%2001%20-%20Getting%20Started.html"
  },"363": {
    "doc": "Topic 02 - Python basics",
    "title": "Topic 02 - Python basics",
    "content": " ",
    "url": "/lectures/Topic%2002%20-%20Python%20basics.html",
    "relUrl": "/lectures/Topic%2002%20-%20Python%20basics.html"
  },"364": {
    "doc": "Topic 03 - Generating Data",
    "title": "Topic 03 - Generating Data",
    "content": " ",
    "url": "/lectures/Topic%2003%20-%20Generating%20Data.html",
    "relUrl": "/lectures/Topic%2003%20-%20Generating%20Data.html"
  },"365": {
    "doc": "Topic 04 - Writing Testable Code",
    "title": "Topic 04 - Writing Testable Code",
    "content": " ",
    "url": "/lectures/Topic%2004%20-%20Writing%20Testable%20Code.html",
    "relUrl": "/lectures/Topic%2004%20-%20Writing%20Testable%20Code.html"
  },"366": {
    "doc": "Topic 05 - Processing files",
    "title": "Topic 05 - Processing files",
    "content": " ",
    "url": "/lectures/Topic%2005%20-%20Processing%20files.html",
    "relUrl": "/lectures/Topic%2005%20-%20Processing%20files.html"
  },"367": {
    "doc": "Topic 06 - Comprehensions and Generators",
    "title": "Topic 06 - Comprehensions and Generators",
    "content": " ",
    "url": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators.html",
    "relUrl": "/lectures/Topic%2006%20-%20Comprehensions%20and%20Generators.html"
  },"368": {
    "doc": "Topic 07 - Classes",
    "title": "Topic 07 - Classes",
    "content": " ",
    "url": "/lectures/Topic%2007%20-%20Classes.html",
    "relUrl": "/lectures/Topic%2007%20-%20Classes.html"
  },"369": {
    "doc": "Topic 08 - Making Web Requests",
    "title": "Topic 08 - Making Web Requests",
    "content": " ",
    "url": "/lectures/Topic%2008%20-%20Making%20Web%20Requests.html",
    "relUrl": "/lectures/Topic%2008%20-%20Making%20Web%20Requests.html"
  },"370": {
    "doc": "Topic 09 - Interfacing with Databases",
    "title": "Topic 09 - Interfacing with Databases",
    "content": " ",
    "url": "/lectures/Topic%2009%20-%20Interfacing%20with%20Databases.html",
    "relUrl": "/lectures/Topic%2009%20-%20Interfacing%20with%20Databases.html"
  },"371": {
    "doc": "Topic 10 - Data Processing and Visualization Part 1",
    "title": "Topic 10 - Data Processing and Visualization Part 1",
    "content": " ",
    "url": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201.html",
    "relUrl": "/lectures/Topic%2010%20-%20Data%20Processing%20and%20Visualization%20Part%201.html"
  },"372": {
    "doc": "Topic 11 - Data Processing and Visualization Part 2",
    "title": "Topic 11 - Data Processing and Visualization Part 2",
    "content": " ",
    "url": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202.html",
    "relUrl": "/lectures/Topic%2011%20-%20Data%20Processing%20and%20Visualization%20Part%202.html"
  },"373": {
    "doc": "Topic 12 - Web Scraping",
    "title": "Topic 12 - Web Scraping",
    "content": " ",
    "url": "/lectures/Topic%2012%20-%20Web%20Scraping.html",
    "relUrl": "/lectures/Topic%2012%20-%20Web%20Scraping.html"
  },"374": {
    "doc": "Lectures",
    "title": "Topic 01 - Getting Started",
    "content": ". | 01 - Getting Started (ipynb) | 02 - Running Python (ipynb) | 03 - Your First Python Program (ipynb) | 04 - Python Modules And Packages (ipynb) | 05 - Start A Process In Python (ipynb) | 06.A - Writing Terminal Application (ipynb) | 06.B - Argparse (ipynb) | . ",
    "url": "/lectures#topic-01---getting-started",
    "relUrl": "/lectures#topic-01---getting-started"
  },"375": {
    "doc": "Lectures",
    "title": "Topic 02 - Python basics",
    "content": ". | 01 - Decision Structures And Boolean Logic (ipynb) | 02 - Python Tuples (ipynb) | 03 - Loops (ipynb) | 04 - Python Functions (ipynb) | . ",
    "url": "/lectures#topic-02---python-basics",
    "relUrl": "/lectures#topic-02---python-basics"
  },"376": {
    "doc": "Lectures",
    "title": "Topic 03 - Generating Data",
    "content": ". | 01 - Generating Fake Data (ipynb) | 02 - Using Faker (ipynb) | 03 - Generating Synthetic Healthcare Data (ipynb) | 04 - Using Synthea (ipynb) | 05 - Generating Other Synthetic Data (ipynb) | . ",
    "url": "/lectures#topic-03---generating-data",
    "relUrl": "/lectures#topic-03---generating-data"
  },"377": {
    "doc": "Lectures",
    "title": "Topic 04 - Writing Testable Code",
    "content": ". | 01 - Python Testing (ipynb) | 02 - Getting Started With Testing In Python (ipynb) | 03- Choosing A Test Runner (ipynb) | 04 - Writing Your First Test (ipynb) | 05 - Writing Integration Tests (ipynb) | 06 - Testing In Multiple Environments (ipynb) | 07 - Introducing Linters Into Your Application (ipynb) | 08 - Testing For Performance Degradation Between Changes (ipynb) | 09 - Testing For Security Flaws In Your Application (ipynb) | . ",
    "url": "/lectures#topic-04---writing-testable-code",
    "relUrl": "/lectures#topic-04---writing-testable-code"
  },"378": {
    "doc": "Lectures",
    "title": "Topic 05 - Processing files",
    "content": ". | 01.A - File Processing (ipynb) | 01.B - Csv Files (ipynb) | 01.C - Json Files (ipynb) | 01.D - Xml Files (ipynb) | 03 - Using Fhirclient To Parse Healthcare Data (ipynb) | 04 - Reading Synthea Data (ipynb) | . ",
    "url": "/lectures#topic-05---processing-files",
    "relUrl": "/lectures#topic-05---processing-files"
  },"379": {
    "doc": "Lectures",
    "title": "Topic 06 - Comprehensions and Generators",
    "content": ". | 02 - Lists And List Comprehensions (ipynb) | 03 - Sets And Set Comprehensions (ipynb) | 04.A - Dictionaries (ipynb) | 04.B - Dictionary Comprehensions (ipynb) | 05 - Python Generators (ipynb) | . ",
    "url": "/lectures#topic-06---comprehensions-and-generators",
    "relUrl": "/lectures#topic-06---comprehensions-and-generators"
  },"380": {
    "doc": "Lectures",
    "title": "Topic 07 - Classes",
    "content": ". | 01 - Python Classes (ipynb) | . ",
    "url": "/lectures#topic-07---classes",
    "relUrl": "/lectures#topic-07---classes"
  },"381": {
    "doc": "Lectures",
    "title": "Topic 08 - Making Web Requests",
    "content": ". | 03 - Using Requests To Fetch Healthcare Data (ipynb) | 05 - Getting Data From Web Apis (ipynb) | . ",
    "url": "/lectures#topic-08---making-web-requests",
    "relUrl": "/lectures#topic-08---making-web-requests"
  },"382": {
    "doc": "Lectures",
    "title": "Topic 09 - Interfacing with Databases",
    "content": " ",
    "url": "/lectures#topic-09---interfacing-with-databases",
    "relUrl": "/lectures#topic-09---interfacing-with-databases"
  },"383": {
    "doc": "Lectures",
    "title": "Topic 10 - Data Processing and Visualization Part 1",
    "content": ". | 01.A - Cleaning And Transforming Data (ipynb) | 01.B - Grouping Data (ipynb) | 01.C - Descriptive Statistics (ipynb) | 02 - Basic Medical Data Visualization (ipynb) | 03.A - Numpy Introduction (ipynb) | 03.B - Numpy Data Analysis (ipynb) | . ",
    "url": "/lectures#topic-10---data-processing-and-visualization-part-1",
    "relUrl": "/lectures#topic-10---data-processing-and-visualization-part-1"
  },"384": {
    "doc": "Lectures",
    "title": "Topic 11 - Data Processing and Visualization Part 2",
    "content": ". | 00 - Additional Links (ipynb) | 01 - Pandas Introduction (ipynb) | 02.A - Pandas Data Analysis Part 1 (ipynb) | 02.B - Pandas Data Analysis Part 2 (ipynb) | 03 - Matplotlib Tutorial Python Plotting (ipynb) | 04 - The Ultimate Python Seaborn Tutorial (ipynb) | . ",
    "url": "/lectures#topic-11---data-processing-and-visualization-part-2",
    "relUrl": "/lectures#topic-11---data-processing-and-visualization-part-2"
  },"385": {
    "doc": "Lectures",
    "title": "Topic 12 - Web Scraping",
    "content": ". | 01. Introduction To Web Scraping (ipynb) | 02. Css Selectors (ipynb) | 03.A Web Scraping With Beautiful Soup (ipynb) | 03.B Web Scraping Using Selenium (ipynb) | 03.C Web Scraping With Scrapy (ipynb) | 04.A - More Web Scraping With Beautifulsoup (ipynb) | 04.B - Even More Web Scraping With Beautifulsoup (ipynb) | . ",
    "url": "/lectures#topic-12---web-scraping",
    "relUrl": "/lectures#topic-12---web-scraping"
  },"386": {
    "doc": "Lectures",
    "title": "Lectures",
    "content": " ",
    "url": "/lectures",
    "relUrl": "/lectures"
  },"387": {
    "doc": "Resources",
    "title": "Useful Books",
    "content": ". | Python for Data Analysis Note: Available at the CMU Library as an eBook | Automate the Boring Stuff | . ",
    "url": "/resources#useful-books",
    "relUrl": "/resources#useful-books"
  },"388": {
    "doc": "Resources",
    "title": "Synthetic Data Generation",
    "content": ". | Synthetic patient generation | Mimic Critical Care Database | Synthetic data generation (roll your own) | . ",
    "url": "/resources#synthetic-data-generation",
    "relUrl": "/resources#synthetic-data-generation"
  },"389": {
    "doc": "Resources",
    "title": "Useful Links",
    "content": ". | 24 Ultimate Data Science Projects To Boost Your Knowledge and Skills | . ",
    "url": "/resources#useful-links",
    "relUrl": "/resources#useful-links"
  },"390": {
    "doc": "Resources",
    "title": "Resources",
    "content": "This page contains a collection of useful resources related to the course. ",
    "url": "/resources",
    "relUrl": "/resources"
  }
}
