{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Using Selenium\n",
    "\n",
    "[Source](https://towardsdatascience.com/web-scraping-using-selenium-python-8a60f4cf40ab)\n",
    "\n",
    "## What is web scraping?\n",
    "\n",
    "Web scraping is a technique for extracting information from the internet automatically using a software that simulates human web surfing.\n",
    "\n",
    "## How is web-scraping useful?\n",
    "\n",
    "Web scraping helps us extract large volumes of data about customers, products, people, stock markets, etc. It is usually difficult to get this kind of information on a large scale using traditional data collection methods. We can utilize the data collected from a website such as e-commerce portal, social media channels to understand customer behaviors and sentiments, buying patterns, and brand attribute associations which are critical insights for any business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Since we have defined our purpose of scraping, let us delve into the nitty-gritty of how to actually do all the fun stuff! Before that below are some of the housekeeping instructions regarding installations of packages.\n",
    "\n",
    "* Python version: We will be using Python 3.0\n",
    "* Selenium package: You can install selenium package using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Chrome driver: Please install the latest version of chromedriver from [here](https://chromedriver.storage.googleapis.com/index.html?path=2.42/).\n",
    "\n",
    "Please note you need [Google Chrome](https://support.google.com/chrome/answer/95346?co=GENIE.Platform%3DDesktop&hl=en)installed on your machines to work through this illustration.\n",
    "\n",
    "The first and foremost thing while scraping a website is to understand the structure of the website. We will be scraping [Edmunds.com](https://www.edmunds.com/), a car forum. This website aids people in their car buying decisions. People can post their reviews about different cars in the discussion forums (very similar to how one posts reviews on Amazon). We will be scraping the discussion about [entry level luxury car brands](https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p702).\n",
    "\n",
    "We will scrape ~5000 comments from different users across multiple pages. We will scrape user id, date of comment and comments and export it into a csv file for any further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let’s begin\n",
    "\n",
    "We will first import important packages in our Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now create a new instance of google chrome. This will help our program open an url in google chrome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os import path\n",
    "# home = path.expanduser(\"~\")\n",
    "# driverPath = path.join(home, 'Source/chromedriver')\n",
    "# driver = webdriver.Chrome(driverPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - \n",
      "\n",
      "[WDM] - \n",
      "\n",
      "\n",
      "\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 96.0.4664\n",
      "[WDM] - Current google-chrome version is 96.0.4664\n",
      "Current google-chrome version is 96.0.4664\n",
      "[WDM] - Get LATEST chromedriver version for 96.0.4664 google-chrome\n",
      "[WDM] - Get LATEST chromedriver version for 96.0.4664 google-chrome\n",
      "Get LATEST chromedriver version for 96.0.4664 google-chrome\n",
      "[WDM] - Driver [/Users/bk/.wdm/drivers/chromedriver/mac64/96.0.4664.45/chromedriver] found in cache\n",
      "[WDM] - Driver [/Users/bk/.wdm/drivers/chromedriver/mac64/96.0.4664.45/chromedriver] found in cache\n",
      "Driver [/Users/bk/.wdm/drivers/chromedriver/mac64/96.0.4664.45/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "# from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "# driver = webdriver.Edge(EdgeChromiumDriverManager().install())\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now access google chrome and open our [website](https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p702). By the way, chrome knows that you are accessing it through an automated software!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Woha! We just opened an url from python notebook.**\n",
    "\n",
    "So, how does our web page look like?\n",
    "\n",
    "We will inspect 3 items (user id, date and comment) on our web page and understand how we can extract them.\n",
    "\n",
    "1. User id: Inspecting the userid, we can see the highlighted text represents the XML code for user id.\n",
    "\n",
    "![img1](https://miro.medium.com/max/3840/1*JZ9A8VrVdr7GXllaxCfq-w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XML path (XPath)for the userid is shown below. There is an interesting thing to note here that the XML path contains a comment id, which uniquely denotes each comment on the website. This will be very helpful as we try to recursively scrape multiple comments .\n",
    "\n",
    "```\n",
    "//*[@id=”Comment_5561090\"]/div/div[2]/div[1]/span[1]/a[2]\n",
    "```\n",
    "\n",
    "If we see the XPath in the picture, we will observe that it contains the user id ‘dino001’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we extract the values inside a XPath?\n",
    "\n",
    "Selenium has a function called “find_elements_by_xpath”. We will pass our XPath into this function and get a selenium element. Once we have the element, we can extract the text inside our XPath using the ‘text’ function. In our case the text is basically the user id (‘merc1’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"de4b05220f981f7f4d63c45c478541ca\", element=\"3a2d2809-d889-4a6a-824a-944d7f5d94e4\")>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userid_element = driver.find_elements(By.XPATH, '//*[@id=\"Comment_1726631\"]/div/div[2]/div[1]/span[1]/a[2]')\n",
    "userid_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'merc1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userid = userid_element[0].text\n",
    "userid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Comment Date: Similar to the user id, we will now inspect the date when the comment was posted.\n",
    "\n",
    "![img2](https://miro.medium.com/max/3840/1*GxVX1WtmgjKhuxliygMH8Q.png)\n",
    "\n",
    "Let’s also see the XPath for the comment date. Again note the unique comment id in the XPath.\n",
    "\n",
    "```\n",
    "//*[@id=\"Comment_5561090\"]/div/div[2]/div[2]/span[1]/a/time\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, how do we extract date from the above XPath?\n",
    "\n",
    "We will again use the function “find_elements_by_xpath” to get the selenium element. Now, if we carefully observe the highlighted text in the picture, we will see that the date is stored inside the ‘title’ attribute. We can access the values inside attributes using the function ‘get_attribute’. We will pass the tag name in this function to get the value inside the same.\n",
    "\n",
    "```\n",
    "user_date = driver.find_elements_by_xpath('//*[@id=\"Comment_5561090\"]/div/div[2]/div[2]/span[1]/a/time')[0]\n",
    "date = user_date.get_attribute('title')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Comments: Lastly, let’s explore how to extract the comments of each user.\n",
    "\n",
    "![img3](https://miro.medium.com/max/3840/1*MmLjPWxX6Pn9SV0ia1NajQ.png)\n",
    "\n",
    "Below is the XPath for the user comment.\n",
    "\n",
    "```\n",
    "//*[@id=\"Comment_5561090\"]/div/div[3]/div/div[1]\n",
    "```\n",
    "\n",
    "Once again, we have the comment id in our XPath. Similar to the userid we will extract the comment from the above XPath\n",
    "\n",
    "```python\n",
    "user_message = driver.find_elements_by_xpath('//*[@id=\"Comment_5561090\"]/div/div[3]/div/div[1]')[0]\n",
    "comment = user_message.text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We just learnt how to scrape different elements from a web page. Now how to recursively extract these items for 5000 users?\n",
    "\n",
    "As discussed above, we will use the comment ids, which are unique for a comment to extract different users data. If we see the XPath for the entire comment block, we will see that it has a comment id associated with it.\n",
    "\n",
    "```\n",
    "//*[@id=\"Comment_5561090\"]\n",
    "```\n",
    "\n",
    "![img4](https://miro.medium.com/max/3840/1*npG4HgblaPtFO7Q3qK69rA.png)\n",
    "\n",
    "The following code snippet will help us extract all the comment ids on a particular web page. We will again use the function ‘find_elements_by_xpath’ on the above XPath and extract the ids from the ‘id’ attribute.\n",
    "\n",
    "This code gives us a list of all the comment ids from a particular web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Comment_1738047', 'Comment_1738152', 'Comment_1738262', 'Comment_1738371', 'Comment_1738478', 'Comment_1738589', 'Comment_1738699', 'Comment_1738809', 'Comment_1738919', 'Comment_1739025', 'Comment_1739133', 'Comment_1739239', 'Comment_1739348', 'Comment_1739457', 'Comment_1739566', 'Comment_1739669', 'Comment_1739779', 'Comment_1739888', 'Comment_1739998', 'Comment_1740218', 'Comment_1740328', 'Comment_1740438', 'Comment_1740548', 'Comment_1740653', 'Comment_1740760', 'Comment_1740869', 'Comment_1740981', 'Comment_1741091', 'Comment_1741198', 'Comment_1741299', 'Comment_1741407', 'Comment_1741512', 'Comment_1741613', 'Comment_1741723', 'Comment_1734276', 'Comment_1741943', 'Comment_1742054', 'Comment_1742164', 'Comment_1742272', 'Comment_1742383', 'Comment_1742484', 'Comment_1742554', 'Comment_1742664', 'Comment_1742884', 'Comment_1742966', 'Comment_1726633', 'Comment_1726738', 'Comment_1726849', 'Comment_1726960', 'Comment_1727067']\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "ids = driver.find_elements(By.XPATH, \"//*[contains(@id,'Comment_')]\")\n",
    "comment_ids = []\n",
    "for i in ids:\n",
    "    comment_ids.append(i.get_attribute('id'))\n",
    "print(comment_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to bring all this together?\n",
    "\n",
    "Now we will bring all the things we have seen so far into one big code, which will recursively help us extract 5000 comments. We can extract user ids, date and comments for each user on a particular web page by looping through all the comment ids we found in the previous code.\n",
    "\n",
    "Below is the code snippet to extract all comments from a particular web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "comments = pd.DataFrame(columns = ['Date','user_id','comments']) \n",
    "comment_ids = []\n",
    "\n",
    "for page in range(1, 2, 1):\n",
    "    driver.get(f'https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p{page}')\n",
    "    ids = driver.find_elements(By.XPATH, \"//*[contains(@id,'Comment_')]\")\n",
    "    for i in ids:\n",
    "        comment_ids.append(i.get_attribute('id'))\n",
    "    \n",
    "    for x in comment_ids:\n",
    "        #Extract dates from for each user on a page\n",
    "        user_date = driver.find_elements(By.XPATH, '//*[@id=\"' + x +'\"]/div/div[2]/div[2]/span[1]/a/time')[0]\n",
    "        date = user_date.get_attribute('title')\n",
    "\n",
    "        #Extract user ids from each user on a page\n",
    "        userid_element = driver.find_elements(By.XPATH, '//*[@id=\"' + x +'\"]/div/div[2]/div[1]/span[1]/a[2]')[0]\n",
    "        userid = userid_element.text\n",
    "\n",
    "        #Extract Message for each user on a page\n",
    "        user_message = driver.find_elements(By.XPATH, '//*[@id=\"' + x +'\"]/div/div[3]/div/div[1]')[0]\n",
    "        comment = user_message.text\n",
    "\n",
    "        #Adding date, userid and comment for each user in a dataframe    \n",
    "        comments.loc[len(comments)] = [date,userid,comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>March 24, 2002 9:54PM</td>\n",
       "      <td>merc1</td>\n",
       "      <td>I personally think that with a few tweaks the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>March 24, 2002 11:06PM</td>\n",
       "      <td>fredvh</td>\n",
       "      <td>I am debating a new purchase and these two are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March 25, 2002 9:02AM</td>\n",
       "      <td>blueguydotcom</td>\n",
       "      <td>Great handling, RWD, excellent engine and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>March 25, 2002 3:04PM</td>\n",
       "      <td>hungrywhale</td>\n",
       "      <td>And no manual tranny. That may not matter to y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>March 25, 2002 4:44PM</td>\n",
       "      <td>riez</td>\n",
       "      <td>One beauty of BMW 3 Series is that there are s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Date        user_id  \\\n",
       "0   March 24, 2002 9:54PM          merc1   \n",
       "1  March 24, 2002 11:06PM         fredvh   \n",
       "2   March 25, 2002 9:02AM  blueguydotcom   \n",
       "3   March 25, 2002 3:04PM    hungrywhale   \n",
       "4   March 25, 2002 4:44PM           riez   \n",
       "\n",
       "                                            comments  \n",
       "0  I personally think that with a few tweaks the ...  \n",
       "1  I am debating a new purchase and these two are...  \n",
       "2  Great handling, RWD, excellent engine and the ...  \n",
       "3  And no manual tranny. That may not matter to y...  \n",
       "4  One beauty of BMW 3 Series is that there are s...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
